{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import wandb\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# Set precision to 3 decimal places\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I create a dataset of optimal demonstrations for \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"maze_4to50_hist8_bc\",\n",
    "    \"maze\": {\n",
    "        \"width\": {\"min\": 4, \"max\": 50},\n",
    "        \"height\": {\"min\": 4, \"max\": 50},\n",
    "        \"cell_occupancy_prob\": {\"min\": 0, \"max\": 0.3},\n",
    "        \"max_steps\": \"ShortestPath\", # Use this to set the max steps to the shortest path between source and target * 2\n",
    "        # \"max_steps\": \"L1SourceTarget\", # Use this to set the max steps to the L1 distance between source and target * 2\n",
    "        # To set paramters to constant values, use a float\n",
    "        # \"width\": 4,\n",
    "        # \"height\": 4,\n",
    "        # \"cell_occupancy_prob\": 0,\n",
    "        # \"max_steps\": 5, \n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 300,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "        \"history_length\": 8, # Number of current and previous states to consider, 1 for current state only\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 100,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_parallel_games\": 250,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"save_every\": 50,\n",
    "        \"use_wandb\": True,\n",
    "        # \"load_checkpoint\": \"maze_4to10_rtg_model_99\",\n",
    "        \"use_curriculum\": True,\n",
    "        \"curriculum_success_threshold\": 0.95,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that the reward stored in the state is unnormalized\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 100\n",
    "    MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -50\n",
    "\n",
    "    def __init__(self, width: Optional[int]=None, height: Optional[int]=None, cell_occupancy_prob: Optional[float] = 0.3,seed: Optional[int] = None,\n",
    "                 map: Optional[np.ndarray] = None, source: Optional[Tuple[int, int]] = None, target: Optional[Tuple[int, int]] = None, shortest_path: Optional[List[Tuple[int, int]]] = None):\n",
    "        \"\"\"Optionally pass in a map, source, target, and shortest path to avoid generating a new maze.\"\"\"\n",
    "        \n",
    "        if cell_occupancy_prob is not None and width is not None and height is not None:\n",
    "            assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "            assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "            self.width = int(width)\n",
    "            self.height = int(height)\n",
    "            self.seed = seed\n",
    "            self.cell_occupancy_prob = cell_occupancy_prob\n",
    "            self.generate_map()\n",
    "        elif map is not None:\n",
    "            assert source is not None and target is not None and shortest_path is not None, \"Must provide source, target, and shortest path if map is provided\"\n",
    "            self.width, self.height = map.shape\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "            self.map = map\n",
    "            self.shortest_path = shortest_path\n",
    "            \n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "\n",
    "        # Max steps configuration\n",
    "        # Option 1: Set the max steps to be the width * height\n",
    "        # self.max_steps=width*height\n",
    "        if cfg.maze.max_steps == \"L1SourceTarget\":\n",
    "            # Option 2: Set the max steps to be 2 * the L1 distance between source and target\n",
    "            self.max_steps = 2 * (abs(self.source[0] - self.target[0]) + abs(self.source[1] - self.target[1]))\n",
    "        elif cfg.maze.max_steps == \"ShortestPath\":\n",
    "            # Option 3: Set the max steps to be the shortest path between source and target * 2\n",
    "            self.max_steps = len(self.shortest_path) * 2\n",
    "        elif type(cfg.maze.max_steps) == int:\n",
    "            # Option 4: Manually set the max steps\n",
    "            self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "    @classmethod\n",
    "    def generate_maze_params(cls, num_mazes:int, maze_cfg, seed: Optional[int]=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        maze_params = []\n",
    "        for param_name in ['width', 'height', 'cell_occupancy_prob']:\n",
    "            param = getattr(maze_cfg, param_name)\n",
    "            if isinstance(param, (float, int)):\n",
    "                values = np.full(num_mazes, param)\n",
    "            elif isinstance(param, dict) or isinstance(param, DictConfig) and 'min' in param and 'max' in param:\n",
    "                min_val, max_val = param['min'], param['max']\n",
    "                if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                    # Assumes that if the min and max are integers we want all integers\n",
    "                    values = np.random.randint(min_val, max_val + 1, size=num_mazes)\n",
    "                else:\n",
    "                    values = np.random.uniform(min_val, max_val, size=num_mazes)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid parameter configuration: {param}\")\n",
    "            maze_params.append(values)\n",
    "\n",
    "        # Combine into a single n x 3 array\n",
    "        maze_params = np.column_stack(maze_params)\n",
    "        return maze_params\n",
    "\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "        # Plane 1: Target if in local observation window\n",
    "        plane_target = copy.deepcopy(plane_obstacles)\n",
    "\n",
    "        # Plane 0:\n",
    "        # Make sure that any number that is not 1 is 0 for the obstacle plane\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "        \n",
    "        # Plane 1:\n",
    "        # Make all non-target cells 0\n",
    "        plane_target[plane_target != 3] = 0\n",
    "        # Make target cells 1\n",
    "        plane_target[plane_target == 3] = 1\n",
    "\n",
    "        return np.stack([plane_obstacles, plane_target], axis=0)\n",
    "    \n",
    "    def get_encoded_action(self, action):\n",
    "        # One hot encoded action within the observation window\n",
    "        action_plane = np.zeros((self.observation_width, self.observation_width))\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        action_plane[self.observation_width // 2 + dx, self.observation_width // 2 + dy] = 1\n",
    "        return action_plane\n",
    "    \n",
    "    def get_one_hot_action(self, action):\n",
    "        # One hot encoded action\n",
    "        one_hot_action = np.zeros(self.action_size)\n",
    "        one_hot_action[action] = 1\n",
    "        return one_hot_action\n",
    "\n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "    \n",
    "    def get_normalized_steps_left(self, state: State):\n",
    "        return state.steps_left / self.max_steps\n",
    "    \n",
    "    # Not used because history is sufficient\n",
    "    # def get_normalized_distances(self):\n",
    "    #     # Returns the normalized distances in the x and y directions that can be travelled by the agent in 50% of the max steps\n",
    "    #     scaling_factor = 0.5\n",
    "\n",
    "    #     return (self.max_steps * scaling_factor / self.width, self.max_steps * scaling_factor / self.height)\n",
    "    \n",
    "    def get_encoded_scalar_features_less_target(self, state: State):\n",
    "        return np.array((\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            self.get_normalized_steps_left(state),\n",
    "        ))\n",
    "\n",
    "    def get_encoded_scalar_features(self, state: State):\n",
    "        return np.array((\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            self.get_normalized_steps_left(state),\n",
    "            *self.get_normalized_target_position(),\n",
    "        ))\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        \"\"\"Returns the unnormalized reward and whether the episode is terminated\"\"\"\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "        return state.reward, False\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        # Normalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 2 * ((reward - min_reward) / (max_reward - min_reward)) - 1\n",
    "    \n",
    "    def unnormalize_reward(self, normalized_reward):\n",
    "        # Unnormalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 0.5 * (normalized_reward + 1) * (max_reward - min_reward) + min_reward\n",
    "    \n",
    "    ACTION_TO_DELTA = [(0, 1), (0, -1), (-1, 0), (1, 0)] # Down, Up, Left, Right\n",
    "    ACTION_TO_STRING = ['Down', 'Up', 'Left', 'Right']\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        return Maze.ACTION_TO_DELTA[action]\n",
    "    \n",
    "    def delta_to_action(self, dx, dy):\n",
    "        return Maze.ACTION_TO_DELTA.index((dx, dy))\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        return Maze.ACTION_TO_STRING[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "    \n",
    "    def path_to_actions(self, path):\n",
    "        actions = []\n",
    "        for i in range(1, len(path)):\n",
    "            dx = path[i][0] - path[i-1][0]\n",
    "            dy = path[i][1] - path[i-1][1]\n",
    "            action = self.delta_to_action(dx, dy)\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def generate_maze_dataset(file_name, maze_cfg, num_episodes, seed: Optional[int] = None):\n",
    "    \"\"\"Generate a dataset of mazes and save it to a file. Does not save the observations/actions etc.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    else:\n",
    "        seed = np.random.randint(0, 2**32 - 1)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    maze_params = Maze.generate_maze_params(num_episodes, maze_cfg=maze_cfg)\n",
    "    mazes = [Maze(*params) for params in maze_params]\n",
    "    episodes = [\n",
    "        {\n",
    "            \"map\": maze.map,\n",
    "            \"source\": maze.source,\n",
    "            \"target\": maze.target,\n",
    "            \"cell_occupancy_prob\": maze.cell_occupancy_prob,\n",
    "            \"shortest_path\": maze.shortest_path,\n",
    "        } for maze in mazes\n",
    "    ]\n",
    "\n",
    "    dataset = {\n",
    "        \"episodes\": episodes,\n",
    "        \"dataset_maze_cfg\": OmegaConf.to_container(maze_cfg, resolve=True, throw_on_missing=True),\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "\n",
    "    file_path = f\"datasets/{file_name}.pkl\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    print(f\"Dataset saved to {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_maze_dataset(\"maze_4to50_0to0p3_50000\", cfg.maze, 50000)\n",
    "# generate_maze_dataset(\"maze_4to50_0to0p3_10000\", cfg.maze, 10000)\n",
    "# generate_maze_dataset(\"maze_4to50_0to0p3_100\", cfg.maze, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "def create_zarr(file_name, cfg):\n",
    "    # Load the dataset\n",
    "    with open(f\"datasets/{file_name}.pkl\", 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        print(f\"Loaded {file_name} dataset with {len(dataset['episodes'])} episodes, that used seed {dataset['seed']}\")\n",
    "        print(dataset['dataset_maze_cfg'])\n",
    "    \n",
    "    assert dataset['dataset_maze_cfg'] == OmegaConf.to_container(cfg.maze, resolve=True, throw_on_missing=True), \"Configuration mismatch\"\n",
    "\n",
    "    \n",
    "    # Collect all inputs\n",
    "    encoded_observations = []\n",
    "    encoded_actions = []\n",
    "    scalar_features = []\n",
    "    \n",
    "    # Collect ground truth outputs\n",
    "    policy_targets = []\n",
    "    value_targets = []\n",
    "\n",
    "    episode_ends = []\n",
    "    idx = 0\n",
    "\n",
    "    for episode in tqdm(dataset['episodes']):\n",
    "        maze = Maze(map=episode['map'], source=episode['source'], target=episode['target'], shortest_path=episode['shortest_path'])\n",
    "        state = maze.get_initial_state()\n",
    "        ep_rewards_to_come = []\n",
    "\n",
    "        for action in maze.path_to_actions(maze.shortest_path):\n",
    "            encoded_observations.append(maze.get_encoded_observation(state))\n",
    "            encoded_actions.append(maze.get_encoded_action(action))\n",
    "            scalar_features.append(maze.get_encoded_scalar_features(state))\n",
    "            policy_targets.append(maze.get_one_hot_action(action))\n",
    "            \n",
    "            ep_rewards_to_come.append(state.reward)\n",
    "\n",
    "            state = maze.get_next_state(state, action)\n",
    "            final_reward, terminated = maze.get_value_and_terminated(state)\n",
    "            \n",
    "            if terminated:\n",
    "                episode_ends.append(idx)\n",
    "                for reward_to_come in ep_rewards_to_come:\n",
    "                    reward_to_go = final_reward - reward_to_come\n",
    "                    value_targets.append(maze.normalize_reward(reward_to_go))\n",
    "            idx += 1\n",
    "\n",
    "    # Save the dataset to a Zarr file\n",
    "    zarr_path = f\"datasets/{file_name}.zarr\"\n",
    "    root = zarr.open(zarr_path, mode='w')\n",
    "    data_group = root.create_group('data')\n",
    "    meta_group = root.create_group('meta')\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    encoded_observations = np.array(encoded_observations)\n",
    "    encoded_actions = np.array(encoded_actions)\n",
    "    scalar_features = np.array(scalar_features)\n",
    "    policy_targets = np.array(policy_targets)\n",
    "    value_targets = np.array(value_targets)\n",
    "    episode_ends = np.array(episode_ends)\n",
    "\n",
    "    assert episode_ends[-1] == len(encoded_observations)-1, \"Episode ends must end at the last index\"\n",
    "\n",
    "    # Save the data (False chunks means we load the whole thing into memory)\n",
    "    data_group.create_dataset('encoded_observations', data=encoded_observations, chunks=False)\n",
    "    data_group.create_dataset('encoded_actions', data=encoded_actions, chunks=False)\n",
    "    data_group.create_dataset('scalar_features', data=scalar_features, chunks=False)\n",
    "    data_group.create_dataset('policy_targets', data=policy_targets, chunks=False)\n",
    "    data_group.create_dataset('value_targets', data=value_targets, chunks=False)\n",
    "    meta_group.create_dataset('episode_ends', data=episode_ends, chunks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded maze_4to50_0to0p3_100 dataset with 100 episodes, that used seed 838777499\n",
      "{'width': {'min': 4, 'max': 50}, 'height': {'min': 4, 'max': 50}, 'cell_occupancy_prob': {'min': 0, 'max': 0.3}, 'max_steps': 'ShortestPath'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6637cac3194fa88f67eb36310fce57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_zarr(\"maze_4to50_0to0p3_100\", cfg=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the seed makes the generated datasets reproducible\n",
    "from deepdiff import DeepDiff\n",
    "generate_maze_dataset(\"maze_4to50_0to0p3_10_a\", cfg.maze, 100, seed=0)\n",
    "generate_maze_dataset(\"maze_4to50_0to0p3_10_b\", cfg.maze, 100, seed=0)\n",
    "\n",
    "with open(\"datasets/maze_4to50_0to0p3_10_a.pkl\", \"rb\") as file:\n",
    "    loaded_data_a = pickle.load(file)\n",
    "with open(\"datasets/maze_4to50_0to0p3_10_b.pkl\", \"rb\") as file:\n",
    "    loaded_data_b = pickle.load(file)\n",
    "\n",
    "diff = DeepDiff(loaded_data_a, loaded_data_b, ignore_order=True)\n",
    "\n",
    "if not diff:\n",
    "    print(\"The objects are identical.\")\n",
    "else:\n",
    "    print(\"The objects differ:\", diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test observation history\n",
    "maze = Maze(10, 10, 0.3, seed=0)\n",
    "state = maze.get_initial_state()\n",
    "maze.visualize_state()\n",
    "maze.visualize_path()\n",
    "# actions = maze.path_to_actions(maze.shortest_path)\n",
    "# print([maze.action_to_string(action) for action in actions])\n",
    "\n",
    "# for action in actions:\n",
    "#     state = maze.get_next_state(state, action)\n",
    "#     print(state)\n",
    "#     observation = maze.get_encoded_observation(state)\n",
    "#     action_plane = maze.get_encoded_action(action)\n",
    "#     scalar_features = maze.get_encoded_scalar_features_less_target(state)\n",
    "#     print(observation)\n",
    "#     print(observation.shape)\n",
    "\n",
    "episode_data = {\n",
    "    \"map\": maze.map,\n",
    "    \"source\": maze.source,\n",
    "    \"target\": maze.target,\n",
    "    \"cell_occupancy_prob\": maze.cell_occupancy_prob,\n",
    "    \"shortest_path\": maze.shortest_path,\n",
    "}\n",
    "episodes = []\n",
    "episodes.append(episode_data)\n",
    "dataset = {\n",
    "    \"episodes\": episodes,\n",
    "    \"dataset_maze_cfg\": OmegaConf.to_container(cfg.maze, resolve=True, throw_on_missing=True),\n",
    "}\n",
    "print(dataset)\n",
    "file_name = f\"maze_4to100_0to0p3_50000\"\n",
    "file_path = f\"datasets/{file_name}.pkl\"\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, model_cfg, device):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        SCALAR_FEATURES_SIZE = 3*model_cfg.history_length + 2  # see Maze.get_encoded_scalar_features\n",
    "        num_filters = model_cfg.num_filters\n",
    "        num_resBlocks = model_cfg.num_resBlocks\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # Input channels are stack of (all observation planes (hist len), all target planes (hist len), all action planes (hist len - 1))\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3 * model_cfg.history_length - 1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + SCALAR_FEATURES_SIZE, 256),  # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + SCALAR_FEATURES_SIZE, 256), # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, scalar_features):\n",
    "        # x: Input tensor of shape (batch_size, 2 * history_length, maze_height, maze_width)\n",
    "        # scalar_features: (batch_size, 3 * history length + 2), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, scalar_features], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, scalar_features], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Search node in the MCTS tree\"\"\"\n",
    "    def __init__(self, state, game: Maze, parent: \"Node\"=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.game = game\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = game.get_valid_actions(state)\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "        history_length = cfg.model.history_length\n",
    "\n",
    "        if parent is None:\n",
    "            # Root node\n",
    "            # Initialize histories with the initial observation and scalar features\n",
    "            obs = game.get_encoded_observation(state)\n",
    "            scalar_features = game.get_encoded_scalar_features_less_target(state)\n",
    "\n",
    "            # Initialize observation history array\n",
    "            # Shape: (history_length, num_obs_planes, H, W)\n",
    "            obs_shape = obs.shape\n",
    "            self.obs_history = np.zeros((history_length, *obs_shape), dtype=obs.dtype)\n",
    "            for i in range(history_length):\n",
    "                self.obs_history[i] = obs\n",
    "\n",
    "            # Initialize scalar features history array\n",
    "            # Shape: (history_length, scalar_feature_size)\n",
    "            scalar_feature_size = scalar_features.shape[0]\n",
    "            self.scalar_features_history = np.zeros((history_length, scalar_feature_size), dtype=scalar_features.dtype)\n",
    "            for i in range(history_length):\n",
    "                self.scalar_features_history[i] = scalar_features\n",
    "\n",
    "            # Initialize action plane history array\n",
    "            # Shape: (history_length - 1, H, W)\n",
    "            action_plane_shape = game.get_encoded_action(0).shape\n",
    "            self.action_plane_history = np.zeros((history_length - 1, *action_plane_shape), dtype=np.float32)\n",
    "        else:\n",
    "            # Copy histories from parent and append current observation, action plane, and scalar features\n",
    "            self.obs_history = np.roll(parent.obs_history, shift=1, axis=0)\n",
    "            self.obs_history[0] = game.get_encoded_observation(state)\n",
    "\n",
    "            self.scalar_features_history = np.roll(parent.scalar_features_history, shift=1, axis=0)\n",
    "            self.scalar_features_history[0] = game.get_encoded_scalar_features_less_target(state)\n",
    "\n",
    "            # For action plane history, we need to handle history_length - 1 entries\n",
    "            if parent.action_plane_history.shape[0] > 0:\n",
    "                self.action_plane_history = np.roll(parent.action_plane_history, shift=1, axis=0)\n",
    "                self.action_plane_history[0] = game.get_encoded_action(last_action)\n",
    "            else:\n",
    "                # If history length is 1, there are no action planes\n",
    "                self.action_plane_history = parent.action_plane_history\n",
    "    \n",
    "    def get_spatial_history(self):\n",
    "        # Extract obstacle planes from obs_history\n",
    "        obstacle_planes = self.obs_history[:, 0, :, :]  # Shape: (history_length, H, W)\n",
    "\n",
    "        # Extract target planes from obs_history\n",
    "        target_planes = self.obs_history[:, 1, :, :]  # Shape: (history_length, H, W)\n",
    "\n",
    "        # Append action planes if they exist\n",
    "        if self.action_plane_history.shape[0] > 0:\n",
    "            # Stack obstacle, target, and action planes along the channel dimension\n",
    "            planes = np.concatenate([obstacle_planes, target_planes, self.action_plane_history], axis=0)\n",
    "        else:\n",
    "            # Stack obstacle and target planes along the channel dimension\n",
    "            planes = np.concatenate([obstacle_planes, target_planes], axis=0) # Shape: (2 * history_length, H, W)\n",
    "        # No else needed since action_plane_history could be empty\n",
    "\n",
    "        return planes\n",
    "    \n",
    "    def get_scalar_history(self):\n",
    "        # Flatten scalar_features_history\n",
    "        # self.scalar_features_history shape: (hist_len, scalar_feature_size)\n",
    "        return np.concatenate((self.scalar_features_history.flatten('F'), self.game.get_normalized_target_position()))\n",
    "    \n",
    "\n",
    "class GameEpisode:\n",
    "    \"\"\"Stateful episode of a game\"\"\"\n",
    "    def __init__(self, game: Maze):\n",
    "        self.game = game\n",
    "        self.state: Maze.State = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.reward_history = []\n",
    "        self.root: Optional[Node] = Node(self.state, self.game)\n",
    "        self.node: Optional[Node] = None\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, search_cfg, model: ResNet):\n",
    "        self.cfg = search_cfg\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, game: Maze, max_iters = 1000, verbose=True, visualize=True):\n",
    "        \"\"\"Play a single game\"\"\"\n",
    "        state = game.get_initial_state()\n",
    "        path = []\n",
    "        root = Node(state, game)\n",
    "        for i in range(max_iters):\n",
    "            action_probs = self.search(game, root=root)\n",
    "            path.append((state.x, state.y))\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {game.action_to_string(action)}\")\n",
    "            #     print(f\"agent: {game.get_normalized_agent_position(state)}, target: {game.get_normalized_target_position()}, steps_left: {game.get_normalized_steps_left(state)}, distances: {game.get_normalized_distances()}\")\n",
    "                # obs_planes = game.get_encoded_observation(state)\n",
    "                # game.visualize_state(obs_planes[0])\n",
    "                # game.visualize_state(obs_planes[1])\n",
    "\n",
    "            for child in root.children:\n",
    "                if child.last_action == action:\n",
    "                    # Set the child as the new root to preserve the search tree\n",
    "                    root = child\n",
    "                    break\n",
    "            state = root.state\n",
    "            \n",
    "            value, is_terminal = game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    game.visualize_path(path)\n",
    "                \n",
    "                return path, value\n",
    "                \n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, game: Maze, state: Optional[Maze.State] = None, root: Optional[Node] = None) -> np.ndarray:\n",
    "        if root is None and state is not None:\n",
    "            root = Node(state, game)\n",
    "        elif state is None and root is None:\n",
    "            assert False, \"Either state or root must be provided\"\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node, game)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(node=node, game=game)\n",
    "                value = game.unnormalize_reward(value)\n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy, game=game)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batch_search(self, episodes: List[GameEpisode]):\n",
    "        \n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            # Collect nodes for expansion and evaluation\n",
    "            for ep in episodes:\n",
    "                ep.node = None # Reset the node marked for expansion and evaluation for each episode\n",
    "                node = ep.root\n",
    "                # Selection all the way down till a leaf node\n",
    "                while not node.is_leaf:\n",
    "                    node = self.select(node, ep.game)\n",
    "\n",
    "                # Evaluate the leaf node\n",
    "                value, is_terminal = ep.game.get_value_and_terminated(node.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    self.backpropagate(node, value)\n",
    "                else:\n",
    "                    ep.node = node # Mark the leaf node for expansion and evaluation\n",
    "\n",
    "            # Batch query the model for the policy and value\n",
    "            expandable_episodes = [ep_idx for ep_idx, ep in enumerate(episodes) if ep.node is not None]\n",
    "\n",
    "            if len(expandable_episodes) > 0:\n",
    "                obs = np.stack([episodes[ep_idx].node.get_spatial_history() for ep_idx in expandable_episodes])\n",
    "                scalar_features = np.stack([episodes[ep_idx].node.get_scalar_history() for ep_idx in expandable_episodes])\n",
    "                tensor_obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "                tensor_scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.model(\n",
    "                    tensor_obs, tensor_scalar_features\n",
    "                    )\n",
    "                \n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "            \n",
    "            # Expand the nodes and backpropagate\n",
    "            for batch_idx, ep_idx in enumerate(expandable_episodes):\n",
    "                node = episodes[ep_idx].node\n",
    "                ep_policy, ep_value = policy[batch_idx], value[batch_idx].item()\n",
    "\n",
    "                valid_policy = np.zeros_like(ep_policy)\n",
    "                valid_policy[node.valid_actions] = ep_policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                ep_value = ep.game.unnormalize_reward(ep_value)\n",
    "                self.expand(node, policy=valid_policy, game=episodes[ep_idx].game)\n",
    "                self.backpropagate(node, ep_value)\n",
    "    \n",
    "    def query_model(self, node: Node) -> Tuple[np.ndarray, float]:\n",
    "        tensor_obs = torch.tensor(node.get_spatial_history(), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        tensor_scalar_features = torch.tensor(node.get_scalar_history(), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_scalar_features\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        normalized_policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return normalized_policy, value\n",
    "\n",
    "    def select(self, node: Node, game: Maze) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child, game) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node, game: Maze) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # Q-value needs to be noramalized between -1 and 1 for this formula.\n",
    "            q_value = game.normalize_reward(child.value_sum / child.visit_count)\n",
    "\n",
    "        u_value = self.cfg.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        \n",
    "        return q_value + u_value\n",
    "\n",
    "    \n",
    "    def expand(self, node: Node, policy, game: Maze) -> None:\n",
    "        _, is_terminal = game.get_value_and_terminated(node.state)\n",
    "        assert not is_terminal, \"Cannot expand a terminal node\"\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  game,\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        \n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        \"\"\"Takes in unnormalized value\"\"\"\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test observation history\n",
    "maze = Maze(10, 10, 0.3, seed=0)\n",
    "state = maze.get_initial_state()\n",
    "node = Node(state, maze)\n",
    "maze.visualize_state()\n",
    "\n",
    "# Checking the spatial features\n",
    "# print(node.obs_history.shape)\n",
    "# print(node.action_plane_history.shape)\n",
    "# print(node.get_spatial_history().shape)\n",
    "# print(node.get_spatial_history())\n",
    "\n",
    "# # Checking the scalar features\n",
    "# print(node.get_scalar_history().shape)\n",
    "# print(node.get_scalar_history())\n",
    "\n",
    "# Check the histories after taking some actions\n",
    "actions = [0,0] # Down, Down\n",
    "for action in actions:\n",
    "    state = maze.get_next_state(state, action)\n",
    "    node = Node(state, maze, parent=node, last_action=action)\n",
    "\n",
    "# Checking the spatial features\n",
    "print(node.obs_history.shape)\n",
    "print(node.action_plane_history.shape)\n",
    "print(node.get_spatial_history().shape)\n",
    "for plane in node.get_spatial_history():\n",
    "    maze.visualize_state(plane)\n",
    "\n",
    "# Checking the scalar features\n",
    "print(node.get_scalar_history().shape)\n",
    "print(node.get_scalar_history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model: ResNet, optimizer, search_alg: AlphaMCTS, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "\n",
    "        self.cfg = cfg.learn\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.last_success_rates = []\n",
    "        self.last_maze_cfg = None\n",
    "        self.original_maze_cfg = cfg.maze\n",
    "    \n",
    "    def get_maze_cfg_from_curriculum(self):\n",
    "        if len(self.last_success_rates) < 3:\n",
    "            # Not enough data yet, start with initial maze size\n",
    "            maze_cfg = copy.deepcopy(self.original_maze_cfg)\n",
    "            maze_cfg.width.max = 6\n",
    "            maze_cfg.height.max = 6\n",
    "            return maze_cfg\n",
    "\n",
    "        if all(rate >= self.cfg.curriculum_success_threshold for rate in self.last_success_rates):\n",
    "            # Increase the maze size\n",
    "            maze_cfg = copy.deepcopy(self.last_maze_cfg)\n",
    "            maze_cfg.width.max = min(self.last_maze_cfg.width.max + 2, self.original_maze_cfg.width.max)\n",
    "            maze_cfg.height.max = min(self.last_maze_cfg.height.max + 2, self.original_maze_cfg.height.max)\n",
    "        else:\n",
    "            # Keep the maze size the same\n",
    "            maze_cfg = self.last_maze_cfg\n",
    "        \n",
    "        return maze_cfg\n",
    "    \n",
    "    def self_play(self, maze_cfg):\n",
    "\n",
    "        maze_params = Maze.generate_maze_params(self.cfg.num_parallel_games, maze_cfg=maze_cfg)\n",
    "        episodes = [GameEpisode(Maze(*params)) for params in maze_params]\n",
    "        ret_mem = []\n",
    "        n_successes = 0\n",
    "        optimal_path_ratio_sum = 0\n",
    "        while len(episodes) > 0:\n",
    "            \n",
    "            self.search_alg.batch_search(episodes)\n",
    "\n",
    "            # Serially process the episodes\n",
    "            for i in range(len(episodes))[::-1]:\n",
    "                ep = episodes[i]\n",
    "\n",
    "                action_probs = np.zeros(ep.game.action_size)\n",
    "                for child in ep.root.children:\n",
    "                    action_probs[child.last_action] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                ep.memory.append((ep.root.get_spatial_history(),\n",
    "                                  ep.root.get_scalar_history(),\n",
    "                                  action_probs))\n",
    "                ep.reward_history.append(ep.root.state.reward)\n",
    "\n",
    "                action = np.random.choice(ep.game.action_size, p=action_probs)\n",
    "                for child in ep.root.children:\n",
    "                    if child.last_action == action:\n",
    "                        # Set the child as the new root to preserve the search tree\n",
    "                        ep.root = child\n",
    "                        break\n",
    "                ep.state = ep.root.state\n",
    "\n",
    "                final_reward, is_terminal = ep.game.get_value_and_terminated(ep.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    # Unroll the reward history and memory\n",
    "                    for mem, reward_to_come in zip(ep.memory, ep.reward_history):\n",
    "                        reward_to_go = final_reward - reward_to_come\n",
    "                        ret_mem.append((*mem, ep.game.normalize_reward(reward_to_go)))\n",
    "                    if (ep.state.x, ep.state.y) == ep.game.target:\n",
    "                        n_successes += 1\n",
    "                        optimal_path_ratio_sum += (len(ep.memory)+1)/len(ep.game.shortest_path)\n",
    "\n",
    "                    del episodes[i]\n",
    "        \n",
    "        return ret_mem, n_successes, optimal_path_ratio_sum\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.cfg.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.cfg.train_batch_size]\n",
    "            obs, scalar_features, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, scalar_features, policy_targets, value_targets = np.array(obs), np.array(scalar_features), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "            scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, scalar_features)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log metrics for the current batch\n",
    "                wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.cfg.train_batch_size)\n",
    "        if self.cfg.use_wandb:\n",
    "            # Log average loss for the epoch\n",
    "            wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "                name=cfg.name,\n",
    "                config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "                save_code=True)\n",
    "            \n",
    "            wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for iteration in range(self.cfg.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "            optimal_path_ratio_sum = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "\n",
    "            # Initialize all games and an episode for each game\n",
    "            if self.cfg.use_curriculum:\n",
    "                maze_cfg = self.get_maze_cfg_from_curriculum()\n",
    "                wandb.log({\"max_maze_width\": maze_cfg.width.max})\n",
    "                self.last_maze_cfg = maze_cfg\n",
    "            else:\n",
    "                maze_cfg = cfg.maze\n",
    "\n",
    "            # Calculate the number of batches\n",
    "            n_batches = self.cfg.num_self_play_iters // self.cfg.num_parallel_games\n",
    "\n",
    "\n",
    "            for _ in trange(n_batches):\n",
    "                batch_episode_mems, num_episode_successes, opt_path_ratio_sum = self.self_play(maze_cfg)\n",
    "                successes += num_episode_successes\n",
    "                optimal_path_ratio_sum += opt_path_ratio_sum\n",
    "                memory += batch_episode_mems\n",
    "\n",
    "            success_rate = successes / self.cfg.num_self_play_iters\n",
    "            optimal_path_ratio = optimal_path_ratio_sum / successes\n",
    "            self.last_success_rates.append(success_rate)\n",
    "            if len(self.last_success_rates) > 3:\n",
    "                self.last_success_rates.pop(0)  # Keep only the last 3 success rates\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log the success rate for self-play games\n",
    "                wandb.log({\"success_rate\": success_rate, \"optimal_path_ratio\": optimal_path_ratio, \"iteration\": iteration, \"wall_time\": time.time() - start_time})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.cfg.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            # Save if iter divides save_every or if it is the last iteration\n",
    "            if (iteration % self.cfg.save_every == 0 and iteration != 0) or iteration == self.cfg.num_learn_iters - 1:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "                if self.cfg.use_wandb:\n",
    "                    # Log model checkpoint to W&B\n",
    "                    wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "                    wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(cfg.model, device)\n",
    "if \"load_checkpoint\" in cfg.learn:\n",
    "    # Replace \"optimizer\" with \"model\"\n",
    "    model_filename = cfg.learn.load_checkpoint.replace(\"optimizer\", \"model\")\n",
    "    model.load_state_dict(torch.load(f\"checkpoints/{model_filename}.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr, weight_decay=cfg.learn.weight_decay)\n",
    "if \"load_checkpoint\" in cfg.learn:\n",
    "    # Replace \"model\" with \"optimizer\"\n",
    "    model_filename = cfg.learn.load_checkpoint.replace(\"model\", \"optimizer\")\n",
    "    optimizer.load_state_dict(torch.load(f\"checkpoints/{model_filename}.pt\"))\n",
    "\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, seed=0)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = Maze.generate_maze_params(1, cfg)\n",
    "\n",
    "params = np.array([[100, 100, 0.1]])\n",
    "width, height, cell_occupancy_prob = params[0]\n",
    "width, height = int(width), int(height)\n",
    "maze = Maze(*params[0])\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model, device)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{100}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_round3_model_13.pt\"))\n",
    "model.eval()\n",
    "\n",
    "new_cfg = copy.deepcopy(cfg)\n",
    "new_cfg.search.num_simulations = 50\n",
    "mcts = AlphaMCTS(search_cfg=new_cfg.search, model=model)\n",
    "\n",
    "_ = mcts.play_game(game=maze, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positions around target:\n",
    "positions = [(maze.target[0] + dx, maze.target[1] + dy) for dx in [-1, 0, 1] for dy in [-1, 0, 1]]\n",
    "print(f\"Target: {maze.target}\")\n",
    "for pos in positions:\n",
    "    if pos == maze.target or maze.map[pos] == 1:\n",
    "        continue\n",
    "    state = Maze.State(*pos, 1, 0)\n",
    "    policy, final_reward = mcts.query_model(state, game=maze)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {final_reward}\")\n",
    "    search_probs = mcts.search(game=maze, state=state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
