{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Set precision to 3 decimal places\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'm going to do exactly the same as with maxsteps2_wstepsleft, but now I'm going to set the maxsteps to 16. My hypothesis is that the policy will not give significant weight to the most direct path to the target since it can get away with going back and forth and wasting lots of steps before going to the goal. I wonder now that I also pass in the steps left, if when you tell it you only have 1 or 2 steps left, it would have learnt that it needs to go directly to the goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"maze_4x4_binaryreward_maxsteps16\",\n",
    "    \"maze\": {\n",
    "        \"width\": 4,\n",
    "        \"height\": 4,\n",
    "        \"cell_occupancy_prob\": 0,\n",
    "        \"max_steps\": 16\n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 8,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 1\n",
    "    # MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -1\n",
    "\n",
    "    def __init__(self, width: int, height: int, seed: Optional[int] = None, cell_occupancy_prob: float = 0.3):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "\n",
    "        # self.max_steps=width*height\n",
    "        # For this simplest possible maze, set the max length to be 2\n",
    "        self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "        # In this maze, all the free space in the maze is observable from any position\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        # dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        dr = 0\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "\n",
    "        # Make sure that any number that is not 1 is 0\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "\n",
    "        return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "        # # Plane 1: Free Space (1s where free space, 0s where obstacles)\n",
    "        # plane_free_space = plane_obstacles == 0\n",
    "\n",
    "        # # Plane 2: Agent's position\n",
    "        # plane_agent = np.zeros_like(plane_obstacles)\n",
    "        # plane_agent[half_width, half_width] = 1\n",
    "\n",
    "        # encoded_observation = np.stack([plane_obstacles, plane_free_space, plane_agent], axis=0)\n",
    "\n",
    "        # return encoded_observation\n",
    "    \n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "    \n",
    "    def get_normalized_steps_left(self, state: State):\n",
    "        return state.steps_left / self.max_steps\n",
    "    \n",
    "    def get_normalized_distances(self):\n",
    "        # Returns the normalized distances in the x and y directions that can be travelled by the agent in 50% of the max steps\n",
    "        scaling_factor = 0.5\n",
    "\n",
    "        return (self.max_steps * scaling_factor / self.width, self.max_steps * scaling_factor / self.height)\n",
    "    \n",
    "    def get_encoded_scalar_features(self, state: State):\n",
    "        return (\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            *self.get_normalized_target_position(),\n",
    "            self.get_normalized_steps_left(state),\n",
    "            *self.get_normalized_distances()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        # In this case we are using binary reward\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "    \n",
    "        return state.reward, False\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            # Set the max steps to be 3 * the L1 distance between source and target\n",
    "            # self.max_steps = 3 * (abs(source[0] - target[0]) + abs(source[1] - target[1]))\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        num_scalar_features = 7  # see Maze.get_encoded_scalar_features\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The single input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + num_scalar_features, 256),  # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + num_scalar_features, 256), # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, scalar_features):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # scalar_features: (batch_size, 7), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, scalar_features], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, scalar_features], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, game: Maze, num_simulations, c_puct, model):\n",
    "        self.game = game\n",
    "        self.num_simulations = num_simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, max_iters = 1000, verbose=True, visualize=True):\n",
    "        state = self.game.get_initial_state()\n",
    "        path = []\n",
    "        memory = []\n",
    "        for i in range(max_iters):\n",
    "            \n",
    "            action_probs = self.search(state)\n",
    "            path.append((state.x, state.y))\n",
    "            memory.append((self.game.get_encoded_observation(state), \n",
    "                           self.game.get_encoded_scalar_features(state),\n",
    "                           action_probs))\n",
    "\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            state = self.game.get_next_state(state, action)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                ret_mem = [(*mem, value) for mem in memory]\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == self.game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    self.game.visualize_path(path)\n",
    "                \n",
    "                return ret_mem\n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(state, self.game.get_valid_actions(state))\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(node.state)\n",
    "                \n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    def query_model(self, state: Maze.State):\n",
    "        tensor_obs = torch.tensor(self.game.get_encoded_observation(state), dtype=torch.float32).unsqueeze(0)\n",
    "        tensor_scalar_features = torch.tensor(self.game.get_encoded_scalar_features(state), dtype=torch.float32).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_scalar_features\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        normalized_policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return normalized_policy, value\n",
    "\n",
    "\n",
    "    def select(self, node: Node) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = child.value_sum / child.visit_count\n",
    "        u_value = self.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        return q_value + u_value\n",
    "    \n",
    "    def expand(self, node: Node, policy) -> None:\n",
    "        if (node.state.x, node.state.y) == self.game.target:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = self.game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  self.game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, search_alg: AlphaMCTS, num_learn_iters, num_self_play_iters, num_train_epochs, train_batch_size, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "        self.num_learn_iters = num_learn_iters\n",
    "        self.num_self_play_iters = num_self_play_iters\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.train_batch_size = train_batch_size\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def self_play(self):\n",
    "        # Initialize game\n",
    "        # For now train on a fixed size maze\n",
    "        game = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "        self.search_alg.game = game\n",
    "        return self.search_alg.play_game(verbose=False, visualize=False)\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.train_batch_size]\n",
    "            obs, scalar_features, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, scalar_features, policy_targets, value_targets = np.array(obs), np.array(scalar_features), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            scalar_features = torch.tensor(scalar_features, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, scalar_features)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Log metrics for the current batch\n",
    "            wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.train_batch_size)\n",
    "        # Log average loss for the epoch\n",
    "        wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self, save_every=1):\n",
    "        wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "            name=cfg.name,\n",
    "            config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "            save_code=True)\n",
    "        \n",
    "        wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        for iteration in range(self.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "            for _ in trange(self.num_self_play_iters):\n",
    "                game_mem = self.self_play()\n",
    "                if game_mem[-1][-1] > 0:  # Assuming positive value means successful game\n",
    "                    successes += 1\n",
    "                memory += game_mem\n",
    "\n",
    "            success_rate = successes / self.num_self_play_iters\n",
    "            # Log the success rate for self-play games\n",
    "            wandb.log({\"success_rate\": success_rate, \"iteration\": iteration})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            # Save if iter divides save_every or if it is the last iteration\n",
    "            if (iteration % save_every == 0 and iteration != 0) or iteration == self.num_learn_iters - 1:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "                # Log model checkpoint to W&B\n",
    "                wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "                wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaoyuan\u001b[0m (\u001b[33mcontact_placement\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shaoyuan/Documents/Software/AlphaZeroFromScratch/discrete_maze/wandb/run-20241113_100452-vkyvcotv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/vkyvcotv' target=\"_blank\">maze_4x4_binaryreward_maxsteps16</a></strong> to <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/vkyvcotv' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/vkyvcotv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0826e96d43f24ad1ae2ce9b6a9f532e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed4c8402f9d45dda1ad01657e940574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8fd98e5f8745f9a53dc265589145cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47b31d08a63466db114371dc0777bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33312de29e04d34bbc07d4d7e4ed992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437992e3b94042908fb43906e15ae628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bc57faf72d4611b7075896e74aafcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7065514a89b475889a3b60eac8a3e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a451ce68f334e48afe75226f0f8c28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8b6f39056241feb6de74ab9a00d1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacaf08c03294a1d802f77fd8526e4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8765ca0badaf4140867d604431f16c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5befe66a14f48a9b48f5509506fa1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66714170f2cd4f9d9fb0f0f8a5feca10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73a5f0a7a7244f880e090a35cf42cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d6a501eaa14826ad7ce3285b309347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba41e5739a44edc9dd33769cf7a0a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.563 MB of 0.563 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▆▃▃▃▂▃▂▂▂▃▂▂▂▂▂▃▃▃▃▃▃▃▃▄▃▂▁▂▂▂▂▃▂▃▂▂▂▁▂</td></tr><tr><td>epoch</td><td>▁▃▆█▁▃▆█▁▃▆█▁▃▆█▁▃▆█▁▃▆█▁▃▆█▁▃▆█</td></tr><tr><td>iteration</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>success_rate</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>train_epoch_loss</td><td>█▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.65881</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>iteration</td><td>7</td></tr><tr><td>success_rate</td><td>1</td></tr><tr><td>train_epoch_loss</td><td>0.68501</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">maze_4x4_binaryreward_maxsteps16</strong> at: <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/vkyvcotv' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/vkyvcotv</a><br/> View project at: <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_100452-vkyvcotv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters)\n",
    "# model.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_model_7.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr)\n",
    "# optimizer.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_optimizer_7.pt\"))\n",
    "\n",
    "mcts = AlphaMCTS(maze, num_simulations=cfg.search.num_simulations, c_puct=cfg.search.c_puct, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, \n",
    "                      num_learn_iters=cfg.learn.num_learn_iters, \n",
    "                      num_self_play_iters=cfg.learn.num_self_play_iters,\n",
    "                      num_train_epochs=cfg.learn.num_train_epochs,\n",
    "                      train_batch_size=cfg.learn.train_batch_size)\n",
    "alphaZero.learn(save_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took 4m 33s to run, the first thing I'm going to do is implement the GPU and parallelization code to make this faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached target in 2 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_304603/467279036.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: (1, 1), policy: [0.409 0.    0.    0.591], policy argmax:Right policy value: 0.9999102354049683\n",
      "search: [0.939 0.    0.    0.061], search argmax: Down\n",
      "Position: (2, 1), policy: [0.265 0.    0.734 0.   ], policy argmax:Left policy value: 0.9999241232872009\n",
      "search: [0.265 0.    0.735 0.   ], search argmax: Left\n",
      "Position: (2, 2), policy: [0.    0.418 0.582 0.   ], policy argmax:Left policy value: 0.9996839761734009\n",
      "search: [0.    0.041 0.959 0.   ], search argmax: Left\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{100}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_round3_model_13.pt\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "mcts = AlphaMCTS(maze, num_simulations=cfg.search.num_simulations, c_puct=cfg.search.c_puct, model=model)\n",
    "\n",
    "_ = mcts.play_game()\n",
    "\n",
    "positions = [(x, y) for x in range(1, cfg.maze.width-1) for y in range(1, cfg.maze.height-1)]\n",
    "for pos in positions:\n",
    "    if pos == maze.target:\n",
    "        continue\n",
    "    state = Maze.State(*pos, 1, 0)\n",
    "    policy, value = mcts.query_model(state)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {value}\")\n",
    "    search_probs = mcts.search(state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think my hypothesis is true, the values are uniformly high (close to 1), and there are differences between the search argmax and policy argmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
