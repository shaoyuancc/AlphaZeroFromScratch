{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},torctorch.load(f"../checkpoints
   "outputs": [torch.loatorch.load(f"../checkpoints
    {torch.load(f"../chetorch.load(f"../checkpoints
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Set precision to 3 decimal places\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'm going to do exactly the same as with maze_4x4_binaryreward_maxsteps16, but implement the following changes from AlphaTweaks and AlphaParallel.\n",
    "\n",
    "Changes to be implemented:\n",
    "1. Use GPU as device\n",
    "2. Parallelize self-play episodes\n",
    "3. Use weight decay in optimizer\n",
    "4. Reuse search tree of selected child node from previous MCTS search\n",
    "\n",
    "Changes to be skipped:\n",
    "1. Temperature\n",
    "1. Dirichlet Noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"parallel_maze_4x4_binaryreward_maxsteps2\",\n",
    "    \"maze\": {\n",
    "        \"width\": 4,\n",
    "        \"height\": 4,\n",
    "        \"cell_occupancy_prob\": 0,\n",
    "        \"max_steps\": 2\n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 40,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_parallel_games\": 250,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"use_wandb\": False,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 1\n",
    "    # MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -1\n",
    "\n",
    "    def __init__(self, width: int, height: int, seed: Optional[int] = None, cell_occupancy_prob: float = 0.3):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "\n",
    "        # Max steps configuration\n",
    "        # Option 1: Set the max steps to be the width * height\n",
    "        # self.max_steps=width*height\n",
    "        # Option 2: Set the max steps to be 2 * the L1 distance between source and target\n",
    "        # self.max_steps = 2 * (abs(self.source[0] - self.target[0]) + abs(self.source[1] - self.target[1]))\n",
    "        # Option 3: Manually set the max steps\n",
    "        self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        # dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        dr = 0\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "\n",
    "        # Make sure that any number that is not 1 is 0\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "        return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "\n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "    \n",
    "    def get_normalized_steps_left(self, state: State):\n",
    "        return state.steps_left / self.max_steps\n",
    "    \n",
    "    def get_normalized_distances(self):\n",
    "        # Returns the normalized distances in the x and y directions that can be travelled by the agent in 50% of the max steps\n",
    "        scaling_factor = 0.5\n",
    "\n",
    "        return (self.max_steps * scaling_factor / self.width, self.max_steps * scaling_factor / self.height)\n",
    "    \n",
    "    def get_encoded_scalar_features(self, state: State):\n",
    "        return (\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            *self.get_normalized_target_position(),\n",
    "            self.get_normalized_steps_left(state),\n",
    "            *self.get_normalized_distances()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        # In this case we are using binary reward\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "    \n",
    "        return state.reward, False\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters, device):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        SCALAR_FEATURES_SIZE = 7  # see Maze.get_encoded_scalar_features\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The single input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + SCALAR_FEATURES_SIZE, 256),  # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + SCALAR_FEATURES_SIZE, 256), # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, scalar_features):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # scalar_features: (batch_size, 7), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, scalar_features], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, scalar_features], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Search node in the MCTS tree\"\"\"\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "class GameEpisode:\n",
    "    \"\"\"Stateful episode of a game\"\"\"\n",
    "    def __init__(self, game: Maze):\n",
    "        self.game = game\n",
    "        self.state: Maze.State = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root: Optional[Node] = Node(self.state, self.game.get_valid_actions(self.state))\n",
    "        self.node: Optional[Node] = None\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, search_cfg, model: ResNet):\n",
    "        self.cfg = search_cfg\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, game: Maze, max_iters = 1000, verbose=True, visualize=True):\n",
    "        \"\"\"Play a single game\"\"\"\n",
    "        state = game.get_initial_state()\n",
    "        path = []\n",
    "        memory = []\n",
    "        root = Node(state, game.get_valid_actions(state))\n",
    "        for i in range(max_iters):\n",
    "            action_probs = self.search(game, root=root)\n",
    "            path.append((state.x, state.y))\n",
    "            memory.append((game.get_encoded_observation(state), \n",
    "                           game.get_encoded_scalar_features(state),\n",
    "                           action_probs))\n",
    "\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            for child in root.children:\n",
    "                if child.last_action == action:\n",
    "                    # Set the child as the new root to preserve the search tree\n",
    "                    root = child\n",
    "                    break\n",
    "            state = root.state\n",
    "            \n",
    "            value, is_terminal = game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                ret_mem = [(*mem, value) for mem in memory]\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    game.visualize_path(path)\n",
    "                \n",
    "                return ret_mem\n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, game: Maze, state: Optional[Maze.State] = None, root: Optional[Node] = None) -> np.ndarray:\n",
    "        if root is None and state is not None:\n",
    "            root = Node(state, game.get_valid_actions(state))\n",
    "        elif state is None and root is None:\n",
    "            assert False, \"Either state or root must be provided\"\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(state=node.state, game=game)\n",
    "                \n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy, game=game)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batch_search(self, episodes: List[GameEpisode]):\n",
    "        \n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            # Collect nodes for expansion and evaluation\n",
    "            for ep in episodes:\n",
    "                ep.node = None # Reset the node marked for expansion and evaluation for each episode\n",
    "                node = ep.root\n",
    "                # Selection all the way down till a leaf node\n",
    "                while not node.is_leaf:\n",
    "                    node = self.select(node)\n",
    "\n",
    "                # Evaluate the leaf node\n",
    "                value, is_terminal = ep.game.get_value_and_terminated(node.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    self.backpropagate(node, value)\n",
    "                else:\n",
    "                    ep.node = node # Mark the leaf node for expansion and evaluation\n",
    "\n",
    "            # Batch query the model for the policy and value\n",
    "            expandable_episodes = [ep_idx for ep_idx, ep in enumerate(episodes) if ep.node is not None]\n",
    "\n",
    "            if len(expandable_episodes) > 0:\n",
    "                obs = np.stack([episodes[ep_idx].game.get_encoded_observation(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                scalar_features = np.stack([episodes[ep_idx].game.get_encoded_scalar_features(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                tensor_obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "                tensor_scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.model(\n",
    "                    tensor_obs, tensor_scalar_features\n",
    "                    )\n",
    "                \n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "            \n",
    "            # Expand the nodes and backpropagate\n",
    "            for batch_idx, ep_idx in enumerate(expandable_episodes):\n",
    "                node = episodes[ep_idx].node\n",
    "                ep_policy, ep_value = policy[batch_idx], value[batch_idx].item()\n",
    "\n",
    "                valid_policy = np.zeros_like(ep_policy)\n",
    "                valid_policy[node.valid_actions] = ep_policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy, game=episodes[ep_idx].game)\n",
    "                self.backpropagate(node, ep_value)\n",
    "    \n",
    "    def query_model(self, state: Maze.State, game: Maze) -> Tuple[np.ndarray, float]:\n",
    "        tensor_obs = torch.tensor(game.get_encoded_observation(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        tensor_scalar_features = torch.tensor(game.get_encoded_scalar_features(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_scalar_features\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        normalized_policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return normalized_policy, value\n",
    "\n",
    "    def select(self, node: Node) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = child.value_sum / child.visit_count\n",
    "        u_value = self.cfg.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "\n",
    "        return q_value + u_value\n",
    "\n",
    "    \n",
    "    def expand(self, node: Node, policy, game: Maze) -> None:\n",
    "        _, is_terminal = game.get_value_and_terminated(node.state)\n",
    "        assert not is_terminal, \"Cannot expand a terminal node\"\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        \n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model: ResNet, optimizer, search_alg: AlphaMCTS, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "\n",
    "        self.cfg = cfg.learn\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def self_play(self):\n",
    "        # For now train on a fixed size maze\n",
    "        # Initialize all games and an episode for each game\n",
    "        episodes = [GameEpisode(\n",
    "                            Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "                        ) for _ in range(self.cfg.num_parallel_games)\n",
    "                    ]\n",
    "        ret_mem = []\n",
    "        n_successes = 0\n",
    "        while len(episodes) > 0:\n",
    "            \n",
    "            self.search_alg.batch_search(episodes)\n",
    "\n",
    "            # Serially process the episodes\n",
    "            for i in range(len(episodes))[::-1]:\n",
    "                ep = episodes[i]\n",
    "\n",
    "                action_probs = np.zeros(ep.game.action_size)\n",
    "                for child in ep.root.children:\n",
    "                    action_probs[child.last_action] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                ep.memory.append((ep.game.get_encoded_observation(ep.root.state), \n",
    "                                  ep.game.get_encoded_scalar_features(ep.root.state),\n",
    "                                  action_probs))\n",
    "\n",
    "                action = np.random.choice(ep.game.action_size, p=action_probs)\n",
    "                for child in ep.root.children:\n",
    "                    if child.last_action == action:\n",
    "                        # Set the child as the new root to preserve the search tree\n",
    "                        ep.root = child\n",
    "                        break\n",
    "                ep.state = ep.root.state\n",
    "\n",
    "                value, is_terminal = ep.game.get_value_and_terminated(ep.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for mem in ep.memory:\n",
    "                        ret_mem.append((*mem, value))\n",
    "                    if (ep.state.x, ep.state.y) == ep.game.target:\n",
    "                        n_successes += 1\n",
    "                    del episodes[i]\n",
    "        \n",
    "        return ret_mem, n_successes\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.cfg.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.cfg.train_batch_size]\n",
    "            obs, scalar_features, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, scalar_features, policy_targets, value_targets = np.array(obs), np.array(scalar_features), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "            scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, scalar_features)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log metrics for the current batch\n",
    "                wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.cfg.train_batch_size)\n",
    "        if self.cfg.use_wandb:\n",
    "            # Log average loss for the epoch\n",
    "            wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self, save_every=1):\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "                name=cfg.name,\n",
    "                config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "                save_code=True)\n",
    "            \n",
    "            wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        for iteration in range(self.cfg.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "            for _ in trange(self.cfg.num_self_play_iters // self.cfg.num_parallel_games):\n",
    "                batch_episode_mems, num_episode_successes = self.self_play()\n",
    "                successes += num_episode_successes\n",
    "                memory += batch_episode_mems\n",
    "\n",
    "            success_rate = successes / self.cfg.num_self_play_iters\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log the success rate for self-play games\n",
    "                wandb.log({\"success_rate\": success_rate, \"iteration\": iteration})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.cfg.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            # Save if iter divides save_every or if it is the last iteration\n",
    "            if (iteration % save_every == 0 and iteration != 0) or iteration == self.cfg.num_learn_iters - 1:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "                if self.cfg.use_wandb:\n",
    "                    # Log model checkpoint to W&B\n",
    "                    wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "                    wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "maze.visualize_path()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "# model.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_model_7.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr, weight_decay=cfg.learn.weight_decay)\n",
    "# optimizer.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_optimizer_7.pt\"))\n",
    "\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, seed=0)\n",
    "alphaZero.learn(save_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without GPU it took 4m 33s to run,\n",
    "With GPU it surprisingly took 5m 24s to run. Since our observation matrices are small I don't expect GPU support to make a huge difference. Perhaps it's also introducing some overhead?\n",
    "\n",
    "With parallelization of the self play (num parallel = 100) it took 1m 5s, (without wandb took 44s)\n",
    "(num parallel= 250) took 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached target in 1 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332471/3545944966.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: (1, 1), policy: [0.548 0.    0.    0.452], policy argmax:Down policy value: 0.9968104958534241\n",
      "search: [0.551 0.    0.    0.449], search argmax: Down\n",
      "Position: (1, 2), policy: [0.    0.006 0.    0.994], policy argmax:Right policy value: 0.9963508248329163\n",
      "search: [0. 0. 0. 1.], search argmax: Right\n",
      "Position: (2, 1), policy: [0.994 0.    0.005 0.   ], policy argmax:Down policy value: 0.9954111576080322\n",
      "search: [1. 0. 0. 0.], search argmax: Down\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{100}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_round3_model_13.pt\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "_ = mcts.play_game(game=maze)\n",
    "\n",
    "positions = [(x, y) for x in range(1, cfg.maze.width-1) for y in range(1, cfg.maze.height-1)]\n",
    "for pos in positions:\n",
    "    if pos == maze.target:\n",
    "        continue\n",
    "    state = Maze.State(*pos, 1, 0)\n",
    "    policy, value = mcts.query_model(state, game=maze)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {value}\")\n",
    "    search_probs = mcts.search(game=maze, state=state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
