{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [torch.load(torch.load(f"../checkpoints
    {
     "name": "stdout",
     "output_type": "strtorch.load(f"../checkpointsnts/
     "text": [torch.loadtorch.load(f"../checkpoints
      "2.1.3\n",torch.torch.load(f"../checkpoints
      "2.5.1+cu124\n"torch.loatorch.load(f"../checkpoints
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# Set precision to 3 decimal places\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the plateau in performance at mazes with max size 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"maze_4to100_curriculum\",\n",
    "    \"maze\": {\n",
    "        \"width\": {\"min\": 4, \"max\": 100},\n",
    "        \"height\": {\"min\": 4, \"max\": 100},\n",
    "        \"cell_occupancy_prob\": {\"min\": 0, \"max\": 0.3},\n",
    "        \"max_steps\": \"L1SourceTarget\", # Use this to set the max steps to the L1 distance between source and target * 2\n",
    "        # To set paramters to constant values, use a float\n",
    "        # \"width\": 4,\n",
    "        # \"height\": 4,\n",
    "        # \"cell_occupancy_prob\": 0,\n",
    "        # \"max_steps\": 5, \n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 100,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_parallel_games\": 250,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"save_every\": 30,\n",
    "        \"use_wandb\": True,\n",
    "        # \"load_checkpoint\": \"maze_4to10_rtg_model_99\",\n",
    "        \"use_curriculum\": True,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that the reward stored in the state is unnormalized\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 100\n",
    "    MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -50\n",
    "\n",
    "    def __init__(self, width: int, height: int, cell_occupancy_prob: float = 0.3,seed: Optional[int] = None):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = int(width)\n",
    "        self.height = int(height)\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "\n",
    "        # Max steps configuration\n",
    "        # Option 1: Set the max steps to be the width * height\n",
    "        # self.max_steps=width*height\n",
    "        if cfg.maze.max_steps == \"L1SourceTarget\":\n",
    "            # Option 2: Set the max steps to be 2 * the L1 distance between source and target\n",
    "            self.max_steps = 2 * (abs(self.source[0] - self.target[0]) + abs(self.source[1] - self.target[1]))\n",
    "        elif type(cfg.maze.max_steps) == int:\n",
    "            # Option 3: Manually set the max steps\n",
    "            self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "    @classmethod\n",
    "    def generate_maze_params(cls, num_mazes:int, maze_cfg, seed: Optional[int]=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        maze_params = []\n",
    "        for param_name in ['width', 'height', 'cell_occupancy_prob']:\n",
    "            param = getattr(maze_cfg, param_name)\n",
    "            if isinstance(param, (float, int)):\n",
    "                values = np.full(num_mazes, param)\n",
    "            elif isinstance(param, dict) or isinstance(param, DictConfig) and 'min' in param and 'max' in param:\n",
    "                min_val, max_val = param['min'], param['max']\n",
    "                if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                    # Assumes that if the min and max are integers we want all integers\n",
    "                    values = np.random.randint(min_val, max_val + 1, size=num_mazes)\n",
    "                else:\n",
    "                    values = np.random.uniform(min_val, max_val, size=num_mazes)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid parameter configuration: {param}\")\n",
    "            maze_params.append(values)\n",
    "\n",
    "        # Combine into a single n x 3 array\n",
    "        maze_params = np.column_stack(maze_params)\n",
    "        return maze_params\n",
    "\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "\n",
    "        # Make sure that any number that is not 1 is 0\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "        return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "\n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "    \n",
    "    def get_normalized_steps_left(self, state: State):\n",
    "        return state.steps_left / self.max_steps\n",
    "    \n",
    "    def get_normalized_distances(self):\n",
    "        # Returns the normalized distances in the x and y directions that can be travelled by the agent in 50% of the max steps\n",
    "        scaling_factor = 0.5\n",
    "\n",
    "        return (self.max_steps * scaling_factor / self.width, self.max_steps * scaling_factor / self.height)\n",
    "    \n",
    "    def get_encoded_scalar_features(self, state: State):\n",
    "        return (\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            *self.get_normalized_target_position(),\n",
    "            self.get_normalized_steps_left(state),\n",
    "            *self.get_normalized_distances()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        \"\"\"Returns the unnormalized reward and whether the episode is terminated\"\"\"\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "        return state.reward, False\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        # Normalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 2 * ((reward - min_reward) / (max_reward - min_reward)) - 1\n",
    "    \n",
    "    def unnormalize_reward(self, normalized_reward):\n",
    "        # Unnormalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 0.5 * (normalized_reward + 1) * (max_reward - min_reward) + min_reward\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters, device):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        SCALAR_FEATURES_SIZE = 7  # see Maze.get_encoded_scalar_features\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The single input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + SCALAR_FEATURES_SIZE, 256),  # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + SCALAR_FEATURES_SIZE, 256), # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, scalar_features):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # scalar_features: (batch_size, 7), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, scalar_features], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, scalar_features], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Search node in the MCTS tree\"\"\"\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "class GameEpisode:\n",
    "    \"\"\"Stateful episode of a game\"\"\"\n",
    "    def __init__(self, game: Maze):\n",
    "        self.game = game\n",
    "        self.state: Maze.State = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.reward_history = []\n",
    "        self.root: Optional[Node] = Node(self.state, self.game.get_valid_actions(self.state))\n",
    "        self.node: Optional[Node] = None\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, search_cfg, model: ResNet):\n",
    "        self.cfg = search_cfg\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, game: Maze, max_iters = 1000, verbose=True, visualize=True):\n",
    "        \"\"\"Play a single game\"\"\"\n",
    "        state = game.get_initial_state()\n",
    "        path = []\n",
    "        root = Node(state, game.get_valid_actions(state))\n",
    "        for i in range(max_iters):\n",
    "            action_probs = self.search(game, root=root)\n",
    "            path.append((state.x, state.y))\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            for child in root.children:\n",
    "                if child.last_action == action:\n",
    "                    # Set the child as the new root to preserve the search tree\n",
    "                    root = child\n",
    "                    break\n",
    "            state = root.state\n",
    "            \n",
    "            value, is_terminal = game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    game.visualize_path(path)\n",
    "                \n",
    "                return path, value\n",
    "                \n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, game: Maze, state: Optional[Maze.State] = None, root: Optional[Node] = None) -> np.ndarray:\n",
    "        if root is None and state is not None:\n",
    "            root = Node(state, game.get_valid_actions(state))\n",
    "        elif state is None and root is None:\n",
    "            assert False, \"Either state or root must be provided\"\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node, game)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(state=node.state, game=game)\n",
    "                value = game.unnormalize_reward(value)\n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy, game=game)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batch_search(self, episodes: List[GameEpisode]):\n",
    "        \n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            # Collect nodes for expansion and evaluation\n",
    "            for ep in episodes:\n",
    "                ep.node = None # Reset the node marked for expansion and evaluation for each episode\n",
    "                node = ep.root\n",
    "                # Selection all the way down till a leaf node\n",
    "                while not node.is_leaf:\n",
    "                    node = self.select(node, ep.game)\n",
    "\n",
    "                # Evaluate the leaf node\n",
    "                value, is_terminal = ep.game.get_value_and_terminated(node.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    self.backpropagate(node, value)\n",
    "                else:\n",
    "                    ep.node = node # Mark the leaf node for expansion and evaluation\n",
    "\n",
    "            # Batch query the model for the policy and value\n",
    "            expandable_episodes = [ep_idx for ep_idx, ep in enumerate(episodes) if ep.node is not None]\n",
    "\n",
    "            if len(expandable_episodes) > 0:\n",
    "                obs = np.stack([episodes[ep_idx].game.get_encoded_observation(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                scalar_features = np.stack([episodes[ep_idx].game.get_encoded_scalar_features(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                tensor_obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "                tensor_scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.model(\n",
    "                    tensor_obs, tensor_scalar_features\n",
    "                    )\n",
    "                \n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "            \n",
    "            # Expand the nodes and backpropagate\n",
    "            for batch_idx, ep_idx in enumerate(expandable_episodes):\n",
    "                node = episodes[ep_idx].node\n",
    "                ep_policy, ep_value = policy[batch_idx], value[batch_idx].item()\n",
    "\n",
    "                valid_policy = np.zeros_like(ep_policy)\n",
    "                valid_policy[node.valid_actions] = ep_policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                ep_value = ep.game.unnormalize_reward(ep_value)\n",
    "                self.expand(node, policy=valid_policy, game=episodes[ep_idx].game)\n",
    "                self.backpropagate(node, ep_value)\n",
    "    \n",
    "    def query_model(self, state: Maze.State, game: Maze) -> Tuple[np.ndarray, float]:\n",
    "        tensor_obs = torch.tensor(game.get_encoded_observation(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        tensor_scalar_features = torch.tensor(game.get_encoded_scalar_features(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_scalar_features\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        normalized_policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return normalized_policy, value\n",
    "\n",
    "    def select(self, node: Node, game: Maze) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child, game) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node, game: Maze) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # Q-value needs to be noramalized between -1 and 1 for this formula.\n",
    "            q_value = game.normalize_reward(child.value_sum / child.visit_count)\n",
    "\n",
    "        u_value = self.cfg.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        \n",
    "        return q_value + u_value\n",
    "\n",
    "    \n",
    "    def expand(self, node: Node, policy, game: Maze) -> None:\n",
    "        _, is_terminal = game.get_value_and_terminated(node.state)\n",
    "        assert not is_terminal, \"Cannot expand a terminal node\"\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        \n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        \"\"\"Takes in unnormalized value\"\"\"\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model: ResNet, optimizer, search_alg: AlphaMCTS, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "\n",
    "        self.cfg = cfg.learn\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.last_success_rate = None\n",
    "        self.last_maze_cfg = None\n",
    "        self.original_maze_cfg = cfg.maze\n",
    "    \n",
    "    def get_maze_cfg_from_curriculum(self):\n",
    "        if self.last_success_rate is None:\n",
    "            maze_cfg = copy.deepcopy(self.original_maze_cfg)\n",
    "            maze_cfg.width.max = 6\n",
    "            maze_cfg.height.max = 6\n",
    "            return maze_cfg\n",
    "\n",
    "        if self.last_success_rate >= 0.9:\n",
    "            # Increase the maze size\n",
    "            maze_cfg = copy.deepcopy(self.last_maze_cfg)\n",
    "            maze_cfg.width.max = min(self.last_maze_cfg.width.max + 2, self.original_maze_cfg.width.max)\n",
    "            maze_cfg.height.max = min(self.last_maze_cfg.height.max + 2, self.original_maze_cfg.height.max)\n",
    "        else:\n",
    "            # Keep the maze size the same\n",
    "            maze_cfg = self.last_maze_cfg\n",
    "        \n",
    "        return maze_cfg\n",
    "    \n",
    "    def self_play(self, maze_cfg):\n",
    "\n",
    "        maze_params = Maze.generate_maze_params(self.cfg.num_parallel_games, maze_cfg=maze_cfg)\n",
    "        episodes = [GameEpisode(Maze(*params)) for params in maze_params]\n",
    "        ret_mem = []\n",
    "        n_successes = 0\n",
    "        while len(episodes) > 0:\n",
    "            \n",
    "            self.search_alg.batch_search(episodes)\n",
    "\n",
    "            # Serially process the episodes\n",
    "            for i in range(len(episodes))[::-1]:\n",
    "                ep = episodes[i]\n",
    "\n",
    "                action_probs = np.zeros(ep.game.action_size)\n",
    "                for child in ep.root.children:\n",
    "                    action_probs[child.last_action] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                ep.memory.append((ep.game.get_encoded_observation(ep.root.state), \n",
    "                                  ep.game.get_encoded_scalar_features(ep.root.state),\n",
    "                                  action_probs))\n",
    "                ep.reward_history.append(ep.root.state.reward)\n",
    "\n",
    "                action = np.random.choice(ep.game.action_size, p=action_probs)\n",
    "                for child in ep.root.children:\n",
    "                    if child.last_action == action:\n",
    "                        # Set the child as the new root to preserve the search tree\n",
    "                        ep.root = child\n",
    "                        break\n",
    "                ep.state = ep.root.state\n",
    "\n",
    "                final_reward, is_terminal = ep.game.get_value_and_terminated(ep.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    # Unroll the reward history and memory\n",
    "                    for mem, reward_to_go in zip(ep.memory, ep.reward_history):\n",
    "                        reward_to_go = final_reward - reward_to_go\n",
    "                        ret_mem.append((*mem, ep.game.normalize_reward(reward_to_go)))\n",
    "                    if (ep.state.x, ep.state.y) == ep.game.target:\n",
    "                        n_successes += 1\n",
    "                    del episodes[i]\n",
    "        \n",
    "        return ret_mem, n_successes\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.cfg.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.cfg.train_batch_size]\n",
    "            obs, scalar_features, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, scalar_features, policy_targets, value_targets = np.array(obs), np.array(scalar_features), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "            scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, scalar_features)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log metrics for the current batch\n",
    "                wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.cfg.train_batch_size)\n",
    "        if self.cfg.use_wandb:\n",
    "            # Log average loss for the epoch\n",
    "            wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "                name=cfg.name,\n",
    "                config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "                save_code=True)\n",
    "            \n",
    "            wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for iteration in range(self.cfg.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "\n",
    "            # Initialize all games and an episode for each game\n",
    "            if self.cfg.use_curriculum:\n",
    "                maze_cfg = self.get_maze_cfg_from_curriculum()\n",
    "                wandb.log({\"max_maze_width\": maze_cfg.width.max})\n",
    "                self.last_maze_cfg = maze_cfg\n",
    "            else:\n",
    "                maze_cfg = cfg.maze\n",
    "\n",
    "            for _ in trange(self.cfg.num_self_play_iters // self.cfg.num_parallel_games):\n",
    "                batch_episode_mems, num_episode_successes = self.self_play(maze_cfg)\n",
    "                successes += num_episode_successes\n",
    "                memory += batch_episode_mems\n",
    "\n",
    "            success_rate = successes / self.cfg.num_self_play_iters\n",
    "            self.last_success_rate = success_rate\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log the success rate for self-play games\n",
    "                wandb.log({\"success_rate\": success_rate, \"iteration\": iteration, \"wall_time\": time.time() - start_time})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.cfg.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            # Save if iter divides save_every or if it is the last iteration\n",
    "            if (iteration % self.cfg.save_every == 0 and iteration != 0) or iteration == self.cfg.num_learn_iters - 1:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "                if self.cfg.use_wandb:\n",
    "                    # Log model checkpoint to W&B\n",
    "                    wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "                    wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "if \"load_checkpoint\" in cfg.learn:\n",
    "    # Replace \"optimizer\" with \"model\"\n",
    "    model_filename = cfg.learn.load_checkpoint.replace(\"optimizer\", \"model\")\n",
    "    model.load_state_dict(torch.load(f\"checkpoints/{model_filename}.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr, weight_decay=cfg.learn.weight_decay)\n",
    "if \"load_checkpoint\" in cfg.learn:\n",
    "    # Replace \"model\" with \"optimizer\"\n",
    "    model_filename = cfg.learn.load_checkpoint.replace(\"model\", \"optimizer\")\n",
    "    optimizer.load_state_dict(torch.load(f\"checkpoints/{model_filename}.pt\"))\n",
    "\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, seed=0)\n",
    "# alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_450208/3530624854.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"checkpoints/maze_4to100_curriculum_last3_97p_model_149.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached target in 33 steps\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(40, 18), (41, 18), (42, 18), (42, 17), (43, 17), (44, 17), (44, 16), (44, 15), (44, 14), (44, 13), (44, 12), (44, 11), (45, 11), (46, 11), (47, 11), (47, 10), (47, 9), (48, 9), (48, 8), (48, 7), (48, 6), (48, 5), (48, 6), (48, 5), (48, 6), (48, 5), (48, 4), (48, 5), (48, 6), (48, 7), (48, 6), (48, 7), (48, 8), (47, 8)], 67)\n"
     ]
    }
   ],
   "source": [
    "# params = Maze.generate_maze_params(1, cfg)\n",
    "params = np.array([[50, 50, 0.2]])\n",
    "width, height, cell_occupancy_prob = params[0]\n",
    "width, height = int(width), int(height)\n",
    "maze = Maze(*params[0])\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{100}.pt\"))\n",
    "model.load_state_dict(torch.load(f\"checkpoints/maze_4to100_curriculum_last3_97p_model_149.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# new_cfg = copy.deepcopy(cfg)\n",
    "# new_cfg.search.num_simulations = 100\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "print(mcts.play_game(game=maze))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: (31, 28)\n",
      "Position: (30, 27), policy: [0.328 0.096 0.09  0.486], policy argmax:Right policy value: -0.32634344696998596\n",
      "search: [0.327 0.082 0.082 0.51 ], search argmax: Right\n",
      "Position: (32, 27), policy: [0.    0.    0.089 0.911], policy argmax:Right policy value: -0.7954446077346802\n",
      "search: [0.    0.    0.082 0.918], search argmax: Right\n",
      "Position: (32, 29), policy: [0.21  0.    0.487 0.303], policy argmax:Left policy value: 0.7563819885253906\n",
      "search: [0.204 0.    0.49  0.306], search argmax: Left\n"
     ]
    }
   ],
   "source": [
    "\n",
    "positions = [(x, y) for x in range(1, width-1) for y in range(1, height-1)]\n",
    "\n",
    "# Positions around target:\n",
    "positions = [(maze.target[0] + dx, maze.target[1] + dy) for dx in [-1, 1] for dy in [-1, 1]]\n",
    "print(f\"Target: {maze.target}\")\n",
    "for pos in positions:\n",
    "    if pos == maze.target or maze.map[pos] == 1:\n",
    "        continue\n",
    "    state = Maze.State(*pos, 1, 0)\n",
    "    policy, final_reward = mcts.query_model(state, game=maze)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {final_reward}\")\n",
    "    search_probs = mcts.search(game=maze, state=state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5 %\n"
     ]
    }
   ],
   "source": [
    "num_insufficient_paths = 0\n",
    "total = 1000\n",
    "for i in range(total):\n",
    "    params = np.array([[10, 10, 0.3]])\n",
    "    width, height, cell_occupancy_prob = params[0]\n",
    "    width, height = int(width), int(height)\n",
    "    maze = Maze(*params[0])\n",
    "\n",
    "    # maze.visualize_path()\n",
    "    if len(maze.shortest_path) > maze.max_steps:\n",
    "        num_insufficient_paths += 1\n",
    "    # print(f\"optimal path length: {len(maze.shortest_path)} max steps: {maze.max_steps}\")\n",
    "print(num_insufficient_paths*100/total, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just realized that the max length isn't long enough for some percentage of the generated paths (higher when the cell occupancy probability is higher). I'm going to try changing the max path length to be 2 times the shortest path and see if that fixes this problem.\n",
    "\n",
    "Also I realized that at 0.3 cell occupancy probability, some of the mazes are really hard and require significant detours, I think that implementing a history might allow the policy to overcome these challenging mazes and lead it not to get stuck in deadends\n",
    "\n",
    "I’m going to do a run with the cell probability between 0 and 0.1 to check if it can solve up to 100 by 100 for that easier problem, then I’m going to try adding in history and see if it can solve harder problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
