{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"maze_4x4_binaryreward_maxsteps2\",\n",
    "    \"maze\": {\n",
    "        \"width\": 4,\n",
    "        \"height\": 4,\n",
    "        \"cell_occupancy_prob\": 0,\n",
    "        \"max_steps\": 2\n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 8,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 1\n",
    "    # MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -1\n",
    "\n",
    "    def __init__(self, width: int, height: int, seed: Optional[int] = None, cell_occupancy_prob: float = 0.3):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "\n",
    "        # self.max_steps=width*height\n",
    "        # For this simplest possible maze, set the max length to be 2\n",
    "        self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "        # In this maze, all the free space in the maze is observable from any position\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        # dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        dr = 0\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "\n",
    "        # Make sure that any number that is not 1 is 0\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "\n",
    "        return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "        # # Plane 1: Free Space (1s where free space, 0s where obstacles)\n",
    "        # plane_free_space = plane_obstacles == 0\n",
    "\n",
    "        # # Plane 2: Agent's position\n",
    "        # plane_agent = np.zeros_like(plane_obstacles)\n",
    "        # plane_agent[half_width, half_width] = 1\n",
    "\n",
    "        # encoded_observation = np.stack([plane_obstacles, plane_free_space, plane_agent], axis=0)\n",
    "\n",
    "        # return encoded_observation\n",
    "    \n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        # In this case we are using binary reward\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "    \n",
    "        return state.reward, False\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            # Set the max steps to be 3 * the L1 distance between source and target\n",
    "            # self.max_steps = 3 * (abs(source[0] - target[0]) + abs(source[1] - target[1]))\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The single input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + 4, 256),  # Adding 4 for the positions of agent and target\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + 4, 256),  # Adding 4 for the positions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, agent_pos, goal_pos):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # agent_pos and goal_pos: tensors of shape (batch_size, 2), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, agent_pos, goal_pos], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, agent_pos, goal_pos], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, game: Maze, num_simulations, c_puct, model):\n",
    "        self.game = game\n",
    "        self.num_simulations = num_simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, max_iters = 1000, verbose=True, visualize=True):\n",
    "        state = self.game.get_initial_state()\n",
    "        path = []\n",
    "        memory = []\n",
    "        for i in range(max_iters):\n",
    "            \n",
    "            action_probs = self.search(state)\n",
    "            path.append((state.x, state.y))\n",
    "            memory.append((self.game.get_encoded_observation(state), \n",
    "                           self.game.get_normalized_agent_position(state), \n",
    "                           self.game.get_normalized_target_position(),\n",
    "                           action_probs))\n",
    "\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            state = self.game.get_next_state(state, action)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                ret_mem = [(*mem, value) for mem in memory]\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == self.game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    self.game.visualize_path(path)\n",
    "                \n",
    "                return ret_mem\n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(state, self.game.get_valid_actions(state))\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(node.state)\n",
    "                \n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    def query_model(self, state: Maze.State):\n",
    "        tensor_obs = torch.tensor(self.game.get_encoded_observation(state), dtype=torch.float32).unsqueeze(0)\n",
    "        tensor_agent_pos = torch.tensor(self.game.get_normalized_agent_position(state), dtype=torch.float32).unsqueeze(0)\n",
    "        tensor_goal_pos = torch.tensor(self.game.get_normalized_target_position(), dtype=torch.float32).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_agent_pos, tensor_goal_pos\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        # policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        policy = policy.squeeze(0).detach().cpu().numpy()\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "    def select(self, node: Node) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = child.value_sum / child.visit_count\n",
    "        u_value = self.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        return q_value + u_value\n",
    "    \n",
    "    def expand(self, node: Node, policy) -> None:\n",
    "        if (node.state.x, node.state.y) == self.game.target:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = self.game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  self.game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, search_alg: AlphaMCTS, num_learn_iters, num_self_play_iters, num_train_epochs, train_batch_size, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "        self.num_learn_iters = num_learn_iters\n",
    "        self.num_self_play_iters = num_self_play_iters\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.train_batch_size = train_batch_size\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def self_play(self):\n",
    "        # Initialize game\n",
    "        # For now train on a fixed size maze\n",
    "        game = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "        self.search_alg.game = game\n",
    "        return self.search_alg.play_game(verbose=False, visualize=False)\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.train_batch_size]\n",
    "            obs, agent_pos, goal_pos, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, agent_pos, goal_pos, policy_targets, value_targets = np.array(obs), np.array(agent_pos), np.array(goal_pos), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            agent_pos = torch.tensor(agent_pos, dtype=torch.float32)\n",
    "            goal_pos = torch.tensor(goal_pos, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, agent_pos, goal_pos)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Log metrics for the current batch\n",
    "            wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.train_batch_size)\n",
    "        # Log average loss for the epoch\n",
    "        wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "            name=cfg.name,\n",
    "            config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "            save_code=True)\n",
    "        \n",
    "        wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        for iteration in range(self.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "            for _ in trange(self.num_self_play_iters):\n",
    "                game_mem = self.self_play()\n",
    "                if game_mem[-1][-1] > 0:  # Assuming positive value means successful game\n",
    "                    successes += 1\n",
    "                memory += game_mem\n",
    "\n",
    "            success_rate = successes / self.num_self_play_iters\n",
    "            # Log the success rate for self-play games\n",
    "            wandb.log({\"success_rate\": success_rate, \"iteration\": iteration})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "            # Log model checkpoint to W&B\n",
    "            wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "            wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr)\n",
    "\n",
    "mcts = AlphaMCTS(maze, num_simulations=cfg.search.num_simulations, c_puct=cfg.search.c_puct, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, \n",
    "                      num_learn_iters=cfg.learn.num_learn_iters, \n",
    "                      num_self_play_iters=cfg.learn.num_self_play_iters,\n",
    "                      num_train_epochs=cfg.learn.num_train_epochs,\n",
    "                      train_batch_size=cfg.learn.train_batch_size)\n",
    "# alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached target in 1 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255286/305530079.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiu0lEQVR4nO3dbWxUZf7/8c8U6FQiM7Ur7bRSUCwWuYcqMJhAXauIhNh9oqKRygLepCSyGoVu3EVxd2d35bdqDALGYHdVgrdAgorWIhChoFQay43Eug1F0ykqMgNVR2yv/wP/zlppSwtzZqZX36/k+2DOXNc53x4P+XhmzpnjMsYYAQBgsZRENwAAgNMIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUcC7tjx47ptttuk8fjUXp6uubNm6eTJ092OqewsFAul6tN3X333U61CADoJVxO/TbmjBkz1NjYqNWrV+vUqVOaO3eurrzySq1du7bDOYWFhbrsssu0bNmy6LL+/fvL4/E40SIAoJfo68RKDx48qM2bN+vDDz/UFVdcIUl66qmndMMNN2j58uXKycnpcG7//v3l8/mcaAsA0Es5EnZVVVVKT0+PBp0kFRUVKSUlRbt379bvfve7Due++OKLeuGFF+Tz+TRr1iz96U9/Uv/+/TscH4lEFIlEoq9bW1t17Ngx/eY3v5HL5YrNHwQAiBtjjE6cOKGcnBylpMTm2zZHwi4YDCozM7Pthvr2VUZGhoLBYIfzbr31Vg0ZMkQ5OTn6+OOPtXjxYh06dEivv/56h3MCgYAeeeSRmPUOAEgOR44c0aBBg2KzMtMNixcvNpI6rYMHD5q//vWv5rLLLjtt/sCBA83TTz/d5e1VVlYaSaaurq7DMd9//70JhULRamhoOGOPFEVRVPLX8ePHuxNRnerWmd3999+vO+64o9MxQ4cOlc/n09GjR9ss//HHH3Xs2LFufR83adIkSVJdXZ0uvfTSdse43W653e4urxMA0DPE8quoboXdwIEDNXDgwDOO8/v9On78uKqrq1VQUCBJ2rJli1pbW6MB1hU1NTWSpOzs7O60CQBAWzE7R/yV66+/3owfP97s3r3bvP/++2bYsGFm9uzZ0fc///xzk5+fb3bv3m2MMaaurs4sW7bM7Nmzx9TX15uNGzeaoUOHmqlTp3Zru6FQKOGn3hRFUdS5VygUilkmORZ2X3/9tZk9e7Y5//zzjcfjMXPnzjUnTpyIvl9fX28kmffee88YY0xDQ4OZOnWqycjIMG632+Tl5ZkHHnig238sYUdRFGVHxTLsHLupPFHC4bC8Xm+i2wAAnKNQKBSzHxXhtzEBANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1nM87FasWKGLL75YaWlpmjRpkj744INOx7/yyisaPny40tLSNHr0aL355ptOtwgAsJ1x0Lp160xqaqpZs2aN2b9/v1mwYIFJT083TU1N7Y7fsWOH6dOnj/nnP/9pDhw4YB566CHTr18/U1tb2+VthkIhI4miKIrq4RUKhWIVR8bRsJs4caIpLS2Nvm5paTE5OTkmEAi0O/6mm24yM2fObLNs0qRJ5q677uryNgk7iqIoOyqWYefYx5g//PCDqqurVVRUFF2WkpKioqIiVVVVtTunqqqqzXhJmj59eofjJSkSiSgcDrcpAAB+ybGw++qrr9TS0qKsrKw2y7OyshQMBtudEwwGuzVekgKBgLxeb7Ryc3PPvXkAgFV6/NWYZWVlCoVC0Tpy5EiiWwIAJJm+Tq34wgsvVJ8+fdTU1NRmeVNTk3w+X7tzfD5ft8ZLktvtltvtPveGAQDWcuzMLjU1VQUFBaqsrIwua21tVWVlpfx+f7tz/H5/m/GSVFFR0eF4AAC6JGaXurRj3bp1xu12m/LycnPgwAFz5513mvT0dBMMBo0xxtx+++1myZIl0fE7duwwffv2NcuXLzcHDx40S5cu5dYDiqKoXlo95tYDY4x56qmnzODBg01qaqqZOHGi2bVrV/S9adOmmZKSkjbjX375ZXPZZZeZ1NRUM3LkSPPGG290a3uEHUVRlB0Vy7BzGWOMLBIOh+X1ehPdBgDgHIVCIXk8npisq8dfjQkAwJkQdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6zkeditWrNDFF1+stLQ0TZo0SR988EGHY8vLy+VyudpUWlqa0y0CACznaNi99NJLuu+++7R06VJ99NFHGjt2rKZPn66jR492OMfj8aixsTFahw8fdrJFAEAv4GjY/etf/9KCBQs0d+5cjRgxQqtWrVL//v21Zs2aDue4XC75fL5oZWVlOdkiAKAX6OvUin/44QdVV1errKwsuiwlJUVFRUWqqqrqcN7Jkyc1ZMgQtba2asKECfrb3/6mkSNHdjg+EokoEolEX4fD4dj8AegyY0yiW+h9XK5Ed9CrsLd7PsfO7L766iu1tLScdmaWlZWlYDDY7pz8/HytWbNGGzdu1AsvvKDW1lZNmTJFn3/+eYfbCQQC8nq90crNzY3p3wEA6PmS6mpMv9+vOXPmaNy4cZo2bZpef/11DRw4UKtXr+5wTllZmUKhULSOHDkSx44BAD2BYx9jXnjhherTp4+ampraLG9qapLP5+vSOvr166fx48errq6uwzFut1tut/ucegUA2M2xM7vU1FQVFBSosrIyuqy1tVWVlZXy+/1dWkdLS4tqa2uVnZ3tVJsAgF7AsTM7SbrvvvtUUlKiK664QhMnTtQTTzyh5uZmzZ07V5I0Z84cXXTRRQoEApKkZcuWafLkycrLy9Px48f12GOP6fDhw5o/f76TbQIALOdo2N1888368ssv9ec//1nBYFDjxo3T5s2boxetNDQ0KCXlfyeX33zzjRYsWKBgMKgLLrhABQUF2rlzp0aMGOFkmwAAy7mMZdeNh8Nheb3eRLfRq1h2CPUM3HoQV+ztxAiFQvJ4PDFZV1JdjQkAgBMIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1HA277du3a9asWcrJyZHL5dKGDRvOOGfr1q2aMGGC3G638vLyVF5e7mSLAIBewNGwa25u1tixY7VixYouja+vr9fMmTN19dVXq6amRosWLdL8+fP19ttvO9kmAMByLmOMicuGXC6tX79excXFHY5ZvHix3njjDe3bty+67JZbbtHx48e1efPmdudEIhFFIpHo63A4rNzc3Jj1jTOL0yGEX3K5Et1Br8LeToxQKCSPxxOTdSXVd3ZVVVUqKipqs2z69OmqqqrqcE4gEJDX640WQQcA+LWkCrtgMKisrKw2y7KyshQOh/Xdd9+1O6esrEyhUChaR44ciUerAIAepG+iGzhXbrdbbrc70W0AAJJYUp3Z+Xw+NTU1tVnW1NQkj8ej8847L0FdAQB6uqQKO7/fr8rKyjbLKioq5Pf7E9QRAMAGjobdyZMnVVNTo5qaGkk/3VpQU1OjhoYGST993zZnzpzo+Lvvvlv//e9/9eCDD+qTTz7R008/rZdffll/+MMfnGwTAGA746D33nvPSDqtSkpKjDHGlJSUmGnTpp02Z9y4cSY1NdUMHTrUPPfcc93aZigUaneblHOFBJCoOFai/4311gqFQjH7JxO3++ziJRwOy+v1JrqNXsWyQ6hn4D67uGJvJ4a199kBAOAEwg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD1Hw2779u2aNWuWcnJy5HK5tGHDhk7Hb926VS6X67QKBoNOtgkAsJyjYdfc3KyxY8dqxYoV3Zp36NAhNTY2RiszM9OhDgEAvUFfJ1c+Y8YMzZgxo9vzMjMzlZ6e3qWxkUhEkUgk+jocDnd7ewAAuzkadmdr3LhxikQiGjVqlB5++GFdddVVHY4NBAJ65JFH4tgdkHiuhxPdQS/zcKIbwLlKqgtUsrOztWrVKr322mt67bXXlJubq8LCQn300UcdzikrK1MoFIrWkSNH4tgxAKAnSKozu/z8fOXn50dfT5kyRZ999pkef/xxPf/88+3Ocbvdcrvd8WoRANADJdWZXXsmTpyourq6RLcBAOjBkj7sampqlJ2dneg2AAA9mKMfY548ebLNWVl9fb1qamqUkZGhwYMHq6ysTF988YX+85//SJKeeOIJXXLJJRo5cqS+//57Pfvss9qyZYveeecdJ9sEAFjO0bDbs2ePrr766ujr++67T5JUUlKi8vJyNTY2qqGhIfr+Dz/8oPvvv19ffPGF+vfvrzFjxujdd99tsw4AALrLZYwxiW4ilsLhsLxeb6Lb6FUsO4R6BNcjrkS30Ls8nOgGeqdQKCSPxxOTdSX9d3YAAJwrwg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD1Hwy4QCOjKK6/UgAEDlJmZqeLiYh06dOiM81555RUNHz5caWlpGj16tN58800n2wQAWM7RsNu2bZtKS0u1a9cuVVRU6NSpU7ruuuvU3Nzc4ZydO3dq9uzZmjdvnvbu3avi4mIVFxdr3759TrYKALCYyxhj4rWxL7/8UpmZmdq2bZumTp3a7pibb75Zzc3N2rRpU3TZ5MmTNW7cOK1ateqM2wiHw/J6vTHrGWcWx0MI/5/rEVeiW+hdHk50A71TKBSSx+OJybri+p1dKBSSJGVkZHQ4pqqqSkVFRW2WTZ8+XVVVVe2Oj0QiCofDbQoAgF+KW9i1trZq0aJFuuqqqzRq1KgOxwWDQWVlZbVZlpWVpWAw2O74QCAgr9cbrdzc3Jj2DQDo+eIWdqWlpdq3b5/WrVsX0/WWlZUpFApF68iRIzFdPwCg5+sbj40sXLhQmzZt0vbt2zVo0KBOx/p8PjU1NbVZ1tTUJJ/P1+54t9stt9sds14BAPZx9MzOGKOFCxdq/fr12rJliy655JIzzvH7/aqsrGyzrKKiQn6/36k2AQCWc/TMrrS0VGvXrtXGjRs1YMCA6PduXq9X5513niRpzpw5uuiiixQIBCRJ9957r6ZNm6b/+7//08yZM7Vu3Trt2bNHzzzzjJOtAgAs5uiZ3cqVKxUKhVRYWKjs7OxovfTSS9ExDQ0NamxsjL6eMmWK1q5dq2eeeUZjx47Vq6++qg0bNnR6UQsAAJ2J63128cB9dvFn2SHUI3CfXZw9nOgGeqcee58dAACJQNgBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKznaNgFAgFdeeWVGjBggDIzM1VcXKxDhw51Oqe8vFwul6tNpaWlOdkmAMByjobdtm3bVFpaql27dqmiokKnTp3Sddddp+bm5k7neTweNTY2Ruvw4cNOtgkAsFxfJ1e+efPmNq/Ly8uVmZmp6upqTZ06tcN5LpdLPp/PydYAAL2Io2H3a6FQSJKUkZHR6biTJ09qyJAham1t1YQJE/S3v/1NI0eObHdsJBJRJBKJvg6Hw7FrGF3icrkS3QIAdCpuF6i0trZq0aJFuuqqqzRq1KgOx+Xn52vNmjXauHGjXnjhBbW2tmrKlCn6/PPP2x0fCATk9XqjlZub69SfAADooVzGGBOPDd1zzz1666239P7772vQoEFdnnfq1Cldfvnlmj17th599NHT3m/vzI7AA4CeLxQKyePxxGRdcfkYc+HChdq0aZO2b9/eraCTpH79+mn8+PGqq6tr93232y232x2LNgEAlnL0Y0xjjBYuXKj169dry5YtuuSSS7q9jpaWFtXW1io7O9uBDgEAvYGjZ3alpaVau3atNm7cqAEDBigYDEqSvF6vzjvvPEnSnDlzdNFFFykQCEiSli1bpsmTJysvL0/Hjx/XY489psOHD2v+/PlOtgoAsJijYbdy5UpJUmFhYZvlzz33nO644w5JUkNDg1JS/neC+c0332jBggUKBoO64IILVFBQoJ07d2rEiBFOtgoAsFjcLlCJl3A4LK/Xm+g2AADnKJYXqPDbmAAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrORp2K1eu1JgxY+TxeOTxeOT3+/XWW291OueVV17R8OHDlZaWptGjR+vNN990skUAQC/gaNgNGjRIf//731VdXa09e/bot7/9rW688Ubt37+/3fE7d+7U7NmzNW/ePO3du1fFxcUqLi7Wvn37nGwTAGA5lzHGxHODGRkZeuyxxzRv3rzT3rv55pvV3NysTZs2RZdNnjxZ48aN06pVq7q0/nA4LK/XG7N+AQCJEQqF5PF4YrKuuH1n19LSonXr1qm5uVl+v7/dMVVVVSoqKmqzbPr06aqqqupwvZFIROFwuE0BAPBLjoddbW2tzj//fLndbt19991av369RowY0e7YYDCorKysNsuysrIUDAY7XH8gEJDX641Wbm5uTPsHAPR8joddfn6+ampqtHv3bt1zzz0qKSnRgQMHYrb+srIyhUKhaB05ciRm6wYA2KGv0xtITU1VXl6eJKmgoEAffvihnnzySa1evfq0sT6fT01NTW2WNTU1yefzdbh+t9stt9sd26YBAFaJ+312ra2tikQi7b7n9/tVWVnZZllFRUWH3/EBANAlxkFLliwx27ZtM/X19ebjjz82S5YsMS6Xy7zzzjvGGGNuv/12s2TJkuj4HTt2mL59+5rly5ebgwcPmqVLl5p+/fqZ2traLm8zFAoZSRRFUVQPr1AoFLM8cvRjzKNHj2rOnDlqbGyU1+vVmDFj9Pbbb+vaa6+VJDU0NCgl5X8nl1OmTNHatWv10EMP6Y9//KOGDRumDRs2aNSoUU62CQCwXNzvs3Ma99kBgB165H12AAAkCmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsJ6jYbdy5UqNGTNGHo9HHo9Hfr9fb731Vofjy8vL5XK52lRaWpqTLQIAeoG+Tq580KBB+vvf/65hw4bJGKN///vfuvHGG7V3716NHDmy3Tkej0eHDh2Kvna5XE62CADoBRwNu1mzZrV5/de//lUrV67Url27Ogw7l8sln8/X5W1EIhFFIpHo61AodHbNAgCSijEmZuuK23d2LS0tWrdunZqbm+X3+zscd/LkSQ0ZMkS5ubm68cYbtX///k7XGwgE5PV6ozV48OBYtw4ASICvv/46ZutymVhGZztqa2vl9/v1/fff6/zzz9fatWt1ww03tDu2qqpKn376qcaMGaNQKKTly5dr+/bt2r9/vwYNGtTunF+f2R0/flxDhgxRQ0ODvF6vI3+TE8LhsHJzc3XkyBF5PJ5Et9MtPbV3+o4v+o6/ntp7KBTS4MGD9c033yg9PT0m63T0Y0xJys/PV01NjUKhkF599VWVlJRo27ZtGjFixGlj/X5/m7O+KVOm6PLLL9fq1av16KOPtrt+t9stt9t92nKv19uj/uP+7OeLeXqinto7fccXfcdfT+09JSV2Hz46HnapqanKy8uTJBUUFOjDDz/Uk08+qdWrV59xbr9+/TR+/HjV1dU53SYAwGJxv8+utbW1zceOnWlpaVFtba2ys7Md7goAYDNHz+zKyso0Y8YMDR48WCdOnNDatWu1detWvf3225KkOXPm6KKLLlIgEJAkLVu2TJMnT1ZeXp6OHz+uxx57TIcPH9b8+fO7vE23262lS5e2+9FmMuupfUs9t3f6ji/6jr+e2rsTfTt6gcq8efNUWVmpxsZGeb1ejRkzRosXL9a1114rSSosLNTFF1+s8vJySdIf/vAHvf766woGg7rgggtUUFCgv/zlLxo/frxTLQIAegHHr8YEACDR+G1MAID1CDsAgPUIOwCA9Qg7AID1rAi7Y8eO6bbbbpPH41F6errmzZunkydPdjqnsLDwtMcJ3X333Y72uWLFCl188cVKS0vTpEmT9MEHH3Q6/pVXXtHw4cOVlpam0aNH680333S0v850p/dkeFTT9u3bNWvWLOXk5MjlcmnDhg1nnLN161ZNmDBBbrdbeXl50auE4627vW/duvW0/e1yuRQMBuPTsH76jdorr7xSAwYMUGZmpoqLi9s8vaQjiT7Gz6bvZDi+pe4/Qk1K/P6WEvfoNyvC7rbbbtP+/ftVUVGhTZs2afv27brzzjvPOG/BggVqbGyM1j//+U/HenzppZd03333aenSpfroo480duxYTZ8+XUePHm13/M6dOzV79mzNmzdPe/fuVXFxsYqLi7Vv3z7HeuxId3uXfvp5ol/u28OHD8exY6m5uVljx47VihUrujS+vr5eM2fO1NVXX62amhotWrRI8+fPj94TGk/d7f1nhw4darPPMzMzHerwdNu2bVNpaal27dqliooKnTp1Stddd52am5s7nJMMx/jZ9C0l/viW/vcIterqau3Zs0e//e1vO/3x/GTY32fTtxSj/W16uAMHDhhJ5sMPP4wue+utt4zL5TJffPFFh/OmTZtm7r333jh0+JOJEyea0tLS6OuWlhaTk5NjAoFAu+NvuukmM3PmzDbLJk2aZO666y5H+2xPd3t/7rnnjNfrjVN3ZybJrF+/vtMxDz74oBk5cmSbZTfffLOZPn26g52dWVd6f++994wk880338Slp644evSokWS2bdvW4ZhkOsZ/1pW+k+34/qULLrjAPPvss+2+l4z7+2ed9R2r/d3jz+yqqqqUnp6uK664IrqsqKhIKSkp2r17d6dzX3zxRV144YUaNWqUysrK9O233zrS4w8//KDq6moVFRVFl6WkpKioqEhVVVXtzqmqqmozXpKmT5/e4XinnE3vUvcf1ZRoybK/z8W4ceOUnZ2ta6+9Vjt27EhoLz8/VzIjI6PDMcm4z7vSt5R8x3dXHqGWjPvbqUe/tcfxH4J2WjAYPO3jmr59+yojI6PT7yxuvfVWDRkyRDk5Ofr444+1ePFiHTp0SK+//nrMe/zqq6/U0tKirKysNsuzsrL0ySeftDsnGAy2Oz6e38NIZ9d7fn6+1qxZ0+ZRTVOmTOn0UU2J1tH+DofD+u6773TeeeclqLMzy87O1qpVq3TFFVcoEono2WefVWFhoXbv3q0JEybEvZ/W1lYtWrRIV111lUaNGtXhuGQ5xn/W1b6T6fj+9SPU1q9f3+4TZaTk2t/d6TtW+ztpw27JkiX6xz/+0emYgwcPnvX6f/md3ujRo5Wdna1rrrlGn332mS699NKzXi/O7lFNOHv5+fnKz8+Pvp4yZYo+++wzPf7443r++efj3k9paan27dun999/P+7bPhdd7TuZju/uPEItmTj96Lf2JG3Y3X///brjjjs6HTN06FD5fL7TLpT48ccfdezYMfl8vi5vb9KkSZKkurq6mIfdhRdeqD59+qipqanN8qampg579Pl83RrvlLPp/dd6wqOaOtrfHo8nqc/qOjJx4sSEhM3ChQujF4md6f+6k+UYl7rX968l8vjuziPUkml/J+LRb0n7nd3AgQM1fPjwTis1NVV+v1/Hjx9XdXV1dO6WLVvU2toaDbCuqKmpkSRHHieUmpqqgoICVVZWRpe1traqsrKyw8+p/X5/m/GSVFFR0enn2k44m95/rSc8qilZ9nes1NTUxHV/G2O0cOFCrV+/Xlu2bNEll1xyxjnJsM/Ppu9fS6bju7NHqCXD/u5IXB79ds6XuCSB66+/3owfP97s3r3bvP/++2bYsGFm9uzZ0fc///xzk5+fb3bv3m2MMaaurs4sW7bM7Nmzx9TX15uNGzeaoUOHmqlTpzrW47p164zb7Tbl5eXmwIED5s477zTp6ekmGAwaY4y5/fbbzZIlS6Ljd+zYYfr27WuWL19uDh48aJYuXWr69etnamtrHesxVr0/8sgj5u233zafffaZqa6uNrfccotJS0sz+/fvj1vPJ06cMHv37jV79+41ksy//vUvs3fvXnP48GFjjDFLliwxt99+e3T8f//7X9O/f3/zwAMPmIMHD5oVK1aYPn36mM2bN8et57Pt/fHHHzcbNmwwn376qamtrTX33nuvSUlJMe+++27cer7nnnuM1+s1W7duNY2NjdH69ttvo2OS8Rg/m76T4fg25qfjYNu2baa+vt58/PHHZsmSJcblcpl33nmn3b6TYX+fTd+x2t9WhN3XX39tZs+ebc4//3zj8XjM3LlzzYkTJ6Lv19fXG0nmvffeM8YY09DQYKZOnWoyMjKM2+02eXl55oEHHjChUMjRPp966ikzePBgk5qaaiZOnGh27doVfW/atGmmpKSkzfiXX37ZXHbZZSY1NdWMHDnSvPHGG47215nu9L5o0aLo2KysLHPDDTeYjz76KK79/nw5/q/r5z5LSkrMtGnTTpszbtw4k5qaaoYOHWqee+65uPb8yz660/s//vEPc+mll5q0tDSTkZFhCgsLzZYtW+Lac3v9SmqzD5PxGD+bvpPh+DbGmN///vdmyJAhJjU11QwcONBcc8010cBor29jEr+/jel+37Ha3zziBwBgvaT9zg4AgFgh7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1vt/yVNmStTTQBUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: (1, 1), policy: [9.4719106e-01 7.2903608e-06 6.9468055e-04 5.2106962e-02], policy argmax:Down policy value: 0.7701231241226196\n",
      "search: [1. 0. 0. 0.], search argmax: Down\n",
      "Position: (1, 2), policy: [4.1121953e-06 7.9381140e-03 1.1594472e-05 9.9204624e-01], policy argmax:Right policy value: 0.8162508606910706\n",
      "search: [0. 0. 0. 1.], search argmax: Right\n",
      "Position: (2, 1), policy: [8.9184403e-02 2.0494170e-05 9.1073096e-01 6.4216474e-05], policy argmax:Left policy value: 0.8221691250801086\n",
      "search: [0.85714286 0.         0.14285714 0.        ], search argmax: Down\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob, seed=0)\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "mcts = AlphaMCTS(maze, num_simulations=cfg.search.num_simulations, c_puct=cfg.search.c_puct, model=model)\n",
    "\n",
    "_ = mcts.play_game()\n",
    "\n",
    "positions = [(x, y) for x in range(1, cfg.maze.width-1) for y in range(1, cfg.maze.height-1)]\n",
    "for pos in positions:\n",
    "    if pos == maze.target:\n",
    "        continue\n",
    "    state = Maze.State(*pos, cfg.maze.max_steps, 0)\n",
    "    policy, value = mcts.query_model(state)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {value}\")\n",
    "    search_probs = mcts.search(state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of all states are relatively high. Since the network doesn't receive as input the number of remaining steps, how can it accurately predict the value?\n",
    "For example, if you move into a space diagonally opposite the goal, but you only have one remaining step then your value is definitely -1\n",
    "This makes me think we should feed in the # remaining steps into the network. (but probably normalized as well, and normalized in the same way as the current and goal positions.)\n",
    "\n",
    "We can see that the policy is making the wrong recommendations sometimes. That makes me think we should try:\n",
    "1. Adding the 3 planes of obstacles, free, agent\n",
    "2. Adding 4 planes of obstacles, free, agent, goal\n",
    "\n",
    "Unrelatedly, a bug I need to resolve is why is the softmax layer not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maze.action_to_string(3)\n",
    "maze.action_to_delta(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
