{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# Set precision to 3 decimal places\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I want to try adding an observation plane that marks where the goal is if it's in the local area. Also, I want to instrument the training process with recording how much longer the paths lengths for the successful cases compared to the optimal path lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"maze_4to100_curriculum_targetplane\",\n",
    "    \"maze\": {\n",
    "        \"width\": {\"min\": 4, \"max\": 100},\n",
    "        \"height\": {\"min\": 4, \"max\": 100},\n",
    "        \"cell_occupancy_prob\": {\"min\": 0, \"max\": 0.3},\n",
    "        \"max_steps\": \"L1SourceTarget\", # Use this to set the max steps to the L1 distance between source and target * 2\n",
    "        # To set paramters to constant values, use a float\n",
    "        # \"width\": 4,\n",
    "        # \"height\": 4,\n",
    "        # \"cell_occupancy_prob\": 0,\n",
    "        # \"max_steps\": 5, \n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 100,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_parallel_games\": 250,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"save_every\": 30,\n",
    "        \"use_wandb\": True,\n",
    "        # \"load_checkpoint\": \"maze_4to10_rtg_model_99\",\n",
    "        \"use_curriculum\": True,\n",
    "        \"curriculum_success_threshold\": 0.95,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that the reward stored in the state is unnormalized\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 100\n",
    "    MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -50\n",
    "\n",
    "    def __init__(self, width: int, height: int, cell_occupancy_prob: float = 0.3,seed: Optional[int] = None):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = int(width)\n",
    "        self.height = int(height)\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "\n",
    "        # Max steps configuration\n",
    "        # Option 1: Set the max steps to be the width * height\n",
    "        # self.max_steps=width*height\n",
    "        if cfg.maze.max_steps == \"L1SourceTarget\":\n",
    "            # Option 2: Set the max steps to be 2 * the L1 distance between source and target\n",
    "            self.max_steps = 2 * (abs(self.source[0] - self.target[0]) + abs(self.source[1] - self.target[1]))\n",
    "        elif type(cfg.maze.max_steps) == int:\n",
    "            # Option 3: Manually set the max steps\n",
    "            self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "    @classmethod\n",
    "    def generate_maze_params(cls, num_mazes:int, maze_cfg, seed: Optional[int]=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        maze_params = []\n",
    "        for param_name in ['width', 'height', 'cell_occupancy_prob']:\n",
    "            param = getattr(maze_cfg, param_name)\n",
    "            if isinstance(param, (float, int)):\n",
    "                values = np.full(num_mazes, param)\n",
    "            elif isinstance(param, dict) or isinstance(param, DictConfig) and 'min' in param and 'max' in param:\n",
    "                min_val, max_val = param['min'], param['max']\n",
    "                if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                    # Assumes that if the min and max are integers we want all integers\n",
    "                    values = np.random.randint(min_val, max_val + 1, size=num_mazes)\n",
    "                else:\n",
    "                    values = np.random.uniform(min_val, max_val, size=num_mazes)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid parameter configuration: {param}\")\n",
    "            maze_params.append(values)\n",
    "\n",
    "        # Combine into a single n x 3 array\n",
    "        maze_params = np.column_stack(maze_params)\n",
    "        return maze_params\n",
    "\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "        # Plane 1: Target if in local observation window\n",
    "        plane_target = copy.deepcopy(plane_obstacles)\n",
    "\n",
    "        # Plane 0:\n",
    "        # Make sure that any number that is not 1 is 0 for the obstacle plane\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "        \n",
    "        # Plane 1:\n",
    "        # Make all non-target cells 0\n",
    "        plane_target[plane_target != 3] = 0\n",
    "        # Make target cells 1\n",
    "        plane_target[plane_target == 3] = 1\n",
    "\n",
    "        return np.stack([plane_obstacles, plane_target], axis=0)\n",
    "\n",
    "\n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "    \n",
    "    def get_normalized_steps_left(self, state: State):\n",
    "        return state.steps_left / self.max_steps\n",
    "    \n",
    "    def get_normalized_distances(self):\n",
    "        # Returns the normalized distances in the x and y directions that can be travelled by the agent in 50% of the max steps\n",
    "        scaling_factor = 0.5\n",
    "\n",
    "        return (self.max_steps * scaling_factor / self.width, self.max_steps * scaling_factor / self.height)\n",
    "    \n",
    "    def get_encoded_scalar_features(self, state: State):\n",
    "        return (\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            *self.get_normalized_target_position(),\n",
    "            self.get_normalized_steps_left(state),\n",
    "            *self.get_normalized_distances()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        \"\"\"Returns the unnormalized reward and whether the episode is terminated\"\"\"\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "        return state.reward, False\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        # Normalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 2 * ((reward - min_reward) / (max_reward - min_reward)) - 1\n",
    "    \n",
    "    def unnormalize_reward(self, normalized_reward):\n",
    "        # Unnormalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 0.5 * (normalized_reward + 1) * (max_reward - min_reward) + min_reward\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obstacle Plane\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Plane\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze = Maze(5, 5, 0)\n",
    "maze.visualize_state()\n",
    "initial_state = maze.get_initial_state()\n",
    "obs = maze.get_encoded_observation(initial_state)\n",
    "print(\"Obstacle Plane\")\n",
    "maze.visualize_state(obs[0])\n",
    "\n",
    "print(\"Target Plane\")\n",
    "maze.visualize_state(obs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters, device):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        SCALAR_FEATURES_SIZE = 7  # see Maze.get_encoded_scalar_features\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The first input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        # The second input channel is for the observation where the target is 1 and everything else is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + SCALAR_FEATURES_SIZE, 256),  # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + SCALAR_FEATURES_SIZE, 256), # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, scalar_features):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # scalar_features: (batch_size, 7), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, scalar_features], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, scalar_features], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Search node in the MCTS tree\"\"\"\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "class GameEpisode:\n",
    "    \"\"\"Stateful episode of a game\"\"\"\n",
    "    def __init__(self, game: Maze):\n",
    "        self.game = game\n",
    "        self.state: Maze.State = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.reward_history = []\n",
    "        self.root: Optional[Node] = Node(self.state, self.game.get_valid_actions(self.state))\n",
    "        self.node: Optional[Node] = None\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, search_cfg, model: ResNet):\n",
    "        self.cfg = search_cfg\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, game: Maze, max_iters = 1000, verbose=True, visualize=True):\n",
    "        \"\"\"Play a single game\"\"\"\n",
    "        state = game.get_initial_state()\n",
    "        path = []\n",
    "        root = Node(state, game.get_valid_actions(state))\n",
    "        for i in range(max_iters):\n",
    "            action_probs = self.search(game, root=root)\n",
    "            path.append((state.x, state.y))\n",
    "            print(f\"Step {i+1}: {state}, action_probs: {action_probs}\")\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            for child in root.children:\n",
    "                if child.last_action == action:\n",
    "                    # Set the child as the new root to preserve the search tree\n",
    "                    root = child\n",
    "                    break\n",
    "            state = root.state\n",
    "            \n",
    "            value, is_terminal = game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    game.visualize_path(path)\n",
    "                \n",
    "                return path, value\n",
    "                \n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, game: Maze, state: Optional[Maze.State] = None, root: Optional[Node] = None) -> np.ndarray:\n",
    "        if root is None and state is not None:\n",
    "            root = Node(state, game.get_valid_actions(state))\n",
    "        elif state is None and root is None:\n",
    "            assert False, \"Either state or root must be provided\"\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node, game)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(state=node.state, game=game)\n",
    "                value = game.unnormalize_reward(value)\n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy, game=game)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batch_search(self, episodes: List[GameEpisode]):\n",
    "        \n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            # Collect nodes for expansion and evaluation\n",
    "            for ep in episodes:\n",
    "                ep.node = None # Reset the node marked for expansion and evaluation for each episode\n",
    "                node = ep.root\n",
    "                # Selection all the way down till a leaf node\n",
    "                while not node.is_leaf:\n",
    "                    node = self.select(node, ep.game)\n",
    "\n",
    "                # Evaluate the leaf node\n",
    "                value, is_terminal = ep.game.get_value_and_terminated(node.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    self.backpropagate(node, value)\n",
    "                else:\n",
    "                    ep.node = node # Mark the leaf node for expansion and evaluation\n",
    "\n",
    "            # Batch query the model for the policy and value\n",
    "            expandable_episodes = [ep_idx for ep_idx, ep in enumerate(episodes) if ep.node is not None]\n",
    "\n",
    "            if len(expandable_episodes) > 0:\n",
    "                obs = np.stack([episodes[ep_idx].game.get_encoded_observation(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                scalar_features = np.stack([episodes[ep_idx].game.get_encoded_scalar_features(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                tensor_obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "                tensor_scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.model(\n",
    "                    tensor_obs, tensor_scalar_features\n",
    "                    )\n",
    "                \n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "            \n",
    "            # Expand the nodes and backpropagate\n",
    "            for batch_idx, ep_idx in enumerate(expandable_episodes):\n",
    "                node = episodes[ep_idx].node\n",
    "                ep_policy, ep_value = policy[batch_idx], value[batch_idx].item()\n",
    "\n",
    "                valid_policy = np.zeros_like(ep_policy)\n",
    "                valid_policy[node.valid_actions] = ep_policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                ep_value = ep.game.unnormalize_reward(ep_value)\n",
    "                self.expand(node, policy=valid_policy, game=episodes[ep_idx].game)\n",
    "                self.backpropagate(node, ep_value)\n",
    "    \n",
    "    def query_model(self, state: Maze.State, game: Maze) -> Tuple[np.ndarray, float]:\n",
    "        tensor_obs = torch.tensor(game.get_encoded_observation(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        tensor_scalar_features = torch.tensor(game.get_encoded_scalar_features(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_scalar_features\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        normalized_policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return normalized_policy, value\n",
    "\n",
    "    def select(self, node: Node, game: Maze) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child, game) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node, game: Maze) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # Q-value needs to be noramalized between -1 and 1 for this formula.\n",
    "            q_value = game.normalize_reward(child.value_sum / child.visit_count)\n",
    "\n",
    "        u_value = self.cfg.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        \n",
    "        return q_value + u_value\n",
    "\n",
    "    \n",
    "    def expand(self, node: Node, policy, game: Maze) -> None:\n",
    "        _, is_terminal = game.get_value_and_terminated(node.state)\n",
    "        assert not is_terminal, \"Cannot expand a terminal node\"\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        \n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        \"\"\"Takes in unnormalized value\"\"\"\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model: ResNet, optimizer, search_alg: AlphaMCTS, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "\n",
    "        self.cfg = cfg.learn\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.last_success_rates = []\n",
    "        self.last_maze_cfg = None\n",
    "        self.original_maze_cfg = cfg.maze\n",
    "    \n",
    "    def get_maze_cfg_from_curriculum(self):\n",
    "        if len(self.last_success_rates) < 3:\n",
    "            # Not enough data yet, start with initial maze size\n",
    "            maze_cfg = copy.deepcopy(self.original_maze_cfg)\n",
    "            maze_cfg.width.max = 6\n",
    "            maze_cfg.height.max = 6\n",
    "            return maze_cfg\n",
    "\n",
    "        if all(rate >= self.cfg.curriculum_success_threshold for rate in self.last_success_rates):\n",
    "            # Increase the maze size\n",
    "            maze_cfg = copy.deepcopy(self.last_maze_cfg)\n",
    "            maze_cfg.width.max = min(self.last_maze_cfg.width.max + 2, self.original_maze_cfg.width.max)\n",
    "            maze_cfg.height.max = min(self.last_maze_cfg.height.max + 2, self.original_maze_cfg.height.max)\n",
    "        else:\n",
    "            # Keep the maze size the same\n",
    "            maze_cfg = self.last_maze_cfg\n",
    "        \n",
    "        return maze_cfg\n",
    "    \n",
    "    def self_play(self, maze_cfg):\n",
    "\n",
    "        maze_params = Maze.generate_maze_params(self.cfg.num_parallel_games, maze_cfg=maze_cfg)\n",
    "        episodes = [GameEpisode(Maze(*params)) for params in maze_params]\n",
    "        ret_mem = []\n",
    "        n_successes = 0\n",
    "        optimal_path_ratio_sum = 0\n",
    "        while len(episodes) > 0:\n",
    "            \n",
    "            self.search_alg.batch_search(episodes)\n",
    "\n",
    "            # Serially process the episodes\n",
    "            for i in range(len(episodes))[::-1]:\n",
    "                ep = episodes[i]\n",
    "\n",
    "                action_probs = np.zeros(ep.game.action_size)\n",
    "                for child in ep.root.children:\n",
    "                    action_probs[child.last_action] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                ep.memory.append((ep.game.get_encoded_observation(ep.root.state), \n",
    "                                  ep.game.get_encoded_scalar_features(ep.root.state),\n",
    "                                  action_probs))\n",
    "                ep.reward_history.append(ep.root.state.reward)\n",
    "\n",
    "                action = np.random.choice(ep.game.action_size, p=action_probs)\n",
    "                for child in ep.root.children:\n",
    "                    if child.last_action == action:\n",
    "                        # Set the child as the new root to preserve the search tree\n",
    "                        ep.root = child\n",
    "                        break\n",
    "                ep.state = ep.root.state\n",
    "\n",
    "                final_reward, is_terminal = ep.game.get_value_and_terminated(ep.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    # Unroll the reward history and memory\n",
    "                    for mem, reward_to_go in zip(ep.memory, ep.reward_history):\n",
    "                        reward_to_go = final_reward - reward_to_go\n",
    "                        ret_mem.append((*mem, ep.game.normalize_reward(reward_to_go)))\n",
    "                    if (ep.state.x, ep.state.y) == ep.game.target:\n",
    "                        n_successes += 1\n",
    "                        optimal_path_ratio_sum += (len(ep.memory)+1)/len(ep.game.shortest_path)\n",
    "\n",
    "                    del episodes[i]\n",
    "        \n",
    "        return ret_mem, n_successes, optimal_path_ratio_sum\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.cfg.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.cfg.train_batch_size]\n",
    "            obs, scalar_features, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, scalar_features, policy_targets, value_targets = np.array(obs), np.array(scalar_features), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "            scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, scalar_features)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log metrics for the current batch\n",
    "                wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.cfg.train_batch_size)\n",
    "        if self.cfg.use_wandb:\n",
    "            # Log average loss for the epoch\n",
    "            wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "                name=cfg.name,\n",
    "                config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "                save_code=True)\n",
    "            \n",
    "            wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for iteration in range(self.cfg.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "            optimal_path_ratio_sum = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "\n",
    "            # Initialize all games and an episode for each game\n",
    "            if self.cfg.use_curriculum:\n",
    "                maze_cfg = self.get_maze_cfg_from_curriculum()\n",
    "                wandb.log({\"max_maze_width\": maze_cfg.width.max})\n",
    "                self.last_maze_cfg = maze_cfg\n",
    "            else:\n",
    "                maze_cfg = cfg.maze\n",
    "\n",
    "            # Calculate the number of batches\n",
    "            n_batches = self.cfg.num_self_play_iters // self.cfg.num_parallel_games\n",
    "\n",
    "\n",
    "            for _ in trange(n_batches):\n",
    "                batch_episode_mems, num_episode_successes, opt_path_ratio_sum = self.self_play(maze_cfg)\n",
    "                successes += num_episode_successes\n",
    "                optimal_path_ratio_sum += opt_path_ratio_sum\n",
    "                memory += batch_episode_mems\n",
    "\n",
    "            success_rate = successes / self.cfg.num_self_play_iters\n",
    "            optimal_path_ratio = optimal_path_ratio_sum / successes\n",
    "            self.last_success_rates.append(success_rate)\n",
    "            if len(self.last_success_rates) > 3:\n",
    "                self.last_success_rates.pop(0)  # Keep only the last 3 success rates\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log the success rate for self-play games\n",
    "                wandb.log({\"success_rate\": success_rate, \"optimal_path_ratio\": optimal_path_ratio, \"iteration\": iteration, \"wall_time\": time.time() - start_time})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.cfg.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            # Save if iter divides save_every or if it is the last iteration\n",
    "            if (iteration % self.cfg.save_every == 0 and iteration != 0) or iteration == self.cfg.num_learn_iters - 1:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "                if self.cfg.use_wandb:\n",
    "                    # Log model checkpoint to W&B\n",
    "                    wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "                    wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaoyuan\u001b[0m (\u001b[33mcontact_placement\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shaoyuan/Documents/Software/AlphaZeroFromScratch/discrete_maze/wandb/run-20241115_172139-fl5gw7zc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/fl5gw7zc' target=\"_blank\">maze_4to100_curriculum_targetplane</a></strong> to <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/fl5gw7zc' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/fl5gw7zc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901fb0b9ea124c35991639a32774f994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8698275eb744dc59a16d15d8d66a4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5992d4b2c3e4b05bb254b369b7be039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbcc42d6f384f1e9e4737e395c17bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb2073201f4441c891500097e942264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57044dda1d84a1eabee93e4b900c349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c94faab489477f819a14e3aa6019e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96139ca6f3724d6893a96a991f609c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d1fb00235b45e3a3d915dfee45466d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3461965d73e54c33a3f1e6586750bba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160048fae48a468ea263e6a74197ca0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5949e1d564464228a8bfba0041eec5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fb04b365c14a6a8971427a061d37c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42165de59c84cb195669fa51ac91361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fe234d403b41dbae86105417b54867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc239ae39614ef1afc68923806c5edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbf80eecfc0477682cf85e268775c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f07e6e2c73f4a55bb4453ea25248b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6305efa6a510495592235ffdb4cc47a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072046aac2c64aefb3cac388619238c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700d7b2d7da84489af5d64a54fa04e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece4db7169704079b8a4881c9c876385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0111a3efcf4939902101ce547036e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662fa8314c3d4882bb24a7a6731a5dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6397f47dfc0443f4a8eb438f23b15e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c42062b1f74d1e81ddc2813c2c0f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb695eee2cf4367abba161cae053c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7d69ae21944010a1cda79de8e233c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13f31b8b10a47c5b9981307e2944082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad94183cef842b28e9cf1c5d6f26789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba1f72bf398436e83ee74a318d66660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdb3346623c4fb38f017650fc299a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4358585899e64f34a121c7d30eeaa64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2b9936ad6342c683baf4663bb33260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800f740e2cb345fab7533f35e812f0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e8b338e48a47df899a544bb93a7e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a21ce4ea3e427faa79229f1d21c753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d57f5128654376b6f63aa65ab38620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f348ae1ae484c979dbeb057417a87e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8619b1fe96fd40b4806a9bba59779b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5066815e054e05afd76605aa6fc2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1e1ace6c66454b91365bc801a3e81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d8e1c0bc3c4fd0b82567c071e3cabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9458f5bac6426e885d097c4c31928c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fe1b3194244d56a600789aae5214d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "if \"load_checkpoint\" in cfg.learn:\n",
    "    # Replace \"optimizer\" with \"model\"\n",
    "    model_filename = cfg.learn.load_checkpoint.replace(\"optimizer\", \"model\")\n",
    "    model.load_state_dict(torch.load(f\"checkpoints/{model_filename}.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr, weight_decay=cfg.learn.weight_decay)\n",
    "if \"load_checkpoint\" in cfg.learn:\n",
    "    # Replace \"model\" with \"optimizer\"\n",
    "    model_filename = cfg.learn.load_checkpoint.replace(\"model\", \"optimizer\")\n",
    "    optimizer.load_state_dict(torch.load(f\"checkpoints/{model_filename}.pt\"))\n",
    "\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, seed=0)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = Maze.generate_maze_params(1, cfg)\n",
    "params = np.array([[100, 100, 0.2]])\n",
    "width, height, cell_occupancy_prob = params[0]\n",
    "width, height = int(width), int(height)\n",
    "maze = Maze(*params[0])\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{100}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_round3_model_13.pt\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "print(mcts.play_game(game=maze))\n",
    "\n",
    "positions = [(x, y) for x in range(1, width-1) for y in range(1, height-1)]\n",
    "\n",
    "for pos in positions:\n",
    "    if pos == maze.target or maze.map[pos] == 1:\n",
    "        continue\n",
    "    state = Maze.State(*pos, 1, 0)\n",
    "    policy, final_reward = mcts.query_model(state, game=maze)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {final_reward}\")\n",
    "    search_probs = mcts.search(game=maze, state=state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
