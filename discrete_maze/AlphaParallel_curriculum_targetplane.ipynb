{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# Set precision to 3 decimal places\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I want to try adding an observation plane that marks where the goal is if it's in the local area. Also, I want to instrument the training process with recording how much longer the paths lengths for the successful cases compared to the optimal path lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"maze_4to100_curriculum_targetplane_easy\",\n",
    "    \"maze\": {\n",
    "        \"width\": {\"min\": 4, \"max\": 100},\n",
    "        \"height\": {\"min\": 4, \"max\": 100},\n",
    "        \"cell_occupancy_prob\": {\"min\": 0, \"max\": 0.1},\n",
    "        \"max_steps\": \"ShortestPath\", # Use this to set the max steps to the shortest path between source and target * 2\n",
    "        # \"max_steps\": \"L1SourceTarget\", # Use this to set the max steps to the L1 distance between source and target * 2\n",
    "        # To set paramters to constant values, use a float\n",
    "        # \"width\": 4,\n",
    "        # \"height\": 4,\n",
    "        # \"cell_occupancy_prob\": 0,\n",
    "        # \"max_steps\": 5, \n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 100,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_parallel_games\": 250,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"save_every\": 30,\n",
    "        \"use_wandb\": True,\n",
    "        # \"load_checkpoint\": \"maze_4to10_rtg_model_99\",\n",
    "        \"use_curriculum\": True,\n",
    "        \"curriculum_success_threshold\": 0.95,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that the reward stored in the state is unnormalized\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 100\n",
    "    MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -50\n",
    "\n",
    "    def __init__(self, width: int, height: int, cell_occupancy_prob: float = 0.3,seed: Optional[int] = None):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = int(width)\n",
    "        self.height = int(height)\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "\n",
    "        # Max steps configuration\n",
    "        # Option 1: Set the max steps to be the width * height\n",
    "        # self.max_steps=width*height\n",
    "        if cfg.maze.max_steps == \"L1SourceTarget\":\n",
    "            # Option 2: Set the max steps to be 2 * the L1 distance between source and target\n",
    "            self.max_steps = 2 * (abs(self.source[0] - self.target[0]) + abs(self.source[1] - self.target[1]))\n",
    "        elif cfg.maze.max_steps == \"ShortestPath\":\n",
    "            # Option 3: Set the max steps to be the shortest path between source and target * 2\n",
    "            self.max_steps = len(self.shortest_path) * 2\n",
    "        elif type(cfg.maze.max_steps) == int:\n",
    "            # Option 4: Manually set the max steps\n",
    "            self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "    @classmethod\n",
    "    def generate_maze_params(cls, num_mazes:int, maze_cfg, seed: Optional[int]=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        maze_params = []\n",
    "        for param_name in ['width', 'height', 'cell_occupancy_prob']:\n",
    "            param = getattr(maze_cfg, param_name)\n",
    "            if isinstance(param, (float, int)):\n",
    "                values = np.full(num_mazes, param)\n",
    "            elif isinstance(param, dict) or isinstance(param, DictConfig) and 'min' in param and 'max' in param:\n",
    "                min_val, max_val = param['min'], param['max']\n",
    "                if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                    # Assumes that if the min and max are integers we want all integers\n",
    "                    values = np.random.randint(min_val, max_val + 1, size=num_mazes)\n",
    "                else:\n",
    "                    values = np.random.uniform(min_val, max_val, size=num_mazes)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid parameter configuration: {param}\")\n",
    "            maze_params.append(values)\n",
    "\n",
    "        # Combine into a single n x 3 array\n",
    "        maze_params = np.column_stack(maze_params)\n",
    "        return maze_params\n",
    "\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "        # Plane 1: Target if in local observation window\n",
    "        plane_target = copy.deepcopy(plane_obstacles)\n",
    "\n",
    "        # Plane 0:\n",
    "        # Make sure that any number that is not 1 is 0 for the obstacle plane\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "        \n",
    "        # Plane 1:\n",
    "        # Make all non-target cells 0\n",
    "        plane_target[plane_target != 3] = 0\n",
    "        # Make target cells 1\n",
    "        plane_target[plane_target == 3] = 1\n",
    "\n",
    "        return np.stack([plane_obstacles, plane_target], axis=0)\n",
    "\n",
    "\n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "    \n",
    "    def get_normalized_steps_left(self, state: State):\n",
    "        return state.steps_left / self.max_steps\n",
    "    \n",
    "    def get_normalized_distances(self):\n",
    "        # Returns the normalized distances in the x and y directions that can be travelled by the agent in 50% of the max steps\n",
    "        scaling_factor = 0.5\n",
    "\n",
    "        return (self.max_steps * scaling_factor / self.width, self.max_steps * scaling_factor / self.height)\n",
    "    \n",
    "    def get_encoded_scalar_features(self, state: State):\n",
    "        return (\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            *self.get_normalized_target_position(),\n",
    "            self.get_normalized_steps_left(state),\n",
    "            *self.get_normalized_distances()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        \"\"\"Returns the unnormalized reward and whether the episode is terminated\"\"\"\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "        return state.reward, False\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        # Normalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 2 * ((reward - min_reward) / (max_reward - min_reward)) - 1\n",
    "    \n",
    "    def unnormalize_reward(self, normalized_reward):\n",
    "        # Unnormalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 0.5 * (normalized_reward + 1) * (max_reward - min_reward) + min_reward\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obstacle Plane\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Plane\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze = Maze(5, 5, 0)\n",
    "maze.visualize_state()\n",
    "initial_state = maze.get_initial_state()\n",
    "obs = maze.get_encoded_observation(initial_state)\n",
    "print(\"Obstacle Plane\")\n",
    "maze.visualize_state(obs[0])\n",
    "\n",
    "print(\"Target Plane\")\n",
    "maze.visualize_state(obs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters, device):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        SCALAR_FEATURES_SIZE = 7  # see Maze.get_encoded_scalar_features\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The first input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        # The second input channel is for the observation where the target is 1 and everything else is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + SCALAR_FEATURES_SIZE, 256),  # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + SCALAR_FEATURES_SIZE, 256), # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, scalar_features):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # scalar_features: (batch_size, 7), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, scalar_features], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, scalar_features], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Search node in the MCTS tree\"\"\"\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "class GameEpisode:\n",
    "    \"\"\"Stateful episode of a game\"\"\"\n",
    "    def __init__(self, game: Maze):\n",
    "        self.game = game\n",
    "        self.state: Maze.State = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.reward_history = []\n",
    "        self.root: Optional[Node] = Node(self.state, self.game.get_valid_actions(self.state))\n",
    "        self.node: Optional[Node] = None\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, search_cfg, model: ResNet):\n",
    "        self.cfg = search_cfg\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, game: Maze, max_iters = 1000, verbose=True, visualize=True):\n",
    "        \"\"\"Play a single game\"\"\"\n",
    "        state = game.get_initial_state()\n",
    "        path = []\n",
    "        root = Node(state, game.get_valid_actions(state))\n",
    "        for i in range(max_iters):\n",
    "            action_probs = self.search(game, root=root)\n",
    "            path.append((state.x, state.y))\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {game.action_to_string(action)}\")\n",
    "            #     print(f\"agent: {game.get_normalized_agent_position(state)}, target: {game.get_normalized_target_position()}, steps_left: {game.get_normalized_steps_left(state)}, distances: {game.get_normalized_distances()}\")\n",
    "                # obs_planes = game.get_encoded_observation(state)\n",
    "                # game.visualize_state(obs_planes[0])\n",
    "                # game.visualize_state(obs_planes[1])\n",
    "\n",
    "            for child in root.children:\n",
    "                if child.last_action == action:\n",
    "                    # Set the child as the new root to preserve the search tree\n",
    "                    root = child\n",
    "                    break\n",
    "            state = root.state\n",
    "            \n",
    "            value, is_terminal = game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    game.visualize_path(path)\n",
    "                \n",
    "                return path, value\n",
    "                \n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, game: Maze, state: Optional[Maze.State] = None, root: Optional[Node] = None) -> np.ndarray:\n",
    "        if root is None and state is not None:\n",
    "            root = Node(state, game.get_valid_actions(state))\n",
    "        elif state is None and root is None:\n",
    "            assert False, \"Either state or root must be provided\"\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node, game)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(state=node.state, game=game)\n",
    "                value = game.unnormalize_reward(value)\n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy, game=game)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batch_search(self, episodes: List[GameEpisode]):\n",
    "        \n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            # Collect nodes for expansion and evaluation\n",
    "            for ep in episodes:\n",
    "                ep.node = None # Reset the node marked for expansion and evaluation for each episode\n",
    "                node = ep.root\n",
    "                # Selection all the way down till a leaf node\n",
    "                while not node.is_leaf:\n",
    "                    node = self.select(node, ep.game)\n",
    "\n",
    "                # Evaluate the leaf node\n",
    "                value, is_terminal = ep.game.get_value_and_terminated(node.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    self.backpropagate(node, value)\n",
    "                else:\n",
    "                    ep.node = node # Mark the leaf node for expansion and evaluation\n",
    "\n",
    "            # Batch query the model for the policy and value\n",
    "            expandable_episodes = [ep_idx for ep_idx, ep in enumerate(episodes) if ep.node is not None]\n",
    "\n",
    "            if len(expandable_episodes) > 0:\n",
    "                obs = np.stack([episodes[ep_idx].game.get_encoded_observation(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                scalar_features = np.stack([episodes[ep_idx].game.get_encoded_scalar_features(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                tensor_obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "                tensor_scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.model(\n",
    "                    tensor_obs, tensor_scalar_features\n",
    "                    )\n",
    "                \n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "            \n",
    "            # Expand the nodes and backpropagate\n",
    "            for batch_idx, ep_idx in enumerate(expandable_episodes):\n",
    "                node = episodes[ep_idx].node\n",
    "                ep_policy, ep_value = policy[batch_idx], value[batch_idx].item()\n",
    "\n",
    "                valid_policy = np.zeros_like(ep_policy)\n",
    "                valid_policy[node.valid_actions] = ep_policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                ep_value = ep.game.unnormalize_reward(ep_value)\n",
    "                self.expand(node, policy=valid_policy, game=episodes[ep_idx].game)\n",
    "                self.backpropagate(node, ep_value)\n",
    "    \n",
    "    def query_model(self, state: Maze.State, game: Maze) -> Tuple[np.ndarray, float]:\n",
    "        tensor_obs = torch.tensor(game.get_encoded_observation(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        tensor_scalar_features = torch.tensor(game.get_encoded_scalar_features(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_scalar_features\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        normalized_policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return normalized_policy, value\n",
    "\n",
    "    def select(self, node: Node, game: Maze) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child, game) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node, game: Maze) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # Q-value needs to be noramalized between -1 and 1 for this formula.\n",
    "            q_value = game.normalize_reward(child.value_sum / child.visit_count)\n",
    "\n",
    "        u_value = self.cfg.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        \n",
    "        return q_value + u_value\n",
    "\n",
    "    \n",
    "    def expand(self, node: Node, policy, game: Maze) -> None:\n",
    "        _, is_terminal = game.get_value_and_terminated(node.state)\n",
    "        assert not is_terminal, \"Cannot expand a terminal node\"\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        \n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        \"\"\"Takes in unnormalized value\"\"\"\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model: ResNet, optimizer, search_alg: AlphaMCTS, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "\n",
    "        self.cfg = cfg.learn\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.last_success_rates = []\n",
    "        self.last_maze_cfg = None\n",
    "        self.original_maze_cfg = cfg.maze\n",
    "    \n",
    "    def get_maze_cfg_from_curriculum(self):\n",
    "        if len(self.last_success_rates) < 3:\n",
    "            # Not enough data yet, start with initial maze size\n",
    "            maze_cfg = copy.deepcopy(self.original_maze_cfg)\n",
    "            maze_cfg.width.max = 6\n",
    "            maze_cfg.height.max = 6\n",
    "            return maze_cfg\n",
    "\n",
    "        if all(rate >= self.cfg.curriculum_success_threshold for rate in self.last_success_rates):\n",
    "            # Increase the maze size\n",
    "            maze_cfg = copy.deepcopy(self.last_maze_cfg)\n",
    "            maze_cfg.width.max = min(self.last_maze_cfg.width.max + 2, self.original_maze_cfg.width.max)\n",
    "            maze_cfg.height.max = min(self.last_maze_cfg.height.max + 2, self.original_maze_cfg.height.max)\n",
    "        else:\n",
    "            # Keep the maze size the same\n",
    "            maze_cfg = self.last_maze_cfg\n",
    "        \n",
    "        return maze_cfg\n",
    "    \n",
    "    def self_play(self, maze_cfg):\n",
    "\n",
    "        maze_params = Maze.generate_maze_params(self.cfg.num_parallel_games, maze_cfg=maze_cfg)\n",
    "        episodes = [GameEpisode(Maze(*params)) for params in maze_params]\n",
    "        ret_mem = []\n",
    "        n_successes = 0\n",
    "        optimal_path_ratio_sum = 0\n",
    "        while len(episodes) > 0:\n",
    "            \n",
    "            self.search_alg.batch_search(episodes)\n",
    "\n",
    "            # Serially process the episodes\n",
    "            for i in range(len(episodes))[::-1]:\n",
    "                ep = episodes[i]\n",
    "\n",
    "                action_probs = np.zeros(ep.game.action_size)\n",
    "                for child in ep.root.children:\n",
    "                    action_probs[child.last_action] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                ep.memory.append((ep.game.get_encoded_observation(ep.root.state), \n",
    "                                  ep.game.get_encoded_scalar_features(ep.root.state),\n",
    "                                  action_probs))\n",
    "                ep.reward_history.append(ep.root.state.reward)\n",
    "\n",
    "                action = np.random.choice(ep.game.action_size, p=action_probs)\n",
    "                for child in ep.root.children:\n",
    "                    if child.last_action == action:\n",
    "                        # Set the child as the new root to preserve the search tree\n",
    "                        ep.root = child\n",
    "                        break\n",
    "                ep.state = ep.root.state\n",
    "\n",
    "                final_reward, is_terminal = ep.game.get_value_and_terminated(ep.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    # Unroll the reward history and memory\n",
    "                    for mem, reward_to_go in zip(ep.memory, ep.reward_history):\n",
    "                        reward_to_go = final_reward - reward_to_go\n",
    "                        ret_mem.append((*mem, ep.game.normalize_reward(reward_to_go)))\n",
    "                    if (ep.state.x, ep.state.y) == ep.game.target:\n",
    "                        n_successes += 1\n",
    "                        optimal_path_ratio_sum += (len(ep.memory)+1)/len(ep.game.shortest_path)\n",
    "\n",
    "                    del episodes[i]\n",
    "        \n",
    "        return ret_mem, n_successes, optimal_path_ratio_sum\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.cfg.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.cfg.train_batch_size]\n",
    "            obs, scalar_features, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, scalar_features, policy_targets, value_targets = np.array(obs), np.array(scalar_features), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "            scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, scalar_features)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log metrics for the current batch\n",
    "                wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.cfg.train_batch_size)\n",
    "        if self.cfg.use_wandb:\n",
    "            # Log average loss for the epoch\n",
    "            wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "                name=cfg.name,\n",
    "                config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "                save_code=True)\n",
    "            \n",
    "            wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for iteration in range(self.cfg.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "            optimal_path_ratio_sum = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "\n",
    "            # Initialize all games and an episode for each game\n",
    "            if self.cfg.use_curriculum:\n",
    "                maze_cfg = self.get_maze_cfg_from_curriculum()\n",
    "                wandb.log({\"max_maze_width\": maze_cfg.width.max})\n",
    "                self.last_maze_cfg = maze_cfg\n",
    "            else:\n",
    "                maze_cfg = cfg.maze\n",
    "\n",
    "            # Calculate the number of batches\n",
    "            n_batches = self.cfg.num_self_play_iters // self.cfg.num_parallel_games\n",
    "\n",
    "\n",
    "            for _ in trange(n_batches):\n",
    "                batch_episode_mems, num_episode_successes, opt_path_ratio_sum = self.self_play(maze_cfg)\n",
    "                successes += num_episode_successes\n",
    "                optimal_path_ratio_sum += opt_path_ratio_sum\n",
    "                memory += batch_episode_mems\n",
    "\n",
    "            success_rate = successes / self.cfg.num_self_play_iters\n",
    "            optimal_path_ratio = optimal_path_ratio_sum / successes\n",
    "            self.last_success_rates.append(success_rate)\n",
    "            if len(self.last_success_rates) > 3:\n",
    "                self.last_success_rates.pop(0)  # Keep only the last 3 success rates\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log the success rate for self-play games\n",
    "                wandb.log({\"success_rate\": success_rate, \"optimal_path_ratio\": optimal_path_ratio, \"iteration\": iteration, \"wall_time\": time.time() - start_time})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.cfg.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            # Save if iter divides save_every or if it is the last iteration\n",
    "            if (iteration % self.cfg.save_every == 0 and iteration != 0) or iteration == self.cfg.num_learn_iters - 1:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "                if self.cfg.use_wandb:\n",
    "                    # Log model checkpoint to W&B\n",
    "                    wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "                    wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "if \"load_checkpoint\" in cfg.learn:\n",
    "    # Replace \"optimizer\" with \"model\"\n",
    "    model_filename = cfg.learn.load_checkpoint.replace(\"optimizer\", \"model\")\n",
    "    model.load_state_dict(torch.load(f\"checkpoints/{model_filename}.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr, weight_decay=cfg.learn.weight_decay)\n",
    "if \"load_checkpoint\" in cfg.learn:\n",
    "    # Replace \"model\" with \"optimizer\"\n",
    "    model_filename = cfg.learn.load_checkpoint.replace(\"model\", \"optimizer\")\n",
    "    optimizer.load_state_dict(torch.load(f\"checkpoints/{model_filename}.pt\"))\n",
    "\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, seed=0)\n",
    "# alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_452163/3767381654.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached target in 82 steps\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# params = Maze.generate_maze_params(1, cfg)\n",
    "\n",
    "params = np.array([[100, 100, 0.1]])\n",
    "width, height, cell_occupancy_prob = params[0]\n",
    "width, height = int(width), int(height)\n",
    "maze = Maze(*params[0])\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{100}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_round3_model_13.pt\"))\n",
    "model.eval()\n",
    "\n",
    "new_cfg = copy.deepcopy(cfg)\n",
    "new_cfg.search.num_simulations = 50\n",
    "mcts = AlphaMCTS(search_cfg=new_cfg.search, model=model)\n",
    "\n",
    "_ = mcts.play_game(game=maze, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: (25, 81)\n",
      "Position: (24, 80), policy: [0.785 0.162 0.052 0.   ], policy argmax:Down policy value: 0.8769886493682861\n",
      "search: [0.796 0.163 0.041 0.   ], search argmax: Down\n",
      "Position: (24, 81), policy: [0.036 0.    0.    0.964], policy argmax:Right policy value: 0.9554309248924255\n",
      "search: [0. 0. 0. 1.], search argmax: Right\n",
      "Position: (24, 82), policy: [0.    0.672 0.127 0.201], policy argmax:Up policy value: 0.8503240346908569\n",
      "search: [0.    0.673 0.122 0.204], search argmax: Up\n",
      "Position: (25, 82), policy: [0.    0.998 0.001 0.001], policy argmax:Up policy value: 0.9709590673446655\n",
      "search: [0. 1. 0. 0.], search argmax: Up\n",
      "Position: (26, 80), policy: [0.951 0.021 0.004 0.024], policy argmax:Down policy value: 0.8105722665786743\n",
      "search: [0.959 0.02  0.    0.02 ], search argmax: Down\n",
      "Position: (26, 81), policy: [0.046 0.    0.889 0.065], policy argmax:Left policy value: 0.5813804864883423\n",
      "search: [0. 0. 1. 0.], search argmax: Left\n",
      "Position: (26, 82), policy: [0.074 0.001 0.809 0.116], policy argmax:Left policy value: 0.8372030854225159\n",
      "search: [0.061 0.    0.837 0.102], search argmax: Left\n"
     ]
    }
   ],
   "source": [
    "# Positions around target:\n",
    "positions = [(maze.target[0] + dx, maze.target[1] + dy) for dx in [-1, 0, 1] for dy in [-1, 0, 1]]\n",
    "print(f\"Target: {maze.target}\")\n",
    "for pos in positions:\n",
    "    if pos == maze.target or maze.map[pos] == 1:\n",
    "        continue\n",
    "    state = Maze.State(*pos, 1, 0)\n",
    "    policy, final_reward = mcts.query_model(state, game=maze)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {final_reward}\")\n",
    "    search_probs = mcts.search(game=maze, state=state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
