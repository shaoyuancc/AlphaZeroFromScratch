{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Set precision to 3 decimal places\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'm going to do exactly the same as with maxsteps2_softmaxbugfix, but now I'm also going to pass in the number of remaining steps to see if we can get the training loss to be further reduced.\n",
    "\n",
    "Besides the stepsleft, I also need to pass in a value for how much distance you can travel in some fixed proportion of the total trajectory length. For example, in 0.5 of my trajectory I can cover 0.25 of the x axis of my grid, and 0.25 of the y axis of my grid. That would be the case if max length is 2 and grid width and height is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"maze_4x4_binaryreward_maxsteps2_wstepsleft_round3\",\n",
    "    \"maze\": {\n",
    "        \"width\": 4,\n",
    "        \"height\": 4,\n",
    "        \"cell_occupancy_prob\": 0,\n",
    "        \"max_steps\": 2\n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 14,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 1\n",
    "    # MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -1\n",
    "\n",
    "    def __init__(self, width: int, height: int, seed: Optional[int] = None, cell_occupancy_prob: float = 0.3):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "\n",
    "        # self.max_steps=width*height\n",
    "        # For this simplest possible maze, set the max length to be 2\n",
    "        self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "        # In this maze, all the free space in the maze is observable from any position\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        # dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        dr = 0\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "\n",
    "        # Make sure that any number that is not 1 is 0\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "\n",
    "        return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "        # # Plane 1: Free Space (1s where free space, 0s where obstacles)\n",
    "        # plane_free_space = plane_obstacles == 0\n",
    "\n",
    "        # # Plane 2: Agent's position\n",
    "        # plane_agent = np.zeros_like(plane_obstacles)\n",
    "        # plane_agent[half_width, half_width] = 1\n",
    "\n",
    "        # encoded_observation = np.stack([plane_obstacles, plane_free_space, plane_agent], axis=0)\n",
    "\n",
    "        # return encoded_observation\n",
    "    \n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "    \n",
    "    def get_normalized_steps_left(self, state: State):\n",
    "        return state.steps_left / self.max_steps\n",
    "    \n",
    "    def get_normalized_distances(self):\n",
    "        # Returns the normalized distances in the x and y directions that can be travelled by the agent in 50% of the max steps\n",
    "        scaling_factor = 0.5\n",
    "\n",
    "        return (self.max_steps * scaling_factor / self.width, self.max_steps * scaling_factor / self.height)\n",
    "    \n",
    "    def get_encoded_scalar_features(self, state: State):\n",
    "        return (\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            *self.get_normalized_target_position(),\n",
    "            self.get_normalized_steps_left(state),\n",
    "            *self.get_normalized_distances()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        # In this case we are using binary reward\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "    \n",
    "        return state.reward, False\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            # Set the max steps to be 3 * the L1 distance between source and target\n",
    "            # self.max_steps = 3 * (abs(source[0] - target[0]) + abs(source[1] - target[1]))\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        num_scalar_features = 7  # see Maze.get_encoded_scalar_features\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The single input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + num_scalar_features, 256),  # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + num_scalar_features, 256), # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, scalar_features):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # scalar_features: (batch_size, 7), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, scalar_features], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, scalar_features], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, game: Maze, num_simulations, c_puct, model):\n",
    "        self.game = game\n",
    "        self.num_simulations = num_simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, max_iters = 1000, verbose=True, visualize=True):\n",
    "        state = self.game.get_initial_state()\n",
    "        path = []\n",
    "        memory = []\n",
    "        for i in range(max_iters):\n",
    "            \n",
    "            action_probs = self.search(state)\n",
    "            path.append((state.x, state.y))\n",
    "            memory.append((self.game.get_encoded_observation(state), \n",
    "                           self.game.get_encoded_scalar_features(state),\n",
    "                           action_probs))\n",
    "\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            state = self.game.get_next_state(state, action)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                ret_mem = [(*mem, value) for mem in memory]\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == self.game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    self.game.visualize_path(path)\n",
    "                \n",
    "                return ret_mem\n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(state, self.game.get_valid_actions(state))\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(node.state)\n",
    "                \n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    def query_model(self, state: Maze.State):\n",
    "        tensor_obs = torch.tensor(self.game.get_encoded_observation(state), dtype=torch.float32).unsqueeze(0)\n",
    "        tensor_scalar_features = torch.tensor(self.game.get_encoded_scalar_features(state), dtype=torch.float32).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_scalar_features\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        normalized_policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return normalized_policy, value\n",
    "\n",
    "\n",
    "    def select(self, node: Node) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = child.value_sum / child.visit_count\n",
    "        u_value = self.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        return q_value + u_value\n",
    "    \n",
    "    def expand(self, node: Node, policy) -> None:\n",
    "        if (node.state.x, node.state.y) == self.game.target:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = self.game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  self.game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, search_alg: AlphaMCTS, num_learn_iters, num_self_play_iters, num_train_epochs, train_batch_size, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "        self.num_learn_iters = num_learn_iters\n",
    "        self.num_self_play_iters = num_self_play_iters\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.train_batch_size = train_batch_size\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def self_play(self):\n",
    "        # Initialize game\n",
    "        # For now train on a fixed size maze\n",
    "        game = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "        self.search_alg.game = game\n",
    "        return self.search_alg.play_game(verbose=False, visualize=False)\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.train_batch_size]\n",
    "            obs, scalar_features, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, scalar_features, policy_targets, value_targets = np.array(obs), np.array(scalar_features), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            scalar_features = torch.tensor(scalar_features, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, scalar_features)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Log metrics for the current batch\n",
    "            wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.train_batch_size)\n",
    "        # Log average loss for the epoch\n",
    "        wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self, save_every=1):\n",
    "        wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "            name=cfg.name,\n",
    "            config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "            save_code=True)\n",
    "        \n",
    "        wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        for iteration in range(self.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "            for _ in trange(self.num_self_play_iters):\n",
    "                game_mem = self.self_play()\n",
    "                if game_mem[-1][-1] > 0:  # Assuming positive value means successful game\n",
    "                    successes += 1\n",
    "                memory += game_mem\n",
    "\n",
    "            success_rate = successes / self.num_self_play_iters\n",
    "            # Log the success rate for self-play games\n",
    "            wandb.log({\"success_rate\": success_rate, \"iteration\": iteration})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            # Save if iter divides save_every or if it is the last iteration\n",
    "            if iteration % save_every == 0 or iteration == self.num_learn_iters - 1:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "                # Log model checkpoint to W&B\n",
    "                wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "                wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262095/1200954815.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_model_7.pt\"))\n",
      "/tmp/ipykernel_262095/1200954815.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_optimizer_7.pt\"))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters)\n",
    "# model.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_model_7.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr)\n",
    "# optimizer.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_optimizer_7.pt\"))\n",
    "\n",
    "mcts = AlphaMCTS(maze, num_simulations=cfg.search.num_simulations, c_puct=cfg.search.c_puct, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, \n",
    "                      num_learn_iters=cfg.learn.num_learn_iters, \n",
    "                      num_self_play_iters=cfg.learn.num_self_play_iters,\n",
    "                      num_train_epochs=cfg.learn.num_train_epochs,\n",
    "                      train_batch_size=cfg.learn.train_batch_size)\n",
    "# alphaZero.learn(save_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached target in 1 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262095/4052365493.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{13}.pt\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: (1, 1), policy: [0.322 0.    0.    0.678], policy argmax:Right policy value: 0.8137604594230652\n",
      "search: [0.327 0.    0.    0.673], search argmax: Right\n",
      "Position: (1, 2), policy: [0.    0.135 0.    0.865], policy argmax:Right policy value: 0.8400129675865173\n",
      "search: [0.   0.02 0.   0.98], search argmax: Right\n",
      "Position: (2, 1), policy: [0.907 0.    0.092 0.001], policy argmax:Down policy value: 0.850929319858551\n",
      "search: [0.98 0.   0.02 0.  ], search argmax: Down\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters)\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{13}.pt\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "mcts = AlphaMCTS(maze, num_simulations=cfg.search.num_simulations, c_puct=cfg.search.c_puct, model=model)\n",
    "\n",
    "_ = mcts.play_game()\n",
    "\n",
    "positions = [(x, y) for x in range(1, cfg.maze.width-1) for y in range(1, cfg.maze.height-1)]\n",
    "for pos in positions:\n",
    "    if pos == maze.target:\n",
    "        continue\n",
    "    state = Maze.State(*pos, 1, 0)\n",
    "    policy, value = mcts.query_model(state)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {value}\")\n",
    "    search_probs = mcts.search(state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does seem to have allowed the loss to decrease to lower levels than previously. Even after round 3 of running this script (7+7+14 iterations), the loss is still decreasing, and the success rate is still increasing, so with more training it should get better. \n",
    "\n",
    "The next thing I'm going to do is to implement parallization and GPU and then train this for super long to see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
