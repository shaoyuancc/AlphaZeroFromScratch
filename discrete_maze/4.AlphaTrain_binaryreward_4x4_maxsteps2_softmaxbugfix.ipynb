{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"maze_4x4_binaryreward_maxsteps2_softmaxbugfix\",\n",
    "    \"maze\": {\n",
    "        \"width\": 4,\n",
    "        \"height\": 4,\n",
    "        \"cell_occupancy_prob\": 0,\n",
    "        \"max_steps\": 2\n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 8,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 1\n",
    "    # MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -1\n",
    "\n",
    "    def __init__(self, width: int, height: int, seed: Optional[int] = None, cell_occupancy_prob: float = 0.3):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "\n",
    "        # self.max_steps=width*height\n",
    "        # For this simplest possible maze, set the max length to be 2\n",
    "        self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "        # In this maze, all the free space in the maze is observable from any position\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        # dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        dr = 0\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "\n",
    "        # Make sure that any number that is not 1 is 0\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "\n",
    "        return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "        # # Plane 1: Free Space (1s where free space, 0s where obstacles)\n",
    "        # plane_free_space = plane_obstacles == 0\n",
    "\n",
    "        # # Plane 2: Agent's position\n",
    "        # plane_agent = np.zeros_like(plane_obstacles)\n",
    "        # plane_agent[half_width, half_width] = 1\n",
    "\n",
    "        # encoded_observation = np.stack([plane_obstacles, plane_free_space, plane_agent], axis=0)\n",
    "\n",
    "        # return encoded_observation\n",
    "    \n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        # In this case we are using binary reward\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "    \n",
    "        return state.reward, False\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            # Set the max steps to be 3 * the L1 distance between source and target\n",
    "            # self.max_steps = 3 * (abs(source[0] - target[0]) + abs(source[1] - target[1]))\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The single input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + 4, 256),  # Adding 4 for the positions of agent and target\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "        # NOTE: Do not apply softmax here or in the forward method because it is applied in the loss function\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + 4, 256),  # Adding 4 for the positions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, agent_pos, goal_pos):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # agent_pos and goal_pos: tensors of shape (batch_size, 2), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, agent_pos, goal_pos], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, agent_pos, goal_pos], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, game: Maze, num_simulations, c_puct, model):\n",
    "        self.game = game\n",
    "        self.num_simulations = num_simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, max_iters = 1000, verbose=True, visualize=True):\n",
    "        state = self.game.get_initial_state()\n",
    "        path = []\n",
    "        memory = []\n",
    "        for i in range(max_iters):\n",
    "            \n",
    "            action_probs = self.search(state)\n",
    "            path.append((state.x, state.y))\n",
    "            memory.append((self.game.get_encoded_observation(state), \n",
    "                           self.game.get_normalized_agent_position(state), \n",
    "                           self.game.get_normalized_target_position(),\n",
    "                           action_probs))\n",
    "\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            state = self.game.get_next_state(state, action)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                ret_mem = [(*mem, value) for mem in memory]\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == self.game.target:\n",
    "                        print(f\"Reached target in {i+1} steps: {path}\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps: {path}\")\n",
    "                if visualize:\n",
    "                    self.game.visualize_path(path)\n",
    "                \n",
    "                return ret_mem\n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(state, self.game.get_valid_actions(state))\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(node.state)\n",
    "                \n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    def query_model(self, state: Maze.State):\n",
    "        tensor_obs = torch.tensor(self.game.get_encoded_observation(state), dtype=torch.float32).unsqueeze(0)\n",
    "        tensor_agent_pos = torch.tensor(self.game.get_normalized_agent_position(state), dtype=torch.float32).unsqueeze(0)\n",
    "        tensor_goal_pos = torch.tensor(self.game.get_normalized_target_position(), dtype=torch.float32).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_agent_pos, tensor_goal_pos\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "    def select(self, node: Node) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = child.value_sum / child.visit_count\n",
    "        u_value = self.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        return q_value + u_value\n",
    "    \n",
    "    def expand(self, node: Node, policy) -> None:\n",
    "        if (node.state.x, node.state.y) == self.game.target:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = self.game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  self.game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, search_alg: AlphaMCTS, num_learn_iters, num_self_play_iters, num_train_epochs, train_batch_size, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "        self.num_learn_iters = num_learn_iters\n",
    "        self.num_self_play_iters = num_self_play_iters\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.train_batch_size = train_batch_size\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def self_play(self):\n",
    "        # Initialize game\n",
    "        # For now train on a fixed size maze\n",
    "        game = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "        self.search_alg.game = game\n",
    "        return self.search_alg.play_game(verbose=False, visualize=False)\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.train_batch_size]\n",
    "            obs, agent_pos, goal_pos, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, agent_pos, goal_pos, policy_targets, value_targets = np.array(obs), np.array(agent_pos), np.array(goal_pos), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            agent_pos = torch.tensor(agent_pos, dtype=torch.float32)\n",
    "            goal_pos = torch.tensor(goal_pos, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, agent_pos, goal_pos)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Log metrics for the current batch\n",
    "            wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.train_batch_size)\n",
    "        # Log average loss for the epoch\n",
    "        wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "            name=cfg.name,\n",
    "            config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "            save_code=True)\n",
    "        \n",
    "        wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        for iteration in range(self.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "            for _ in trange(self.num_self_play_iters):\n",
    "                game_mem = self.self_play()\n",
    "                if game_mem[-1][-1] > 0:  # Assuming positive value means successful game\n",
    "                    successes += 1\n",
    "                memory += game_mem\n",
    "\n",
    "            success_rate = successes / self.num_self_play_iters\n",
    "            # Log the success rate for self-play games\n",
    "            wandb.log({\"success_rate\": success_rate, \"iteration\": iteration})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "            # Log model checkpoint to W&B\n",
    "            wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "            wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr)\n",
    "\n",
    "mcts = AlphaMCTS(maze, num_simulations=cfg.search.num_simulations, c_puct=cfg.search.c_puct, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, \n",
    "                      num_learn_iters=cfg.learn.num_learn_iters, \n",
    "                      num_self_play_iters=cfg.learn.num_self_play_iters,\n",
    "                      num_train_epochs=cfg.learn.num_train_epochs,\n",
    "                      train_batch_size=cfg.learn.train_batch_size)\n",
    "# alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated due to timeout in 2 steps: [(2, 2), (1, 2), (2, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_257048/1455727174.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: (1, 1), policy: [0.49209002 0.00075327 0.00072235 0.50643444], policy argmax:Right policy value: 0.7362493276596069\n",
      "search: [0.08163265 0.         0.         0.91836735], search argmax: Right\n",
      "Position: (1, 2), policy: [4.7851162e-04 3.9279696e-01 3.8785030e-04 6.0633671e-01], policy argmax:Right policy value: 0.6727354526519775\n",
      "search: [0.         0.32653061 0.         0.67346939], search argmax: Right\n",
      "Position: (2, 2), policy: [2.8904859e-04 6.5927219e-01 3.4004807e-01 3.9065519e-04], policy argmax:Up policy value: 0.7476921677589417\n",
      "search: [0.         0.93877551 0.06122449 0.        ], search argmax: Up\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "mcts = AlphaMCTS(maze, num_simulations=cfg.search.num_simulations, c_puct=cfg.search.c_puct, model=model)\n",
    "\n",
    "memory = mcts.play_game()\n",
    "\n",
    "positions = [(x, y) for x in range(1, cfg.maze.width-1) for y in range(1, cfg.maze.height-1)]\n",
    "for pos in positions:\n",
    "    if pos == maze.target:\n",
    "        continue\n",
    "    state = Maze.State(*pos, cfg.maze.max_steps, 0)\n",
    "    policy, value = mcts.query_model(state)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {value}\")\n",
    "    search_probs = mcts.search(state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[[1, 1, 1, 1, 1],\n",
       "          [1, 0, 0, 1, 1],\n",
       "          [1, 0, 0, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1]]]),\n",
       "  (0.5, 0.5),\n",
       "  (0.5, 0.25),\n",
       "  array([0.        , 0.93877551, 0.06122449, 0.        ]),\n",
       "  -1),\n",
       " (array([[[1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 0, 0, 1, 1],\n",
       "          [1, 0, 0, 1, 1],\n",
       "          [1, 1, 1, 1, 1]]]),\n",
       "  (0.25, 0.5),\n",
       "  (0.5, 0.25),\n",
       "  array([0.       , 0.3877551, 0.       , 0.6122449]),\n",
       "  -1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that removing the softmax layer from the model did help things a lot. The train_epoch_loss chart (see wandb goes lower than any previous learning run). \n",
    "The model seems to have learnt the right things, now there is no discrepancy between the policy argmax and the search argmax (where I initialize search assuming you have enough steps.)\n",
    "It seems that the times where it fails its because we aren't taking the argmax of the search probabilities, instead we are sampling from them according to the probabilities.\n",
    "\n",
    "One question on my mind is whether we should pass in the remaining number of steps to the networks as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: (1, 1), policy: [0.49209002 0.00075327 0.00072235 0.50643444], policy argmax:Right policy value: 0.7362493276596069\n",
      "search: [0.08163265 0.         0.         0.91836735], search argmax: Right\n",
      "[[[1 1 1 1 1]\n",
      "  [1 1 1 1 1]\n",
      "  [1 1 0 0 1]\n",
      "  [1 1 0 0 1]\n",
      "  [1 1 1 1 1]]]\n",
      "Position: (1, 2), policy: [4.7851162e-04 3.9279696e-01 3.8785030e-04 6.0633671e-01], policy argmax:Right policy value: 0.6727354526519775\n",
      "search: [0.         0.32653061 0.         0.67346939], search argmax: Right\n",
      "[[[1 1 1 1 1]\n",
      "  [1 1 1 1 1]\n",
      "  [1 0 0 1 1]\n",
      "  [1 0 0 1 1]\n",
      "  [1 1 1 1 1]]]\n",
      "Position: (2, 2), policy: [2.8904859e-04 6.5927219e-01 3.4004807e-01 3.9065519e-04], policy argmax:Up policy value: 0.7476921677589417\n",
      "search: [0.         0.93877551 0.06122449 0.        ], search argmax: Up\n",
      "[[[1 1 1 1 1]\n",
      "  [1 0 0 1 1]\n",
      "  [1 0 0 1 1]\n",
      "  [1 1 1 1 1]\n",
      "  [1 1 1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "positions = [(x, y) for x in range(1, cfg.maze.width-1) for y in range(1, cfg.maze.height-1)]\n",
    "for pos in positions:\n",
    "    if pos == maze.target:\n",
    "        continue\n",
    "    state = Maze.State(*pos, cfg.maze.max_steps, 0)\n",
    "    policy, value = mcts.query_model(state)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {value}\")\n",
    "    search_probs = mcts.search(state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "    print(maze.get_encoded_observation(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position (1,2) is showing the observation for being in the top right corner instead of the bottom left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
