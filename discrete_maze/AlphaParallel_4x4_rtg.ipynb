{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Set precision to 3 decimal places\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, implement reward to go \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"parallel_maze_4x4_rtg_maxsteps2\",\n",
    "    \"maze\": {\n",
    "        \"width\": 4,\n",
    "        \"height\": 4,\n",
    "        \"cell_occupancy_prob\": 0,\n",
    "        \"max_steps\": 2\n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 10,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_parallel_games\": 250,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"use_wandb\": True,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that the reward stored in the state is unnormalized\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 100\n",
    "    MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -50\n",
    "\n",
    "    def __init__(self, width: int, height: int, seed: Optional[int] = None, cell_occupancy_prob: float = 0.3):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "\n",
    "        # Max steps configuration\n",
    "        # Option 1: Set the max steps to be the width * height\n",
    "        # self.max_steps=width*height\n",
    "        # Option 2: Set the max steps to be 2 * the L1 distance between source and target\n",
    "        # self.max_steps = 2 * (abs(self.source[0] - self.target[0]) + abs(self.source[1] - self.target[1]))\n",
    "        # Option 3: Manually set the max steps\n",
    "        self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "\n",
    "        # Make sure that any number that is not 1 is 0\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "        return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "\n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "    \n",
    "    def get_normalized_steps_left(self, state: State):\n",
    "        return state.steps_left / self.max_steps\n",
    "    \n",
    "    def get_normalized_distances(self):\n",
    "        # Returns the normalized distances in the x and y directions that can be travelled by the agent in 50% of the max steps\n",
    "        scaling_factor = 0.5\n",
    "\n",
    "        return (self.max_steps * scaling_factor / self.width, self.max_steps * scaling_factor / self.height)\n",
    "    \n",
    "    def get_encoded_scalar_features(self, state: State):\n",
    "        return (\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            *self.get_normalized_target_position(),\n",
    "            self.get_normalized_steps_left(state),\n",
    "            *self.get_normalized_distances()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        \"\"\"Returns the unnormalized reward and whether the episode is terminated\"\"\"\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "        return state.reward, False\n",
    "    \n",
    "    def normalize_reward(self, reward):\n",
    "        # Normalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 2 * ((reward - min_reward) / (max_reward - min_reward)) - 1\n",
    "    \n",
    "    def unnormalize_reward(self, normalized_reward):\n",
    "        # Unnormalize the reward between -1 and 1\n",
    "        max_reward = Maze.TARGET_REWARD\n",
    "        min_reward = Maze.TIMEOUT_REWARD + Maze.MOVE_REWARD * self.max_steps\n",
    "        return 0.5 * (normalized_reward + 1) * (max_reward - min_reward) + min_reward\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.32894736842105265\n",
      "-3.736842105263158\n",
      "-0.5789473684210527\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(5, 5, cell_occupancy_prob=0)\n",
    "print(maze.normalize_reward(-1))\n",
    "# Note that the normalization is not linear. The following are not equal:\n",
    "print(maze.normalize_reward(-10) + 10 * maze.normalize_reward(-1))\n",
    "print(maze.normalize_reward(-20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-50.0\n",
      "-40.0\n",
      "-30.0\n",
      "-20.0\n",
      "-9.999999999999993\n",
      "0.0\n",
      "10.0\n",
      "20.0\n",
      "30.0\n",
      "40.0\n",
      "50.0\n",
      "60.0\n",
      "70.00000000000001\n",
      "80.0\n",
      "90.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(-50, 100, 10):\n",
    "    print(maze.unnormalize_reward(maze.normalize_reward(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters, device):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        SCALAR_FEATURES_SIZE = 7  # see Maze.get_encoded_scalar_features\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The single input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + SCALAR_FEATURES_SIZE, 256),  # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + SCALAR_FEATURES_SIZE, 256), # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, scalar_features):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # scalar_features: (batch_size, 7), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, scalar_features], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, scalar_features], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Search node in the MCTS tree\"\"\"\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "class GameEpisode:\n",
    "    \"\"\"Stateful episode of a game\"\"\"\n",
    "    def __init__(self, game: Maze):\n",
    "        self.game = game\n",
    "        self.state: Maze.State = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.reward_history = []\n",
    "        self.root: Optional[Node] = Node(self.state, self.game.get_valid_actions(self.state))\n",
    "        self.node: Optional[Node] = None\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, search_cfg, model: ResNet):\n",
    "        self.cfg = search_cfg\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, game: Maze, max_iters = 1000, verbose=True, visualize=True):\n",
    "        \"\"\"Play a single game\"\"\"\n",
    "        state = game.get_initial_state()\n",
    "        path = []\n",
    "        root = Node(state, game.get_valid_actions(state))\n",
    "        for i in range(max_iters):\n",
    "            action_probs = self.search(game, root=root)\n",
    "            path.append((state.x, state.y))\n",
    "            print(f\"Step {i+1}: {state}, action_probs: {action_probs}\")\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            for child in root.children:\n",
    "                if child.last_action == action:\n",
    "                    # Set the child as the new root to preserve the search tree\n",
    "                    root = child\n",
    "                    break\n",
    "            state = root.state\n",
    "            \n",
    "            value, is_terminal = game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    game.visualize_path(path)\n",
    "                \n",
    "                return path, value\n",
    "                \n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, game: Maze, state: Optional[Maze.State] = None, root: Optional[Node] = None) -> np.ndarray:\n",
    "        if root is None and state is not None:\n",
    "            root = Node(state, game.get_valid_actions(state))\n",
    "        elif state is None and root is None:\n",
    "            assert False, \"Either state or root must be provided\"\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node, game)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(state=node.state, game=game)\n",
    "                value = game.unnormalize_reward(value)\n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy, game=game)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batch_search(self, episodes: List[GameEpisode]):\n",
    "        \n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            # Collect nodes for expansion and evaluation\n",
    "            for ep in episodes:\n",
    "                ep.node = None # Reset the node marked for expansion and evaluation for each episode\n",
    "                node = ep.root\n",
    "                # Selection all the way down till a leaf node\n",
    "                while not node.is_leaf:\n",
    "                    node = self.select(node, ep.game)\n",
    "\n",
    "                # Evaluate the leaf node\n",
    "                value, is_terminal = ep.game.get_value_and_terminated(node.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    self.backpropagate(node, value)\n",
    "                else:\n",
    "                    ep.node = node # Mark the leaf node for expansion and evaluation\n",
    "\n",
    "            # Batch query the model for the policy and value\n",
    "            expandable_episodes = [ep_idx for ep_idx, ep in enumerate(episodes) if ep.node is not None]\n",
    "\n",
    "            if len(expandable_episodes) > 0:\n",
    "                obs = np.stack([episodes[ep_idx].game.get_encoded_observation(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                scalar_features = np.stack([episodes[ep_idx].game.get_encoded_scalar_features(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                tensor_obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "                tensor_scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.model(\n",
    "                    tensor_obs, tensor_scalar_features\n",
    "                    )\n",
    "                \n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "            \n",
    "            # Expand the nodes and backpropagate\n",
    "            for batch_idx, ep_idx in enumerate(expandable_episodes):\n",
    "                node = episodes[ep_idx].node\n",
    "                ep_policy, ep_value = policy[batch_idx], value[batch_idx].item()\n",
    "\n",
    "                valid_policy = np.zeros_like(ep_policy)\n",
    "                valid_policy[node.valid_actions] = ep_policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                ep_value = ep.game.unnormalize_reward(ep_value)\n",
    "                self.expand(node, policy=valid_policy, game=episodes[ep_idx].game)\n",
    "                self.backpropagate(node, ep_value)\n",
    "    \n",
    "    def query_model(self, state: Maze.State, game: Maze) -> Tuple[np.ndarray, float]:\n",
    "        tensor_obs = torch.tensor(game.get_encoded_observation(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        tensor_scalar_features = torch.tensor(game.get_encoded_scalar_features(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_scalar_features\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        normalized_policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return normalized_policy, value\n",
    "\n",
    "    def select(self, node: Node, game: Maze) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child, game) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node, game: Maze) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # Q-value needs to be noramalized between -1 and 1 for this formula.\n",
    "            q_value = game.normalize_reward(child.value_sum / child.visit_count)\n",
    "\n",
    "        u_value = self.cfg.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        \n",
    "        return q_value + u_value\n",
    "\n",
    "    \n",
    "    def expand(self, node: Node, policy, game: Maze) -> None:\n",
    "        _, is_terminal = game.get_value_and_terminated(node.state)\n",
    "        assert not is_terminal, \"Cannot expand a terminal node\"\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        \n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        \"\"\"Takes in unnormalized value\"\"\"\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model: ResNet, optimizer, search_alg: AlphaMCTS, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "\n",
    "        self.cfg = cfg.learn\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def self_play(self):\n",
    "        # For now train on a fixed size maze\n",
    "        # Initialize all games and an episode for each game\n",
    "        episodes = [GameEpisode(\n",
    "                            Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "                        ) for _ in range(self.cfg.num_parallel_games)\n",
    "                    ]\n",
    "        ret_mem = []\n",
    "        n_successes = 0\n",
    "        while len(episodes) > 0:\n",
    "            \n",
    "            self.search_alg.batch_search(episodes)\n",
    "\n",
    "            # Serially process the episodes\n",
    "            for i in range(len(episodes))[::-1]:\n",
    "                ep = episodes[i]\n",
    "\n",
    "                action_probs = np.zeros(ep.game.action_size)\n",
    "                for child in ep.root.children:\n",
    "                    action_probs[child.last_action] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                ep.memory.append((ep.game.get_encoded_observation(ep.root.state), \n",
    "                                  ep.game.get_encoded_scalar_features(ep.root.state),\n",
    "                                  action_probs))\n",
    "                ep.reward_history.append(ep.root.state.reward)\n",
    "\n",
    "                action = np.random.choice(ep.game.action_size, p=action_probs)\n",
    "                for child in ep.root.children:\n",
    "                    if child.last_action == action:\n",
    "                        # Set the child as the new root to preserve the search tree\n",
    "                        ep.root = child\n",
    "                        break\n",
    "                ep.state = ep.root.state\n",
    "\n",
    "                final_reward, is_terminal = ep.game.get_value_and_terminated(ep.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    # Unroll the reward history and memory\n",
    "                    for mem, reward_to_go in zip(ep.memory, ep.reward_history):\n",
    "                        reward_to_go = final_reward - reward_to_go\n",
    "                        ret_mem.append((*mem, ep.game.normalize_reward(reward_to_go)))\n",
    "                    if (ep.state.x, ep.state.y) == ep.game.target:\n",
    "                        n_successes += 1\n",
    "                    del episodes[i]\n",
    "        \n",
    "        return ret_mem, n_successes\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.cfg.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.cfg.train_batch_size]\n",
    "            obs, scalar_features, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, scalar_features, policy_targets, value_targets = np.array(obs), np.array(scalar_features), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "            scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, scalar_features)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log metrics for the current batch\n",
    "                wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.cfg.train_batch_size)\n",
    "        if self.cfg.use_wandb:\n",
    "            # Log average loss for the epoch\n",
    "            wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self, save_every=1):\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "                name=cfg.name,\n",
    "                config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "                save_code=True)\n",
    "            \n",
    "            wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        for iteration in range(self.cfg.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "            for _ in trange(self.cfg.num_self_play_iters // self.cfg.num_parallel_games):\n",
    "                batch_episode_mems, num_episode_successes = self.self_play()\n",
    "                successes += num_episode_successes\n",
    "                memory += batch_episode_mems\n",
    "\n",
    "            success_rate = successes / self.cfg.num_self_play_iters\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log the success rate for self-play games\n",
    "                wandb.log({\"success_rate\": success_rate, \"iteration\": iteration})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.cfg.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            # Save if iter divides save_every or if it is the last iteration\n",
    "            if (iteration % save_every == 0 and iteration != 0) or iteration == self.cfg.num_learn_iters - 1:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "                if self.cfg.use_wandb:\n",
    "                    # Log model checkpoint to W&B\n",
    "                    wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "                    wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaoyuan\u001b[0m (\u001b[33mcontact_placement\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shaoyuan/Documents/Software/AlphaZeroFromScratch/discrete_maze/wandb/run-20241114_102529-g8jgnanw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/g8jgnanw' target=\"_blank\">parallel_maze_4x4_rtg_maxsteps2</a></strong> to <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/g8jgnanw' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/g8jgnanw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866b9512a72e4bcbb43f0d71774c54ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a77fabad2c94b32822a0627a891d3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd42fc04cac4418d92d71a52fb1db22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29a51a649a147caa3b2f05b2e76fda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08b55442764418b872620fd6ade3c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6942939391144c25a79d20b3a2cbe75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae011c0f3a244577aba4fa8fcf15756d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6fae4f9239409e8daf2caf9e0ab642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd3c9ec4c2a4962bd4bf1ee43fc0af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228f8cf6df09445a81e35e083bb9bef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0c5486d07f4475bdbb2df7f13ffe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbe3a05fdb84e499b52d559a6468382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1595ee7430e445faabce83e65d3345f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353ceb63d2a74ca2805b7bf3cb181fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a09f7a2e3ee4bea94aac41479229377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ecbbdd14a04ac7a285686ed82aa6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d899f33346497781509b388ec0cb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23232135459c4e939c3783a705fe3f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee95150d8f4f70a589d9d59a75f190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08e8c5dc81d478382166301f59caf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11079dc5c7dd45488d02108c39aa2c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.234 MB of 0.234 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>iteration</td><td></td></tr><tr><td>success_rate</td><td></td></tr><tr><td>train_epoch_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.99985</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>iteration</td><td>9</td></tr><tr><td>success_rate</td><td>0.926</td></tr><tr><td>train_epoch_loss</td><td>1.02273</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">parallel_maze_4x4_rtg_maxsteps2</strong> at: <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/g8jgnanw' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/g8jgnanw</a><br/> View project at: <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze</a><br/>Synced 4 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241114_102529-g8jgnanw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "maze.visualize_path()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "# model.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_model_7.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr, weight_decay=cfg.learn.weight_decay)\n",
    "# optimizer.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_optimizer_7.pt\"))\n",
    "\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, seed=0)\n",
    "alphaZero.learn(save_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without GPU it took 4m 33s to run,\n",
    "With GPU it surprisingly took 5m 24s to run. Since our observation matrices are small I don't expect GPU support to make a huge difference. Perhaps it's also introducing some overhead?\n",
    "\n",
    "With parallelization of the self play (num parallel = 100) it took 1m 5s, (without wandb took 44s)\n",
    "(num parallel= 250) took 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: State(x=1, y=1, steps_left=2, reward=0), action_probs: [0.082 0.    0.    0.918]\n",
      "Step 2: State(x=1, y=2, steps_left=1, reward=-1), action_probs: [0.    0.358 0.    0.642]\n",
      "Terminated due to timeout in 2 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_381080/2649961046.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiu0lEQVR4nO3de2xUdf7/8dcU6FQiM7Ur7bRSEKwUuZcqMJgAarUiIXb/0UUjyAJeUhIRo9KNK4pfHS94i0FhY7C7KkFdBRK81iIQoKBUGstFYrWhaDpFRWag6ojt5/eHP2ettKWFOdPpp89H8v5jznw+57x7OOblmXNmjssYYwQAgMWSuroBAACcRtgBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCs51jYHTlyRDfeeKM8Ho9SU1M1d+5cHT9+vN05U6dOlcvlalG33XabUy0CAHoIl1O/jTlt2jTV19dr5cqVOnHihObMmaNLLrlEq1evbnPO1KlTNXToUC1dujS6rG/fvvJ4PE60CADoIXo7sdL9+/frvffe0yeffKKLL75YkvTcc8/pmmuu0bJly5SVldXm3L59+8rn8znRFgCgh3Ik7CoqKpSamhoNOkkqKChQUlKSdu7cqb/+9a9tzn311Vf1yiuvyOfzacaMGfrnP/+pvn37tjk+EokoEolEXzc3N+vIkSP6y1/+IpfLFZs/CAAQN8YYHTt2TFlZWUpKis3VNkfCLhgMKj09veWGevdWWlqagsFgm/NuuOEGDRo0SFlZWfrss89077336sCBA3rrrbfanBMIBPTggw/GrHcAQGI4dOiQBgwYEJuVmU649957jaR2a//+/ebhhx82Q4cOPWl+//79zfPPP9/h7ZWXlxtJpqamps0xP//8swmFQtGqq6s7ZY8URVFU4tfRo0c7E1Ht6tSZ3V133aWbb7653TFDhgyRz+fT4cOHWyz/9ddfdeTIkU5dj5swYYIkqaamRhdccEGrY9xut9xud4fXCQDoHmJ5KapTYde/f3/179//lOP8fr+OHj2qyspK5efnS5I2btyo5ubmaIB1RFVVlSQpMzOzM20CANBSzM4R/+Tqq682eXl5ZufOnWbr1q3mwgsvNDNnzoy+//XXX5vc3Fyzc+dOY4wxNTU1ZunSpWbXrl2mtrbWrF+/3gwZMsRMnjy5U9sNhUJdfupNURRFnXmFQqGYZZJjYff999+bmTNnmrPPPtt4PB4zZ84cc+zYsej7tbW1RpL56KOPjDHG1NXVmcmTJ5u0tDTjdrtNTk6Oufvuuzv9xxJ2FEVRdlQsw86xL5V3lXA4LK/X29VtAADOUCgUitmPivDbmAAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrOR52y5cv1/nnn6+UlBRNmDBBH3/8cbvj33jjDQ0bNkwpKSkaNWqU3nnnHadbBADYzjhozZo1Jjk52axatcrs3bvXzJ8/36SmppqGhoZWx2/bts306tXLPP7442bfvn3mvvvuM3369DHV1dUd3mYoFDKSKIqiqG5eoVAoVnFkHA278ePHm+Li4ujrpqYmk5WVZQKBQKvjr7vuOjN9+vQWyyZMmGBuvfXWDm+TsKMoirKjYhl2jn2M+csvv6iyslIFBQXRZUlJSSooKFBFRUWrcyoqKlqMl6TCwsI2x0tSJBJROBxuUQAA/JFjYffdd9+pqalJGRkZLZZnZGQoGAy2OicYDHZqvCQFAgF5vd5oZWdnn3nzAACrdPu7MUtKShQKhaJ16NChrm4JAJBgeju14nPPPVe9evVSQ0NDi+UNDQ3y+XytzvH5fJ0aL0lut1tut/vMGwYAWMuxM7vk5GTl5+ervLw8uqy5uVnl5eXy+/2tzvH7/S3GS1JZWVmb4wEA6JCY3erSijVr1hi3221KS0vNvn37zC233GJSU1NNMBg0xhhz0003mcWLF0fHb9u2zfTu3dssW7bM7N+/3yxZsoSvHlAURfXQ6jZfPTDGmOeee84MHDjQJCcnm/Hjx5sdO3ZE35syZYqZPXt2i/Gvv/66GTp0qElOTjYjRowwb7/9dqe2R9hRFEXZUbEMO5cxxsgi4XBYXq+3q9sAAJyhUCgkj8cTk3V1+7sxAQA4FcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9x8Nu+fLlOv/885WSkqIJEybo448/bnNsaWmpXC5Xi0pJSXG6RQCA5RwNu9dee02LFi3SkiVL9Omnn2rMmDEqLCzU4cOH25zj8XhUX18frYMHDzrZIgCgB3A07J566inNnz9fc+bM0fDhw7VixQr17dtXq1atanOOy+WSz+eLVkZGhpMtAgB6gN5OrfiXX35RZWWlSkpKosuSkpJUUFCgioqKNucdP35cgwYNUnNzs8aNG6dHHnlEI0aMaHN8JBJRJBKJvg6Hw7H5A9Bhpqsb6IFcD3R1Bz3MA13dAM6UY2d23333nZqamk46M8vIyFAwGGx1Tm5urlatWqX169frlVdeUXNzsyZNmqSvv/66ze0EAgF5vd5oZWdnx/TvAAB0fwl1N6bf79esWbM0duxYTZkyRW+99Zb69++vlStXtjmnpKREoVAoWocOHYpjxwCA7sCxjzHPPfdc9erVSw0NDS2WNzQ0yOfzdWgdffr0UV5enmpqatoc43a75Xa7z6hXAIDdHDuzS05OVn5+vsrLy6PLmpubVV5eLr/f36F1NDU1qbq6WpmZmU61CQDoARw7s5OkRYsWafbs2br44os1fvx4PfPMM2psbNScOXMkSbNmzdJ5552nQCAgSVq6dKkmTpyonJwcHT16VE888YQOHjyoefPmOdkmAMByjobd9ddfr2+//Vb333+/gsGgxo4dq/feey9600pdXZ2Skv53cvnDDz9o/vz5CgaDOuecc5Sfn6/t27dr+PDhTrYJALCcyxhj1Z3j4XBYXq+3q9voUaw6gLoJvnoQZw90dQM9UygUksfjicm6EupuTAAAnEDYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKznaNht2bJFM2bMUFZWllwul9atW3fKOZs2bdK4cePkdruVk5Oj0tJSJ1sEAPQAjoZdY2OjxowZo+XLl3dofG1traZPn67LLrtMVVVVWrhwoebNm6f333/fyTYBAJbr7eTKp02bpmnTpnV4/IoVKzR48GA9+eSTkqSLLrpIW7du1dNPP63CwsJW50QiEUUikejrcDh8Zk0DAKyTUNfsKioqVFBQ0GJZYWGhKioq2pwTCATk9XqjlZ2d7XSbAIBuJqHCLhgMKiMjo8WyjIwMhcNh/fTTT63OKSkpUSgUitahQ4fi0SoAoBtx9GPMeHC73XK73V3dBgAggSXUmZ3P51NDQ0OLZQ0NDfJ4PDrrrLO6qCsAQHeXUGHn9/tVXl7eYllZWZn8fn8XdQQAsIGjYXf8+HFVVVWpqqpK0m9fLaiqqlJdXZ2k3663zZo1Kzr+tttu01dffaV77rlHn3/+uZ5//nm9/vrruvPOO51sEwBgOUfDbteuXcrLy1NeXp4kadGiRcrLy9P9998vSaqvr48GnyQNHjxYb7/9tsrKyjRmzBg9+eSTevHFF9v82gEAAB3hMsaYrm4ilsLhsLxeb1e30aNYdQB1E64HurqDHuaBrm6gZwqFQvJ4PDFZV0JdswMAwAmEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeo6G3ZYtWzRjxgxlZWXJ5XJp3bp17Y7ftGmTXC7XSRUMBp1sEwBgOUfDrrGxUWPGjNHy5cs7Ne/AgQOqr6+PVnp6ukMdAgB6gt5OrnzatGmaNm1ap+elp6crNTW1Q2MjkYgikUj0dTgc7vT2AAB2czTsTtfYsWMViUQ0cuRIPfDAA7r00kvbHBsIBPTggw/GsTv8mcuYrm6hx2GPx5frAVdXt4AzlFA3qGRmZmrFihV688039eabbyo7O1tTp07Vp59+2uackpIShUKhaB06dCiOHQMAuoOEOrPLzc1Vbm5u9PWkSZP05Zdf6umnn9bLL7/c6hy32y232x2vFgEA3VBCndm1Zvz48aqpqenqNgAA3VjCh11VVZUyMzO7ug0AQDfm6MeYx48fb3FWVltbq6qqKqWlpWngwIEqKSnRN998o//85z+SpGeeeUaDBw/WiBEj9PPPP+vFF1/Uxo0b9cEHHzjZJgDAco6G3a5du3TZZZdFXy9atEiSNHv2bJWWlqq+vl51dXXR93/55Rfddddd+uabb9S3b1+NHj1aH374YYt1AADQWS5j7LpvPBwOy+v1dnUbPYtdh1C3wB6PL5eLrx50hVAoJI/HE5N1Jfw1OwAAzhRhBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwnqNhFwgEdMkll6hfv35KT09XUVGRDhw4cMp5b7zxhoYNG6aUlBSNGjVK77zzjpNtAgAs52jYbd68WcXFxdqxY4fKysp04sQJXXXVVWpsbGxzzvbt2zVz5kzNnTtXu3fvVlFRkYqKirRnzx4nWwUAWMxljDHx2ti3336r9PR0bd68WZMnT251zPXXX6/GxkZt2LAhumzixIkaO3asVqxYccpthMNheb3emPWMDojfIYT/jz0eXy6Xq6tb6JFCoZA8Hk9M1hXXa3ahUEiSlJaW1uaYiooKFRQUtFhWWFioioqKVsdHIhGFw+EWBQDAH8Ut7Jqbm7Vw4UJdeumlGjlyZJvjgsGgMjIyWizLyMhQMBhsdXwgEJDX641WdnZ2TPsGAHR/cQu74uJi7dmzR2vWrInpektKShQKhaJ16NChmK4fAND99Y7HRhYsWKANGzZoy5YtGjBgQLtjfT6fGhoaWixraGiQz+drdbzb7Zbb7Y5ZrwAA+zh6ZmeM0YIFC7R27Vpt3LhRgwcPPuUcv9+v8vLyFsvKysrk9/udahMAYDlHz+yKi4u1evVqrV+/Xv369Yted/N6vTrrrLMkSbNmzdJ5552nQCAgSbrjjjs0ZcoUPfnkk5o+fbrWrFmjXbt26V//+peTrQIAbGYcpN/ukD6pXnrppeiYKVOmmNmzZ7eY9/rrr5uhQ4ea5ORkM2LECPP22293eJuhUKjN7VIOlTFUnAvx1eX/jfXQCoVCMfs3jOv37OKB79l1AbsOoW6BPR5ffM+ua3Tb79kBANAVCDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUcDbtAIKBLLrlE/fr1U3p6uoqKinTgwIF255SWlsrlcrWolJQUJ9sEAFjO0bDbvHmziouLtWPHDpWVlenEiRO66qqr1NjY2O48j8ej+vr6aB08eNDJNgEAluvt5Mrfe++9Fq9LS0uVnp6uyspKTZ48uc15LpdLPp/PydYAAD2Io2H3Z6FQSJKUlpbW7rjjx49r0KBBam5u1rhx4/TII49oxIgRrY6NRCKKRCLR1+FwOHYNo2Ncrq7uoMdhjwOdE7cbVJqbm7Vw4UJdeumlGjlyZJvjcnNztWrVKq1fv16vvPKKmpubNWnSJH399detjg8EAvJ6vdHKzs526k8AAHRTLmOMiceGbr/9dr377rvaunWrBgwY0OF5J06c0EUXXaSZM2fqoYceOun91s7sCDwA6P5CoZA8Hk9M1hWXjzEXLFigDRs2aMuWLZ0KOknq06eP8vLyVFNT0+r7brdbbrc7Fm0CACzl6MeYxhgtWLBAa9eu1caNGzV48OBOr6OpqUnV1dXKzMx0oEMAQE/g6JldcXGxVq9erfXr16tfv34KBoOSJK/Xq7POOkuSNGvWLJ133nkKBAKSpKVLl2rixInKycnR0aNH9cQTT+jgwYOaN2+ek60CACzmaNi98MILkqSpU6e2WP7SSy/p5ptvliTV1dUpKel/J5g//PCD5s+fr2AwqHPOOUf5+fnavn27hg8f7mSrAACLxe0GlXgJh8Pyer1d3QYA4AzF8gYVfhsTAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD1Hw+6FF17Q6NGj5fF45PF45Pf79e6777Y754033tCwYcOUkpKiUaNG6Z133nGyRQBAD+Bo2A0YMECPPvqoKisrtWvXLl1++eW69tprtXfv3lbHb9++XTNnztTcuXO1e/duFRUVqaioSHv27HGyTQCA5VzGGBPPDaalpemJJ57Q3LlzT3rv+uuvV2NjozZs2BBdNnHiRI0dO1YrVqzo0PrD4bC8Xm/M+gUAdI1QKCSPxxOTdcXtml1TU5PWrFmjxsZG+f3+VsdUVFSooKCgxbLCwkJVVFS0ud5IJKJwONyiAAD4I8fDrrq6Wmeffbbcbrduu+02rV27VsOHD291bDAYVEZGRotlGRkZCgaDba4/EAjI6/VGKzs7O6b9AwC6P8fDLjc3V1VVVdq5c6duv/12zZ49W/v27YvZ+ktKShQKhaJ16NChmK0bAGCH3k5vIDk5WTk5OZKk/Px8ffLJJ3r22We1cuXKk8b6fD41NDS0WNbQ0CCfz9fm+t1ut9xud2ybBgBYJe7fs2tublYkEmn1Pb/fr/Ly8hbLysrK2rzGBwBAhxgHLV682GzevNnU1taazz77zCxevNi4XC7zwQcfGGOMuemmm8zixYuj47dt22Z69+5tli1bZvbv32+WLFli+vTpY6qrqzu8zVAoZCRRFEVR3bxCoVDM8sjRjzEPHz6sWbNmqb6+Xl6vV6NHj9b777+vK6+8UpJUV1enpKT/nVxOmjRJq1ev1n333ad//OMfuvDCC7Vu3TqNHDnSyTYBAJaL+/fsnMb37ADADt3ye3YAAHQVwg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPUfD7oUXXtDo0aPl8Xjk8Xjk9/v17rvvtjm+tLRULperRaWkpDjZIgCgB+jt5MoHDBigRx99VBdeeKGMMfr3v/+ta6+9Vrt379aIESNanePxeHTgwIHoa5fL5WSLAIAewNGwmzFjRovXDz/8sF544QXt2LGjzbBzuVzy+Xwd3kYkElEkEom+DoVCp9csACChGGNitq64XbNramrSmjVr1NjYKL/f3+a448ePa9CgQcrOzta1116rvXv3trveQCAgr9cbrYEDB8a6dQBAF/j+++9jti6XiWV0tqK6ulp+v18///yzzj77bK1evVrXXHNNq2MrKir0xRdfaPTo0QqFQlq2bJm2bNmivXv3asCAAa3O+fOZ3dGjRzVo0CDV1dXJ6/U68jc5IRwOKzs7W4cOHZLH4+nqdjqlu/ZO3/FF3/HXXXsPhUIaOHCgfvjhB6WmpsZknY5+jClJubm5qqqqUigU0n//+1/Nnj1bmzdv1vDhw08a6/f7W5z1TZo0SRdddJFWrlyphx56qNX1u91uud3uk5Z7vd5u9Y/7u99v5umOumvv9B1f9B1/3bX3pKTYffjoeNglJycrJydHkpSfn69PPvlEzz77rFauXHnKuX369FFeXp5qamqcbhMAYLG4f8+uubm5xceO7WlqalJ1dbUyMzMd7goAYDNHz+xKSko0bdo0DRw4UMeOHdPq1au1adMmvf/++5KkWbNm6bzzzlMgEJAkLV26VBMnTlROTo6OHj2qJ554QgcPHtS8efM6vE23260lS5a0+tFmIuuufUvdt3f6ji/6jr/u2rsTfTt6g8rcuXNVXl6u+vp6eb1ejR49Wvfee6+uvPJKSdLUqVN1/vnnq7S0VJJ055136q233lIwGNQ555yj/Px8/d///Z/y8vKcahEA0AM4fjcmAABdjd/GBABYj7ADAFiPsAMAWI+wAwBYz4qwO3LkiG688UZ5PB6lpqZq7ty5On78eLtzpk6detLjhG677TZH+1y+fLnOP/98paSkaMKECfr444/bHf/GG29o2LBhSklJ0ahRo/TOO+842l97OtN7IjyqacuWLZoxY4aysrLkcrm0bt26U87ZtGmTxo0bJ7fbrZycnOhdwvHW2d43bdp00v52uVwKBoPxaVi//UbtJZdcon79+ik9PV1FRUUtnl7Slq4+xk+n70Q4vqXOP0JN6vr9LXXdo9+sCLsbb7xRe/fuVVlZmTZs2KAtW7bolltuOeW8+fPnq76+PlqPP/64Yz2+9tprWrRokZYsWaJPP/1UY8aMUWFhoQ4fPtzq+O3bt2vmzJmaO3eudu/eraKiIhUVFWnPnj2O9diWzvYu/fbzRH/ctwcPHoxjx1JjY6PGjBmj5cuXd2h8bW2tpk+frssuu0xVVVVauHCh5s2bF/1OaDx1tvffHThwoMU+T09Pd6jDk23evFnFxcXasWOHysrKdOLECV111VVqbGxsc04iHOOn07fU9ce39L9HqFVWVmrXrl26/PLL2/3x/ETY36fTtxSj/W26uX379hlJ5pNPPokue/fdd43L5TLffPNNm/OmTJli7rjjjjh0+Jvx48eb4uLi6OumpiaTlZVlAoFAq+Ovu+46M3369BbLJkyYYG699VZH+2xNZ3t/6aWXjNfrjVN3pybJrF27tt0x99xzjxkxYkSLZddff70pLCx0sLNT60jvH330kZFkfvjhh7j01BGHDx82kszmzZvbHJNIx/jvOtJ3oh3ff3TOOeeYF198sdX3EnF//669vmO1v7v9mV1FRYVSU1N18cUXR5cVFBQoKSlJO3fubHfuq6++qnPPPVcjR45USUmJfvzxR0d6/OWXX1RZWamCgoLosqSkJBUUFKiioqLVORUVFS3GS1JhYWGb451yOr1LnX9UU1dLlP19JsaOHavMzExdeeWV2rZtW5f28vtzJdPS0tock4j7vCN9S4l3fHfkEWqJuL+devRbaxz/IWinBYPBkz6u6d27t9LS0tq9ZnHDDTdo0KBBysrK0meffaZ7771XBw4c0FtvvRXzHr/77js1NTUpIyOjxfKMjAx9/vnnrc4JBoOtjo/ndRjp9HrPzc3VqlWrWjyqadKkSe0+qqmrtbW/w+GwfvrpJ5111lld1NmpZWZmasWKFbr44osViUT04osvaurUqdq5c6fGjRsX936am5u1cOFCXXrppRo5cmSb4xLlGP9dR/tOpOP7z49QW7t2batPlJESa393pu9Y7e+EDbvFixfrsccea3fM/v37T3v9f7ymN2rUKGVmZuqKK67Ql19+qQsuuOC014vTe1QTTl9ubq5yc3OjrydNmqQvv/xSTz/9tF5++eW491NcXKw9e/Zo69atcd/2meho34l0fHfmEWqJxOlHv7UmYcPurrvu0s0339zumCFDhsjn8510o8Svv/6qI0eOyOfzdXh7EyZMkCTV1NTEPOzOPfdc9erVSw0NDS2WNzQ0tNmjz+fr1HinnE7vf9YdHtXU1v72eDwJfVbXlvHjx3dJ2CxYsCB6k9ip/q87UY5xqXN9/1lXHt+deYRaIu3vrnj0W8Jes+vfv7+GDRvWbiUnJ8vv9+vo0aOqrKyMzt24caOam5ujAdYRVVVVkuTI44SSk5OVn5+v8vLy6LLm5maVl5e3+Tm13+9vMV6SysrK2v1c2wmn0/ufdYdHNSXK/o6VqqqquO5vY4wWLFigtWvXauPGjRo8ePAp5yTCPj+dvv8skY7v9h6hlgj7uy1xefTbGd/ikgCuvvpqk5eXZ3bu3Gm2bt1qLrzwQjNz5szo+19//bXJzc01O3fuNMYYU1NTY5YuXWp27dplamtrzfr1682QIUPM5MmTHetxzZo1xu12m9LSUrNv3z5zyy23mNTUVBMMBo0xxtx0001m8eLF0fHbtm0zvXv3NsuWLTP79+83S5YsMX369DHV1dWO9Rir3h988EHz/vvvmy+//NJUVlaav/3tbyYlJcXs3bs3bj0fO3bM7N692+zevdtIMk899ZTZvXu3OXjwoDHGmMWLF5ubbropOv6rr74yffv2NXfffbfZv3+/Wb58uenVq5d577334tbz6fb+9NNPm3Xr1pkvvvjCVFdXmzvuuMMkJSWZDz/8MG4933777cbr9ZpNmzaZ+vr6aP3444/RMYl4jJ9O34lwfBvz23GwefNmU1tbaz777DOzePFi43K5zAcffNBq34mwv0+n71jtbyvC7vvvvzczZ840Z599tvF4PGbOnDnm2LFj0fdra2uNJPPRRx8ZY4ypq6szkydPNmlpacbtdpucnBxz9913m1Ao5Gifzz33nBk4cKBJTk4248ePNzt27Ii+N2XKFDN79uwW419//XUzdOhQk5ycbEaMGGHefvttR/trT2d6X7hwYXRsRkaGueaaa8ynn34a135/vx3/z/V7n7NnzzZTpkw5ac7YsWNNcnKyGTJkiHnppZfi2vMf++hM74899pi54IILTEpKiklLSzNTp041GzdujGvPrfUrqcU+TMRj/HT6ToTj2xhj/v73v5tBgwaZ5ORk079/f3PFFVdEA6O1vo3p+v1tTOf7jtX+5hE/AADrJew1OwAAYoWwAwBYj7ADAFiPsAMAWI+wAwBYj7ADAFiPsAMAWI+wAwBYj7ADAFiPsAMAWI+wAwBY7/8B0UisK/h2ETYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(1, 1), (1, 2), (2, 2)], -52)\n",
      "Position: (1, 1), policy: [0.605 0.001 0.001 0.394], policy argmax:Down policy value: 0.7318284511566162\n",
      "search: [0.082 0.    0.    0.918], search argmax: Right\n",
      "Position: (1, 2), policy: [0.    0.362 0.    0.638], policy argmax:Right policy value: 0.6334097385406494\n",
      "search: [0.    0.367 0.    0.633], search argmax: Right\n",
      "Position: (2, 2), policy: [0.    0.641 0.358 0.   ], policy argmax:Up policy value: 0.6977143287658691\n",
      "search: [0.    0.959 0.041 0.   ], search argmax: Up\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{100}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_round3_model_13.pt\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "mcts = AlphaMCTS(search_cfg=cfg.search, model=model)\n",
    "\n",
    "print(mcts.play_game(game=maze))\n",
    "\n",
    "positions = [(x, y) for x in range(1, cfg.maze.width-1) for y in range(1, cfg.maze.height-1)]\n",
    "for pos in positions:\n",
    "    if pos == maze.target:\n",
    "        continue\n",
    "    state = Maze.State(*pos, 1, 0)\n",
    "    policy, final_reward = mcts.query_model(state, game=maze)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {final_reward}\")\n",
    "    search_probs = mcts.search(game=maze, state=state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
