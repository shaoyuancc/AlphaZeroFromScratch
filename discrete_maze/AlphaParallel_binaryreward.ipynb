{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Set precision to 3 decimal places\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'm going to do exactly the same as with maze_4x4_binaryreward_maxsteps16, but implement the following changes from AlphaTweaks and AlphaParallel.\n",
    "\n",
    "Changes to be implemented:\n",
    "1. Use GPU as device\n",
    "2. \n",
    "\n",
    "Changes to be skipped:\n",
    "1. Use weight decay in optimizer\n",
    "2. Temperature\n",
    "3. Dirichlet Noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"parallel_maze_4x4_binaryreward_maxsteps16\",\n",
    "    \"maze\": {\n",
    "        \"width\": 4,\n",
    "        \"height\": 4,\n",
    "        \"cell_occupancy_prob\": 0,\n",
    "        \"max_steps\": 16\n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 8,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_parallel_games\": 250,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001,\n",
    "        \"use_wandb\": True,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 1\n",
    "    # MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -1\n",
    "\n",
    "    def __init__(self, width: int, height: int, seed: Optional[int] = None, cell_occupancy_prob: float = 0.3):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "\n",
    "        # self.max_steps=width*height\n",
    "        # For this simplest possible maze, set the max length to be 2\n",
    "        self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "        # In this maze, all the free space in the maze is observable from any position\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        # dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        dr = 0\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "\n",
    "        # Make sure that any number that is not 1 is 0\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "\n",
    "        return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "        # # Plane 1: Free Space (1s where free space, 0s where obstacles)\n",
    "        # plane_free_space = plane_obstacles == 0\n",
    "\n",
    "        # # Plane 2: Agent's position\n",
    "        # plane_agent = np.zeros_like(plane_obstacles)\n",
    "        # plane_agent[half_width, half_width] = 1\n",
    "\n",
    "        # encoded_observation = np.stack([plane_obstacles, plane_free_space, plane_agent], axis=0)\n",
    "\n",
    "        # return encoded_observation\n",
    "    \n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "    \n",
    "    def get_normalized_steps_left(self, state: State):\n",
    "        return state.steps_left / self.max_steps\n",
    "    \n",
    "    def get_normalized_distances(self):\n",
    "        # Returns the normalized distances in the x and y directions that can be travelled by the agent in 50% of the max steps\n",
    "        scaling_factor = 0.5\n",
    "\n",
    "        return (self.max_steps * scaling_factor / self.width, self.max_steps * scaling_factor / self.height)\n",
    "    \n",
    "    def get_encoded_scalar_features(self, state: State):\n",
    "        return (\n",
    "            *self.get_normalized_agent_position(state),\n",
    "            *self.get_normalized_target_position(),\n",
    "            self.get_normalized_steps_left(state),\n",
    "            *self.get_normalized_distances()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        # In this case we are using binary reward\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "    \n",
    "        return state.reward, False\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            # Set the max steps to be 3 * the L1 distance between source and target\n",
    "            # self.max_steps = 3 * (abs(source[0] - target[0]) + abs(source[1] - target[1]))\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters, device):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "        SCALAR_FEATURES_SIZE = 7  # see Maze.get_encoded_scalar_features\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The single input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + SCALAR_FEATURES_SIZE, 256),  # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + SCALAR_FEATURES_SIZE, 256), # Adding scalar features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, scalar_features):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # scalar_features: (batch_size, 7), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, scalar_features], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, scalar_features], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "class GameEpisode:\n",
    "    def __init__(self, game: Maze):\n",
    "        self.game = game\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, game: Maze, search_cfg, model: ResNet):\n",
    "        self.game = game\n",
    "        self.cfg = search_cfg\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, max_iters = 1000, verbose=True, visualize=True):\n",
    "        state = self.game.get_initial_state()\n",
    "        path = []\n",
    "        memory = []\n",
    "        for i in range(max_iters):\n",
    "            \n",
    "            action_probs = self.search(state)\n",
    "            path.append((state.x, state.y))\n",
    "            memory.append((self.game.get_encoded_observation(state), \n",
    "                           self.game.get_encoded_scalar_features(state),\n",
    "                           action_probs))\n",
    "\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            state = self.game.get_next_state(state, action)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                ret_mem = [(*mem, value) for mem in memory]\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == self.game.target:\n",
    "                        print(f\"Reached target in {i+1} steps\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps\")\n",
    "                if visualize:\n",
    "                    self.game.visualize_path(path)\n",
    "                \n",
    "                return ret_mem\n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(state, self.game.get_valid_actions(state))\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(node.state)\n",
    "                \n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batch_search(self, episodes: List[GameEpisode]):\n",
    "        for ep in episodes:\n",
    "            # TODO: Right now we are throwing away the search tree after each search, but we should keep the child.\n",
    "            ep.root = Node(ep.state, ep.game.get_valid_actions(ep.state))\n",
    "        \n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.cfg.num_simulations):\n",
    "            # Collect nodes for expansion and evaluation\n",
    "            for ep in episodes:\n",
    "                ep.node = None\n",
    "                node = ep.root\n",
    "                # Selection all the way down till a leaf node\n",
    "                while not node.is_leaf:\n",
    "                    node = self.select(node)\n",
    "\n",
    "                # Evaluate the leaf node\n",
    "                value, is_terminal = ep.game.get_value_and_terminated(node.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    self.backpropagate(node, value)\n",
    "                else:\n",
    "                    ep.node = node # Save the leaf node for expansion and evaluation\n",
    "\n",
    "            # Batch query the model for the policy and value\n",
    "            expandable_episodes = [ep_idx for ep_idx, ep in enumerate(episodes) if ep.node is not None]\n",
    "\n",
    "            if len(expandable_episodes) > 0:\n",
    "                obs = np.stack([episodes[ep_idx].game.get_encoded_observation(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                scalar_features = np.stack([episodes[ep_idx].game.get_encoded_scalar_features(episodes[ep_idx].node.state) for ep_idx in expandable_episodes])\n",
    "                tensor_obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "                tensor_scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.model(\n",
    "                    tensor_obs, tensor_scalar_features\n",
    "                    )\n",
    "                \n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "            \n",
    "            # Expand the nodes and backpropagate\n",
    "            for batch_idx, ep_idx in enumerate(expandable_episodes):\n",
    "                node = episodes[ep_idx].node\n",
    "                ep_policy, ep_value = policy[batch_idx], value[batch_idx].item()\n",
    "\n",
    "                valid_policy = np.zeros_like(ep_policy)\n",
    "                valid_policy[node.valid_actions] = ep_policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy, game=episodes[ep_idx].game)\n",
    "                self.backpropagate(node, ep_value)\n",
    "            \n",
    "\n",
    "    \n",
    "    def query_model(self, state: Maze.State):\n",
    "        tensor_obs = torch.tensor(self.game.get_encoded_observation(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        tensor_scalar_features = torch.tensor(self.game.get_encoded_scalar_features(state), dtype=torch.float32, device=self.model.device).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_scalar_features\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        normalized_policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return normalized_policy, value\n",
    "\n",
    "\n",
    "    def select(self, node: Node) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = child.value_sum / child.visit_count\n",
    "        u_value = self.cfg.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "\n",
    "        return q_value + u_value\n",
    "\n",
    "    \n",
    "    def expand(self, node: Node, policy, game: Optional[Maze] = None) -> None:\n",
    "        if game is None:\n",
    "            game = self.game\n",
    "        value, is_terminal = game.get_value_and_terminated(node.state)\n",
    "        assert not is_terminal, \"Cannot expand a terminal node\"\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        \n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model: ResNet, optimizer, search_alg: AlphaMCTS, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "\n",
    "        self.cfg = cfg.learn\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def self_play(self):\n",
    "        # Initialize game\n",
    "        # For now train on a fixed size maze\n",
    "        # Initialize all games and an episode for each game\n",
    "        episodes = [GameEpisode(\n",
    "                            Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "                        ) for _ in range(self.cfg.num_parallel_games)\n",
    "                    ]\n",
    "        ret_mem = []\n",
    "        n_successes = 0\n",
    "        while len(episodes) > 0:\n",
    "            \n",
    "            self.search_alg.batch_search(episodes)\n",
    "\n",
    "            # Serially process the episodes\n",
    "            for i in range(len(episodes))[::-1]:\n",
    "                ep = episodes[i]\n",
    "\n",
    "                action_probs = np.zeros(ep.game.action_size)\n",
    "                for child in ep.root.children:\n",
    "                    action_probs[child.last_action] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                ep.memory.append((ep.game.get_encoded_observation(ep.root.state), \n",
    "                                  ep.game.get_encoded_scalar_features(ep.root.state),\n",
    "                                  action_probs))\n",
    "\n",
    "                action = np.random.choice(ep.game.action_size, p=action_probs)\n",
    "                ep.state = ep.game.get_next_state(ep.state, action)\n",
    "\n",
    "                value, is_terminal = ep.game.get_value_and_terminated(ep.state)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for mem in ep.memory:\n",
    "                        ret_mem.append((*mem, value))\n",
    "                    if (ep.state.x, ep.state.y) == ep.game.target:\n",
    "                        n_successes += 1\n",
    "                    del episodes[i]\n",
    "        \n",
    "        return ret_mem, n_successes\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.cfg.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.cfg.train_batch_size]\n",
    "            obs, scalar_features, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, scalar_features, policy_targets, value_targets = np.array(obs), np.array(scalar_features), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.model.device)\n",
    "            scalar_features = torch.tensor(scalar_features, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, scalar_features)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log metrics for the current batch\n",
    "                wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.cfg.train_batch_size)\n",
    "        if self.cfg.use_wandb:\n",
    "            # Log average loss for the epoch\n",
    "            wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self, save_every=1):\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "                name=cfg.name,\n",
    "                config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "                save_code=True)\n",
    "            \n",
    "            wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        for iteration in range(self.cfg.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "            for _ in trange(self.cfg.num_self_play_iters // self.cfg.num_parallel_games):\n",
    "                batch_episode_mems, num_episode_successes = self.self_play()\n",
    "                successes += num_episode_successes\n",
    "                memory += batch_episode_mems\n",
    "\n",
    "            success_rate = successes / self.cfg.num_self_play_iters\n",
    "            if self.cfg.use_wandb:\n",
    "                # Log the success rate for self-play games\n",
    "                wandb.log({\"success_rate\": success_rate, \"iteration\": iteration})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.cfg.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            # Save if iter divides save_every or if it is the last iteration\n",
    "            if (iteration % save_every == 0 and iteration != 0) or iteration == self.cfg.num_learn_iters - 1:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "                torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "                if self.cfg.use_wandb:\n",
    "                    # Log model checkpoint to W&B\n",
    "                    wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "                    wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaoyuan\u001b[0m (\u001b[33mcontact_placement\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shaoyuan/Documents/Software/AlphaZeroFromScratch/discrete_maze/wandb/run-20241113_150308-h7zdu3ed</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/h7zdu3ed' target=\"_blank\">parallel_maze_4x4_binaryreward_maxsteps16</a></strong> to <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/h7zdu3ed' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/h7zdu3ed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad7f11d41b2440e9e9d23284919f4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e48af19dbe14d3fbe732f07af4d7c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051b916635124629bfd1d573601683a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8e0a0e051942dcbb84e772e06c63ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3338411fbb14334abc2a1a7dec0b690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2831402e081c4abaa978e3d635fb9802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8da54fa5fde43b28e6f423d61654840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2569c0ea7d4541d6b77d0ce8888879a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8de9d1cac194b7eaf3919033589da8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2ef7bec931454f8c01ba2f11122cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a103a5e892c94a838843296701b1d296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d522095da3534974a7dfe217d753682c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baec82e0df84a898b71fbaa978eac28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c62d16fb3d4478b9169f533efb5e300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1533992be1cc4ffebb84a4c3e3149d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26a7486dfea44e9a3d8717960a9d1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd5f03e082d4b8db7d22e8499f3bdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.249 MB of 0.249 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>iteration</td><td></td></tr><tr><td>success_rate</td><td></td></tr><tr><td>train_epoch_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.68607</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>iteration</td><td>7</td></tr><tr><td>success_rate</td><td>1</td></tr><tr><td>train_epoch_loss</td><td>0.71419</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">parallel_maze_4x4_binaryreward_maxsteps16</strong> at: <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/h7zdu3ed' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze/runs/h7zdu3ed</a><br/> View project at: <a href='https://wandb.ai/contact_placement/alpha-zero-discrete-maze' target=\"_blank\">https://wandb.ai/contact_placement/alpha-zero-discrete-maze</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_150308-h7zdu3ed/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "maze.visualize_path()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "# model.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_model_7.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr)\n",
    "# optimizer.load_state_dict(torch.load(\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_optimizer_7.pt\"))\n",
    "\n",
    "mcts = AlphaMCTS(maze, search_cfg=cfg.search, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, seed=0)\n",
    "alphaZero.learn(save_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without GPU it took 4m 33s to run,\n",
    "With GPU it surprisingly took 5m 24s to run. Since our observation matrices are small I don't expect GPU support to make a huge difference. Perhaps it's also introducing some overhead?\n",
    "\n",
    "With parallelization of the self play (num parallel = 100) it took 1m 5s, (without wandb took 44s)\n",
    "(num parallel= 250) took 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiq0lEQVR4nO3de2xUZf7H8c8U6FQiM7Ur7bRSUKwWuUMVGEygahWVELv/qGiksoCXlETUuNCNuyiujq78vMQgYAx2VyV4BRJUtBaBCAWl0lguEqsNRdMpq8gMdNcR2+f3h3HWSltamDMzffp+Jd8/5szznPPt4ZiPZ+acOS5jjBEAABZLSXQDAAA4jbADAFiPsAMAWI+wAwBYj7ADAFiPsAMAWI+wAwBYj7ADAFiPsAMAWI+wAwBYz7GwO3LkiG699VZ5PB6lp6drzpw5On78eKdzCgsL5XK52tRdd93lVIsAgF7C5dRvY1533XVqbGzUypUrdeLECc2ePVuXXXaZVq9e3eGcwsJCXXzxxVqyZEl0Wf/+/eXxeJxoEQDQS/R1YqX79+/Xxo0b9emnn+rSSy+VJD333HO6/vrrtXTpUuXk5HQ4t3///vL5fE60BQDopRwJu6qqKqWnp0eDTpKKioqUkpKinTt36o9//GOHc1999VW98sor8vl8mjFjhv7617+qf//+HY6PRCKKRCLR162trTpy5Ij+8Ic/yOVyxeYPAgDEjTFGx44dU05OjlJSYvNtmyNhFwwGlZmZ2XZDffsqIyNDwWCww3m33HKLhgwZopycHH3++edauHChDhw4oLfffrvDOYFAQA8//HDMegcAJIdDhw5p0KBBsVmZ6YaFCxcaSZ3W/v37zaOPPmouvvjik+YPHDjQPP/8813eXmVlpZFk6urqOhzz448/mlAoFK2GhoZT9khRFEUlfx09erQ7EdWpbp3Z3X///br99ts7HTN06FD5fD4dPny4zfKff/5ZR44c6db3cRMnTpQk1dXV6cILL2x3jNvtltvt7vI6AQA9Qyy/iupW2A0cOFADBw485Ti/36+jR4+qurpaBQUFkqRNmzaptbU1GmBdUVNTI0nKzs7uTpsAALQVs3PE37n22mvNuHHjzM6dO83HH39sLrroIjNz5szo+998843Jz883O3fuNMYYU1dXZ5YsWWJ27dpl6uvrzfr1683QoUPNlClTurXdUCiU8FNviqIo6swrFArFLJMcC7vvv//ezJw505x99tnG4/GY2bNnm2PHjkXfr6+vN5LMRx99ZIwxpqGhwUyZMsVkZGQYt9tt8vLyzAMPPNDtP5awoyiKsqNiGXaO3VSeKOFwWF6vN9FtAADOUCgUitmPivDbmAAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrOR52y5Yt0/nnn6+0tDRNnDhRn3zySafj33jjDQ0bNkxpaWkaNWqU3n33XadbBADYzjhozZo1JjU11axatcrs3bvXzJs3z6Snp5umpqZ2x2/bts306dPH/OMf/zD79u0zDz74oOnXr5+pra3t8jZDoZCRRFEURfXwCoVCsYoj42jYTZgwwZSWlkZft7S0mJycHBMIBNodf+ONN5rp06e3WTZx4kRz5513dnmbhB1FUZQdFcuwc+xjzJ9++knV1dUqKiqKLktJSVFRUZGqqqranVNVVdVmvCRNmzatw/GSFIlEFA6H2xQAAL/lWNh99913amlpUVZWVpvlWVlZCgaD7c4JBoPdGi9JgUBAXq83Wrm5uWfePADAKj3+asyysjKFQqFoHTp0KNEtAQCSTF+nVnzuueeqT58+ampqarO8qalJPp+v3Tk+n69b4yXJ7XbL7XafecMAAGs5dmaXmpqqgoICVVZWRpe1traqsrJSfr+/3Tl+v7/NeEmqqKjocDwAAF0Ss0td2rFmzRrjdrtNeXm52bdvn7njjjtMenq6CQaDxhhjbrvtNrNo0aLo+G3btpm+ffuapUuXmv3795vFixdz6wFFUVQvrR5z64Exxjz33HNm8ODBJjU11UyYMMHs2LEj+t7UqVNNSUlJm/Gvv/66ufjii01qaqoZMWKEeeedd7q1PcKOoijKjopl2LmMMUYWCYfD8nq9iW4DAHCGQqGQPB5PTNbV46/GBADgVAg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1HA+7ZcuW6fzzz1daWpomTpyoTz75pMOx5eXlcrlcbSotLc3pFgEAlnM07F577TXdd999Wrx4sT777DONGTNG06ZN0+HDhzuc4/F41NjYGK2DBw862SIAoBdwNOyeeuopzZs3T7Nnz9bw4cO1YsUK9e/fX6tWrepwjsvlks/ni1ZWVpaTLQIAeoG+Tq34p59+UnV1tcrKyqLLUlJSVFRUpKqqqg7nHT9+XEOGDFFra6vGjx+vxx57TCNGjOhwfCQSUSQSib4Oh8Ox+QPQZcaYRLcAOMrlciW6BZwhx87svvvuO7W0tJx0ZpaVlaVgMNjunPz8fK1atUrr16/XK6+8otbWVk2ePFnffPNNh9sJBALyer3Rys3NjenfAQDo+ZLqaky/369Zs2Zp7Nixmjp1qt5++20NHDhQK1eu7HBOWVmZQqFQtA4dOhTHjgEAPYFjH2Oee+656tOnj5qamtosb2pqks/n69I6+vXrp3Hjxqmurq7DMW63W263+4x6BQDYzbEzu9TUVBUUFKiysjK6rLW1VZWVlfL7/V1aR0tLi2pra5Wdne1UmwCAXsCxMztJuu+++1RSUqJLL71UEyZM0DPPPKPm5mbNnj1bkjRr1iydd955CgQCkqQlS5Zo0qRJysvL09GjR/Xkk0/q4MGDmjt3rpNtAgAs52jY3XTTTfr3v/+tv/3tbwoGgxo7dqw2btwYvWiloaFBKSn/O7n84YcfNG/ePAWDQZ1zzjkqKCjQ9u3bNXz4cCfbBABYzmUsu248HA7L6/Umuo1exbJDCDgJtx4kRigUksfjicm6kupqTAAAnEDYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKznaNht3bpVM2bMUE5Ojlwul9atW3fKOZs3b9b48ePldruVl5en8vJyJ1sEAPQCjoZdc3OzxowZo2XLlnVpfH19vaZPn64rrrhCNTU1WrBggebOnav333/fyTYBAJZzGWNMXDbkcmnt2rUqLi7ucMzChQv1zjvvaM+ePdFlN998s44ePaqNGze2OycSiSgSiURfh8Nh5ebmxqxvnFqcDiEgYVwuV6Jb6JVCoZA8Hk9M1pVU39lVVVWpqKiozbJp06apqqqqwzmBQEBerzdaBB0A4PeSKuyCwaCysrLaLMvKylI4HNZ///vfdueUlZUpFApF69ChQ/FoFQDQg/RNdANnyu12y+12J7oNAEASS6ozO5/Pp6ampjbLmpqa5PF4dNZZZyWoKwBAT5dUYef3+1VZWdlmWUVFhfx+f4I6AgDYwNGwO378uGpqalRTUyPpl1sLampq1NDQIOmX79tmzZoVHX/XXXfp66+/1p///Gd98cUXev755/X666/r3nvvdbJNAIDtjIM++ugjI+mkKikpMcYYU1JSYqZOnXrSnLFjx5rU1FQzdOhQ89JLL3Vrm6FQqN1tUs4VYLtE/zfWWysUCsXs3zBu99nFSzgcltfrTXQbvYplhxBwEu6zSwxr77MDAMAJhB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqOht3WrVs1Y8YM5eTkyOVyad26dZ2O37x5s1wu10kVDAadbBMAYDlHw665uVljxozRsmXLujXvwIEDamxsjFZmZqZDHQIAeoO+Tq78uuuu03XXXdfteZmZmUpPT+/S2EgkokgkEn0dDoe7vT0AgN0cDbvTNXbsWEUiEY0cOVIPPfSQLr/88g7HBgIBPfzww3HsDidxuRLdQa/jeijRHQA9S1JdoJKdna0VK1borbfe0ltvvaXc3FwVFhbqs88+63BOWVmZQqFQtA4dOhTHjgEAPUFSndnl5+crPz8/+nry5Mn66quv9PTTT+vll19ud47b7Zbb7Y5XiwCAHiipzuzaM2HCBNXV1SW6DQBAD5b0YVdTU6Ps7OxEtwEA6MEc/Rjz+PHjbc7K6uvrVVNTo4yMDA0ePFhlZWX69ttv9a9//UuS9Mwzz+iCCy7QiBEj9OOPP+rFF1/Upk2b9MEHHzjZJgDAco6G3a5du3TFFVdEX993332SpJKSEpWXl6uxsVENDQ3R93/66Sfdf//9+vbbb9W/f3+NHj1aH374YZt1AADQXS5jjEl0E7EUDofl9XoT3UavYtUB1ENw60GcPZToBnqnUCgkj8cTk3Ul/Xd2AACcKcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9R8MuEAjosssu04ABA5SZmani4mIdOHDglPPeeOMNDRs2TGlpaRo1apTeffddJ9sEAFjO0bDbsmWLSktLtWPHDlVUVOjEiRO65ppr1Nzc3OGc7du3a+bMmZozZ452796t4uJiFRcXa8+ePU62CgCwmMsYY+K1sX//+9/KzMzUli1bNGXKlHbH3HTTTWpubtaGDRuiyyZNmqSxY8dqxYoVp9xGOByW1+uNWc84tbgdQIhyPZToDnqZhxLdQO8UCoXk8Xhisq64fmcXCoUkSRkZGR2OqaqqUlFRUZtl06ZNU1VVVbvjI5GIwuFwmwIA4LfiFnatra1asGCBLr/8co0cObLDccFgUFlZWW2WZWVlKRgMtjs+EAjI6/VGKzc3N6Z9AwB6vriFXWlpqfbs2aM1a9bEdL1lZWUKhULROnToUEzXDwDo+frGYyPz58/Xhg0btHXrVg0aNKjTsT6fT01NTW2WNTU1yefztTve7XbL7XbHrFcAgH0cPbMzxmj+/Plau3atNm3apAsuuOCUc/x+vyorK9ssq6iokN/vd6pNAIDlHD2zKy0t1erVq7V+/XoNGDAg+r2b1+vVWWedJUmaNWuWzjvvPAUCAUnSPffco6lTp+r//u//NH36dK1Zs0a7du3SCy+84GSrAACLOXpmt3z5coVCIRUWFio7Oztar732WnRMQ0ODGhsbo68nT56s1atX64UXXtCYMWP05ptvat26dZ1e1AIAQGfiep9dPHCfXfxZdQD1ENxnF2cPJbqB3qnH3mcHAEAiEHYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOs5GnaBQECXXXaZBgwYoMzMTBUXF+vAgQOdzikvL5fL5WpTaWlpTrYJALCco2G3ZcsWlZaWaseOHaqoqNCJEyd0zTXXqLm5udN5Ho9HjY2N0Tp48KCTbQIALNfXyZVv3Lixzevy8nJlZmaqurpaU6ZM6XCey+WSz+dzsjUAQC/iaNj9XigUkiRlZGR0Ou748eMaMmSIWltbNX78eD322GMaMWJEu2MjkYgikUj0dTgcjl3D6BJXohvojR5KdANAzxK3C1RaW1u1YMECXX755Ro5cmSH4/Lz87Vq1SqtX79er7zyilpbWzV58mR988037Y4PBALyer3Rys3NdepPAAD0UC5jjInHhu6++2699957+vjjjzVo0KAuzztx4oQuueQSzZw5U4888shJ77d3ZkfgAUDPFwqF5PF4YrKuuHyMOX/+fG3YsEFbt27tVtBJUr9+/TRu3DjV1dW1+77b7Zbb7Y5FmwAASzn6MaYxRvPnz9fatWu1adMmXXDBBd1eR0tLi2pra5Wdne1AhwCA3sDRM7vS0lKtXr1a69ev14ABAxQMBiVJXq9XZ511liRp1qxZOu+88xQIBCRJS5Ys0aRJk5SXl6ejR4/qySef1MGDBzV37lwnWwUAWMzRsFu+fLkkqbCwsM3yl156SbfffrskqaGhQSkp/zvB/OGHHzRv3jwFg0Gdc845Kigo0Pbt2zV8+HAnWwUAWCxuF6jESzgcltfrTXQbAIAzFMsLVPhtTACA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1HA275cuXa/To0fJ4PPJ4PPL7/Xrvvfc6nfPGG29o2LBhSktL06hRo/Tuu+862SIAoBdwNOwGDRqkxx9/XNXV1dq1a5euvPJK3XDDDdq7d2+747dv366ZM2dqzpw52r17t4qLi1VcXKw9e/Y42SYAwHIuY4yJ5wYzMjL05JNPas6cOSe9d9NNN6m5uVkbNmyILps0aZLGjh2rFStWdGn94XBYXq83Zv0CABIjFArJ4/HEZF1x+86upaVFa9asUXNzs/x+f7tjqqqqVFRU1GbZtGnTVFVV1eF6I5GIwuFwmwIA4LccD7va2lqdffbZcrvduuuuu7R27VoNHz683bHBYFBZWVltlmVlZSkYDHa4/kAgIK/XG63c3NyY9g8A6PkcD7v8/HzV1NRo586duvvuu1VSUqJ9+/bFbP1lZWUKhULROnToUMzWDQCwQ1+nN5Camqq8vDxJUkFBgT799FM9++yzWrly5UljfT6fmpqa2ixramqSz+frcP1ut1tutzu2TQMArBL3++xaW1sViUTafc/v96uysrLNsoqKig6/4wMAoEuMgxYtWmS2bNli6uvrzeeff24WLVpkXC6X+eCDD4wxxtx2221m0aJF0fHbtm0zffv2NUuXLjX79+83ixcvNv369TO1tbVd3mYoFDKSKIqiqB5eoVAoZnnk6MeYhw8f1qxZs9TY2Civ16vRo0fr/fff19VXXy1JamhoUErK/04uJ0+erNWrV+vBBx/UX/7yF1100UVat26dRo4c6WSbAADLxf0+O6dxnx0A2KFH3mcHAECiEHYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6zkadsuXL9fo0aPl8Xjk8Xjk9/v13nvvdTi+vLxcLperTaWlpTnZIgCgF+jr5MoHDRqkxx9/XBdddJGMMfrnP/+pG264Qbt379aIESPanePxeHTgwIHoa5fL5WSLAIBewNGwmzFjRpvXjz76qJYvX64dO3Z0GHYul0s+n6/L24hEIopEItHXoVDo9JoFACQVY0zM1hW37+xaWlq0Zs0aNTc3y+/3dzju+PHjGjJkiHJzc3XDDTdo7969na43EAjI6/VGa/DgwbFuHQCQAN9//33M1uUysYzOdtTW1srv9+vHH3/U2WefrdWrV+v6669vd2xVVZW+/PJLjR49WqFQSEuXLtXWrVu1d+9eDRo0qN05vz+zO3r0qIYMGaKGhgZ5vV5H/iYnhMNh5ebm6tChQ/J4PIlup1t6au/0HV/0HX89tfdQKKTBgwfrhx9+UHp6ekzW6ejHmJKUn5+vmpoahUIhvfnmmyopKdGWLVs0fPjwk8b6/f42Z32TJ0/WJZdcopUrV+qRRx5pd/1ut1tut/uk5V6vt0f94/7q14t5eqKe2jt9xxd9x19P7T0lJXYfPjoedqmpqcrLy5MkFRQU6NNPP9Wzzz6rlStXnnJuv379NG7cONXV1TndJgDAYnG/z661tbXNx46daWlpUW1trbKzsx3uCgBgM0fP7MrKynTddddp8ODBOnbsmFavXq3Nmzfr/ffflyTNmjVL5513ngKBgCRpyZIlmjRpkvLy8nT06FE9+eSTOnjwoObOndvlbbrdbi1evLjdjzaTWU/tW+q5vdN3fNF3/PXU3p3o29ELVObMmaPKyko1NjbK6/Vq9OjRWrhwoa6++mpJUmFhoc4//3yVl5dLku699169/fbbCgaDOuecc1RQUKC///3vGjdunFMtAgB6AcevxgQAINH4bUwAgPUIOwCA9Qg7AID1CDsAgPWsCLsjR47o1ltvlcfjUXp6uubMmaPjx493OqewsPCkxwndddddjva5bNkynX/++UpLS9PEiRP1ySefdDr+jTfe0LBhw5SWlqZRo0bp3XffdbS/znSn92R4VNPWrVs1Y8YM5eTkyOVyad26daecs3nzZo0fP15ut1t5eXnRq4Tjrbu9b968+aT97XK5FAwG49OwfvmN2ssuu0wDBgxQZmamiouL2zy9pCOJPsZPp+9kOL6l7j9CTUr8/pYS9+g3K8Lu1ltv1d69e1VRUaENGzZo69atuuOOO045b968eWpsbIzWP/7xD8d6fO2113Tfffdp8eLF+uyzzzRmzBhNmzZNhw8fbnf89u3bNXPmTM2ZM0e7d+9WcXGxiouLtWfPHsd67Eh3e5d++Xmi3+7bgwcPxrFjqbm5WWPGjNGyZcu6NL6+vl7Tp0/XFVdcoZqaGi1YsEBz586N3hMaT93t/VcHDhxos88zMzMd6vBkW7ZsUWlpqXbs2KGKigqdOHFC11xzjZqbmzuckwzH+On0LSX++Jb+9wi16upq7dq1S1deeWWnP56fDPv7dPqWYrS/TQ+3b98+I8l8+umn0WXvvfeecblc5ttvv+1w3tSpU80999wThw5/MWHCBFNaWhp93dLSYnJyckwgEGh3/I033mimT5/eZtnEiRPNnXfe6Wif7elu7y+99JLxer1x6u7UJJm1a9d2OubPf/6zGTFiRJtlN910k5k2bZqDnZ1aV3r/6KOPjCTzww8/xKWnrjh8+LCRZLZs2dLhmGQ6xn/Vlb6T7fj+rXPOOce8+OKL7b6XjPv7V531Hav93ePP7KqqqpSenq5LL700uqyoqEgpKSnauXNnp3NfffVVnXvuuRo5cqTKysr0n//8x5Eef/rpJ1VXV6uoqCi6LCUlRUVFRaqqqmp3TlVVVZvxkjRt2rQOxzvldHqXuv+opkRLlv19JsaOHavs7GxdffXV2rZtW0J7+fW5khkZGR2OScZ93pW+peQ7vrvyCLVk3N9OPfqtPY7/ELTTgsHgSR/X9O3bVxkZGZ1+Z3HLLbdoyJAhysnJ0eeff66FCxfqwIEDevvtt2Pe43fffaeWlhZlZWW1WZ6VlaUvvvii3TnBYLDd8fH8HkY6vd7z8/O1atWqNo9qmjx5cqePakq0jvZ3OBzWf//7X5111lkJ6uzUsrOztWLFCl166aWKRCJ68cUXVVhYqJ07d2r8+PFx76e1tVULFizQ5ZdfrpEjR3Y4LlmO8V91te9kOr5//wi1tWvXtvtEGSm59nd3+o7V/k7asFu0aJGeeOKJTsfs37//tNf/2+/0Ro0apezsbF111VX66quvdOGFF572enF6j2rC6cvPz1d+fn709eTJk/XVV1/p6aef1ssvvxz3fkpLS7Vnzx59/PHHcd/2mehq38l0fHfnEWrJxOlHv7UnacPu/vvv1+23397pmKFDh8rn8510ocTPP/+sI0eOyOfzdXl7EydOlCTV1dXFPOzOPfdc9enTR01NTW2WNzU1ddijz+fr1ninnE7vv9cTHtXU0f72eDxJfVbXkQkTJiQkbObPnx+9SOxU/9edLMe41L2+fy+Rx3d3HqGWTPs7EY9+S9rv7AYOHKhhw4Z1WqmpqfL7/Tp69Kiqq6ujczdt2qTW1tZogHVFTU2NJDnyOKHU1FQVFBSosrIyuqy1tVWVlZUdfk7t9/vbjJekioqKTj/XdsLp9P57PeFRTcmyv2OlpqYmrvvbGKP58+dr7dq12rRpky644IJTzkmGfX46ff9eMh3fnT1CLRn2d0fi8ui3M77EJQlce+21Zty4cWbnzp3m448/NhdddJGZOXNm9P1vvvnG5Ofnm507dxpjjKmrqzNLliwxu3btMvX19Wb9+vVm6NChZsqUKY71uGbNGuN2u015ebnZt2+fueOOO0x6eroJBoPGGGNuu+02s2jRouj4bdu2mb59+5qlS5ea/fv3m8WLF5t+/fqZ2tpax3qMVe8PP/ywef/9981XX31lqqurzc0332zS0tLM3r1749bzsWPHzO7du83u3buNJPPUU0+Z3bt3m4MHDxpjjFm0aJG57bbbouO//vpr079/f/PAAw+Y/fv3m2XLlpk+ffqYjRs3xq3n0+396aefNuvWrTNffvmlqa2tNffcc49JSUkxH374Ydx6vvvuu43X6zWbN282jY2N0frPf/4THZOMx/jp9J0Mx7cxvxwHW7ZsMfX19ebzzz83ixYtMi6Xy3zwwQft9p0M+/t0+o7V/rYi7L7//nszc+ZMc/bZZxuPx2Nmz55tjh07Fn2/vr7eSDIfffSRMcaYhoYGM2XKFJORkWHcbrfJy8szDzzwgAmFQo72+dxzz5nBgweb1NRUM2HCBLNjx47oe1OnTjUlJSVtxr/++uvm4osvNqmpqWbEiBHmnXfecbS/znSn9wULFkTHZmVlmeuvv9589tlnce3318vxf1+/9llSUmKmTp160pyxY8ea1NRUM3ToUPPSSy/Fteff9tGd3p944glz4YUXmrS0NJORkWEKCwvNpk2b4tpze/1KarMPk/EYP52+k+H4NsaYP/3pT2bIkCEmNTXVDBw40Fx11VXRwGivb2MSv7+N6X7fsdrfPOIHAGC9pP3ODgCAWCHsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADWI+wAANYj7AAA1iPsAADW+3/SZxehMLF1lAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached target in 3 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_324074/258646542.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAitklEQVR4nO3de2xUZf7H8c8U6FQiM7Ur7bRSUKwWuUMVGEygrlVUQuz+I6KRygJeUhJR40I3u8vKXkZXfqvGIGAMdlcleOOSIKC1CEQoKJXGcpFYbSiaTllFZqC7jmz7/P4wzlppSwtzZqZP36/k+8eceZ5zvj0c8/HMnDPHZYwxAgDAYimJbgAAAKcRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOs5FnYnTpzQ3XffLY/Ho/T0dM2dO1enT5/udE5hYaFcLlebeuCBB5xqEQDQS7ic+m3MW2+9VY2NjVq1apXOnDmjOXPm6LrrrtOaNWs6nFNYWKirr75aS5cujS7r37+/PB6PEy0CAHqJvk6s9PDhw9q6das++ugjXXvttZKk5557TrfddpuWLVumnJycDuf2799fPp/PibYAAL2UI2FXVVWl9PT0aNBJUlFRkVJSUrR371796le/6nDuq6++qldeeUU+n08zZszQ73//e/Xv37/D8ZFIRJFIJPq6tbVVJ06c0C9+8Qu5XK7Y/EEAgLgxxujUqVPKyclRSkpsvm1zJOyCwaAyMzPbbqhvX2VkZCgYDHY476677tKQIUOUk5OjTz75RIsWLdKRI0e0bt26DucEAgE9/vjjMesdAJAcjh07pkGDBsVmZaYbFi1aZCR1WocPHzZ/+ctfzNVXX33W/IEDB5rnn3++y9urrKw0kkxdXV2HY7777jsTCoWi1dDQcM4eKYqiqOSvkydPdieiOtWtM7tHH31U9957b6djhg4dKp/Pp+PHj7dZ/t///lcnTpzo1vdxEydOlCTV1dXpyiuvbHeM2+2W2+3u8joBAD1DLL+K6lbYDRw4UAMHDjznOL/fr5MnT6q6uloFBQWSpG3btqm1tTUaYF1RU1MjScrOzu5OmwAAtBWzc8SfueWWW8y4cePM3r17zQcffGCuuuoqM2vWrOj7X375pcnPzzd79+41xhhTV1dnli5davbt22fq6+vNxo0bzdChQ82UKVO6td1QKJTwU2+KoijqwisUCsUskxwLu2+++cbMmjXLXHzxxcbj8Zg5c+aYU6dORd+vr683ksz7779vjDGmoaHBTJkyxWRkZBi3223y8vLMY4891u0/lrCjKIqyo2IZdo7dVJ4o4XBYXq830W0AAC5QKBSK2Y+K8NuYAADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOs5HnbLly/X5ZdfrrS0NE2cOFEffvhhp+PfeOMNDRs2TGlpaRo1apQ2b97sdIsAANsZB61du9akpqaa1atXm4MHD5r58+eb9PR009TU1O74Xbt2mT59+pi//e1v5tChQ+Z3v/ud6devn6mtre3yNkOhkJFEURRF9fAKhUKxiiPjaNhNmDDBlJaWRl+3tLSYnJwcEwgE2h1/xx13mOnTp7dZNnHiRHP//fd3eZuEHUVRlB0Vy7Bz7GPM77//XtXV1SoqKoouS0lJUVFRkaqqqtqdU1VV1Wa8JE2bNq3D8ZIUiUQUDofbFAAAP+VY2H399ddqaWlRVlZWm+VZWVkKBoPtzgkGg90aL0mBQEBerzdaubm5F948AMAqPf5qzLKyMoVCoWgdO3Ys0S0BAJJMX6dWfOmll6pPnz5qampqs7ypqUk+n6/dOT6fr1vjJcntdsvtdl94wwAAazl2ZpeamqqCggJVVlZGl7W2tqqyslJ+v7/dOX6/v814SaqoqOhwPAAAXRKzS13asXbtWuN2u015ebk5dOiQue+++0x6eroJBoPGGGPuueces3jx4uj4Xbt2mb59+5ply5aZw4cPmyVLlnDrAUVRVC+tHnPrgTHGPPfcc2bw4MEmNTXVTJgwwezZsyf63tSpU01JSUmb8a+//rq5+uqrTWpqqhkxYoR5++23u7U9wo6iKMqOimXYuYwxRhYJh8Pyer2JbgMAcIFCoZA8Hk9M1tXjr8YEAOBcCDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUcD7vly5fr8ssvV1pamiZOnKgPP/yww7Hl5eVyuVxtKi0tzekWAQCWczTsXnvtNT3yyCNasmSJPv74Y40ZM0bTpk3T8ePHO5zj8XjU2NgYraNHjzrZIgCgF3A07P7+979r/vz5mjNnjoYPH66VK1eqf//+Wr16dYdzXC6XfD5ftLKyspxsEQDQC/R1asXff/+9qqurVVZWFl2WkpKioqIiVVVVdTjv9OnTGjJkiFpbWzV+/Hj99a9/1YgRIzocH4lEFIlEoq/D4XBs/gB0nTGJ7qDXYY/Hl8vlSnQLuECOndl9/fXXamlpOevMLCsrS8FgsN05+fn5Wr16tTZu3KhXXnlFra2tmjx5sr788ssOtxMIBOT1eqOVm5sb078DANDzJdXVmH6/X7Nnz9bYsWM1depUrVu3TgMHDtSqVas6nFNWVqZQKBStY8eOxbFjAEBP4NjHmJdeeqn69OmjpqamNsubmprk8/m6tI5+/fpp3Lhxqqur63CM2+2W2+2+oF4BAHZz7MwuNTVVBQUFqqysjC5rbW1VZWWl/H5/l9bR0tKi2tpaZWdnO9UmAKAXcOzMTpIeeeQRlZSU6Nprr9WECRP0zDPPqLm5WXPmzJEkzZ49W5dddpkCgYAkaenSpZo0aZLy8vJ08uRJPfXUUzp69KjmzZvnZJsAAMs5GnYzZ87Uv/71L/3hD39QMBjU2LFjtXXr1uhFKw0NDUpJ+d/J5bfffqv58+crGAzqkksuUUFBgXbv3q3hw4c72SYAwHIuY+y6bjwcDsvr9Sa6jd7FrkOoR2CPxxe3HiRGKBSSx+OJybqS6mpMAACcQNgBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArOdo2O3cuVMzZsxQTk6OXC6XNmzYcM4527dv1/jx4+V2u5WXl6fy8nInWwQA9AKOhl1zc7PGjBmj5cuXd2l8fX29pk+frhtuuEE1NTVauHCh5s2bp3feecfJNgEAlnMZY0xcNuRyaf369SouLu5wzKJFi/T222/rwIED0WV33nmnTp48qa1bt7Y7JxKJKBKJRF+Hw2Hl5ubGrG90QXwOIfwEezy+XC5XolvolUKhkDweT0zWlVTf2VVVVamoqKjNsmnTpqmqqqrDOYFAQF6vN1oEHQDg55Iq7ILBoLKystosy8rKUjgc1n/+859255SVlSkUCkXr2LFj8WgVANCD9E10AxfK7XbL7XYnug0AQBJLqjM7n8+npqamNsuamprk8Xh00UUXJagrAEBPl1Rh5/f7VVlZ2WZZRUWF/H5/gjoCANjA0bA7ffq0ampqVFNTI+mHWwtqamrU0NAg6Yfv22bPnh0d/8ADD+iLL77Qb37zG3366ad6/vnn9frrr+vhhx92sk0AgO2Mg95//32jH66SblMlJSXGGGNKSkrM1KlTz5ozduxYk5qaaoYOHWpeeumlbm0zFAq1u03KwTKGinMhvhL+31gvrVAoFLN/w7jdZxcv4XBYXq830W30LnYdQj0Cezy+uM8uMay9zw4AACcQdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6xF2AADrEXYAAOsRdgAA6zkadjt37tSMGTOUk5Mjl8ulDRs2dDp++/btcrlcZ1UwGHSyTQCA5RwNu+bmZo0ZM0bLly/v1rwjR46osbExWpmZmQ51CADoDfo6ufJbb71Vt956a7fnZWZmKj09vUtjI5GIIpFI9HU4HO729gAAdnM07M7X2LFjFYlENHLkSP3xj3/U9ddf3+HYQCCgxx9/PI7dAYnnetyV6BaAHiWpLlDJzs7WypUr9dZbb+mtt95Sbm6uCgsL9fHHH3c4p6ysTKFQKFrHjh2LY8cAgJ4gqc7s8vPzlZ+fH309efJkff7553r66af18ssvtzvH7XbL7XbHq0UAQA+UVGd27ZkwYYLq6uoS3QYAoAdL+rCrqalRdnZ2otsAAPRgjn6Mefr06TZnZfX19aqpqVFGRoYGDx6ssrIyffXVV/rnP/8pSXrmmWd0xRVXaMSIEfruu+/04osvatu2bXr33XedbBMAYDlHw27fvn264YYboq8feeQRSVJJSYnKy8vV2NiohoaG6Pvff/+9Hn30UX311Vfq37+/Ro8erffee6/NOgAA6C6XMcYkuolYCofD8nq9iW6jd7HrEOoZuPUgvv6Y6AZ6p1AoJI/HE5N1Jf13dgAAXCjCDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPcIOAGA9wg4AYD3CDgBgPUfDLhAI6LrrrtOAAQOUmZmp4uJiHTly5Jzz3njjDQ0bNkxpaWkaNWqUNm/e7GSbAADLORp2O3bsUGlpqfbs2aOKigqdOXNGN998s5qbmzucs3v3bs2aNUtz587V/v37VVxcrOLiYh04cMDJVgEAFnMZY0y8Nvavf/1LmZmZ2rFjh6ZMmdLumJkzZ6q5uVmbNm2KLps0aZLGjh2rlStXnnMb4XBYXq83Zj2jC+J3COFHj7sS3UHv8sdEN9A7hUIheTyemKwrrt/ZhUIhSVJGRkaHY6qqqlRUVNRm2bRp01RVVdXu+EgkonA43KYAAPipuIVda2urFi5cqOuvv14jR47scFwwGFRWVlabZVlZWQoGg+2ODwQC8nq90crNzY1p3wCAni9uYVdaWqoDBw5o7dq1MV1vWVmZQqFQtI4dOxbT9QMAer6+8djIggULtGnTJu3cuVODBg3qdKzP51NTU1ObZU1NTfL5fO2Od7vdcrvdMesVAGAfR8/sjDFasGCB1q9fr23btumKK6445xy/36/Kyso2yyoqKuT3+51qEwBgOUfP7EpLS7VmzRpt3LhRAwYMiH7v5vV6ddFFF0mSZs+ercsuu0yBQECS9NBDD2nq1Kn6v//7P02fPl1r167Vvn379MILLzjZKgDAYo6e2a1YsUKhUEiFhYXKzs6O1muvvRYd09DQoMbGxujryZMna82aNXrhhRc0ZswYvfnmm9qwYUOnF7UAANCZuN5nFw/cZ5cAdh1CPQP32cXXHxPdQO/UY++zAwAgEQg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1CDsAgPUIOwCA9Qg7AID1HA27QCCg6667TgMGDFBmZqaKi4t15MiRTueUl5fL5XK1qbS0NCfbBABYztGw27Fjh0pLS7Vnzx5VVFTozJkzuvnmm9Xc3NzpPI/Ho8bGxmgdPXrUyTYBAJbr6+TKt27d2uZ1eXm5MjMzVV1drSlTpnQ4z+VyyefzOdkaAKAXcTTsfi4UCkmSMjIyOh13+vRpDRkyRK2trRo/frz++te/asSIEe2OjUQiikQi0dfhcDh2DaNrXK5EdwAAnYrbBSqtra1auHChrr/+eo0cObLDcfn5+Vq9erU2btyoV155Ra2trZo8ebK+/PLLdscHAgF5vd5o5ebmOvUnAAB6KJcxxsRjQw8++KC2bNmiDz74QIMGDeryvDNnzuiaa67RrFmz9Kc//ems99s7syPwAKDnC4VC8ng8MVlXXD7GXLBggTZt2qSdO3d2K+gkqV+/fho3bpzq6urafd/tdsvtdseiTQCApRz9GNMYowULFmj9+vXatm2brrjiim6vo6WlRbW1tcrOznagQwBAb+DomV1paanWrFmjjRs3asCAAQoGg5Ikr9eriy66SJI0e/ZsXXbZZQoEApKkpUuXatKkScrLy9PJkyf11FNP6ejRo5o3b56TrQIALOZo2K1YsUKSVFhY2Gb5Sy+9pHvvvVeS1NDQoJSU/51gfvvtt5o/f76CwaAuueQSFRQUaPfu3Ro+fLiTrQIALBa3C1TiJRwOy+v1JroNAMAFiuUFKvw2JgDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6hB0AwHqEHQDAeoQdAMB6jobdihUrNHr0aHk8Hnk8Hvn9fm3ZsqXTOW+88YaGDRumtLQ0jRo1Sps3b3ayRQBAL+Bo2A0aNEhPPPGEqqurtW/fPv3yl7/U7bffroMHD7Y7fvfu3Zo1a5bmzp2r/fv3q7i4WMXFxTpw4ICTbQIALOcyxph4bjAjI0NPPfWU5s6de9Z7M2fOVHNzszZt2hRdNmnSJI0dO1YrV67s0vrD4bC8Xm/M+gUAJEYoFJLH44nJuuL2nV1LS4vWrl2r5uZm+f3+dsdUVVWpqKiozbJp06apqqqqw/VGIhGFw+E2BQDATzkedrW1tbr44ovldrv1wAMPaP369Ro+fHi7Y4PBoLKystosy8rKUjAY7HD9gUBAXq83Wrm5uTHtHwDQ8zkedvn5+aqpqdHevXv14IMPqqSkRIcOHYrZ+svKyhQKhaJ17NixmK0bAGCHvk5vIDU1VXl5eZKkgoICffTRR3r22We1atWqs8b6fD41NTW1WdbU1CSfz9fh+t1ut9xud2ybBgBYJe732bW2tioSibT7nt/vV2VlZZtlFRUVHX7HBwBAlxgHLV682OzYscPU19ebTz75xCxevNi4XC7z7rvvGmOMueeee8zixYuj43ft2mX69u1rli1bZg4fPmyWLFli+vXrZ2pra7u8zVAoZCRRFEVRPbxCoVDM8sjRjzGPHz+u2bNnq7GxUV6vV6NHj9Y777yjm266SZLU0NCglJT/nVxOnjxZa9as0e9+9zv99re/1VVXXaUNGzZo5MiRTrYJALBc3O+zcxr32QGAHXrkfXYAACQKYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwHmEHALAeYQcAsB5hBwCwnqNht2LFCo0ePVoej0cej0d+v19btmzpcHx5eblcLlebSktLc7JFAEAv0NfJlQ8aNEhPPPGErrrqKhlj9I9//EO333679u/frxEjRrQ7x+Px6MiRI9HXLpfLyRYBAL2Ao2E3Y8aMNq//8pe/aMWKFdqzZ0+HYedyueTz+bq8jUgkokgkEn0dCoXOr1kAQFIxxsRsXXH7zq6lpUVr165Vc3Oz/H5/h+NOnz6tIUOGKDc3V7fffrsOHjzY6XoDgYC8Xm+0Bg8eHOvWAQAJ8M0338RsXS4Ty+hsR21trfx+v7777jtdfPHFWrNmjW677bZ2x1ZVVemzzz7T6NGjFQqFtGzZMu3cuVMHDx7UoEGD2p3z8zO7kydPasiQIWpoaJDX63Xkb3JCOBxWbm6ujh07Jo/Hk+h2uqWn9k7f8UXf8ddTew+FQho8eLC+/fZbpaenx2Sdjn6MKUn5+fmqqalRKBTSm2++qZKSEu3YsUPDhw8/a6zf729z1jd58mRdc801WrVqlf70pz+1u3632y23233Wcq/X26P+cX/048U8PVFP7Z2+44u+46+n9p6SErsPHx0Pu9TUVOXl5UmSCgoK9NFHH+nZZ5/VqlWrzjm3X79+GjdunOrq6pxuEwBgsbjfZ9fa2trmY8fOtLS0qLa2VtnZ2Q53BQCwmaNndmVlZbr11ls1ePBgnTp1SmvWrNH27dv1zjvvSJJmz56tyy67TIFAQJK0dOlSTZo0SXl5eTp58qSeeuopHT16VPPmzevyNt1ut5YsWdLuR5vJrKf2LfXc3uk7vug7/npq70707egFKnPnzlVlZaUaGxvl9Xo1evRoLVq0SDfddJMkqbCwUJdffrnKy8slSQ8//LDWrVunYDCoSy65RAUFBfrzn/+scePGOdUiAKAXcPxqTAAAEo3fxgQAWI+wAwBYj7ADAFiPsAMAWM+KsDtx4oTuvvtueTwepaena+7cuTp9+nSncwoLC896nNADDzzgaJ/Lly/X5ZdfrrS0NE2cOFEffvhhp+PfeOMNDRs2TGlpaRo1apQ2b97saH+d6U7vyfCopp07d2rGjBnKycmRy+XShg0bzjln+/btGj9+vNxut/Ly8qJXCcdbd3vfvn37Wfvb5XIpGAzGp2H98Bu11113nQYMGKDMzEwVFxe3eXpJRxJ9jJ9P38lwfEvdf4SalPj9LSXu0W9WhN3dd9+tgwcPqqKiQps2bdLOnTt13333nXPe/Pnz1djYGK2//e1vjvX42muv6ZFHHtGSJUv08ccfa8yYMZo2bZqOHz/e7vjdu3dr1qxZmjt3rvbv36/i4mIVFxfrwIEDjvXYke72Lv3w80Q/3bdHjx6NY8dSc3OzxowZo+XLl3dpfH19vaZPn64bbrhBNTU1WrhwoebNmxe9JzSeutv7j44cOdJmn2dmZjrU4dl27Nih0tJS7dmzRxUVFTpz5oxuvvlmNTc3dzgnGY7x8+lbSvzxLf3vEWrV1dXat2+ffvnLX3b64/nJsL/Pp28pRvvb9HCHDh0yksxHH30UXbZlyxbjcrnMV1991eG8qVOnmoceeigOHf5gwoQJprS0NPq6paXF5OTkmEAg0O74O+64w0yfPr3NsokTJ5r777/f0T7b093eX3rpJeP1euPU3blJMuvXr+90zG9+8xszYsSINstmzpxppk2b5mBn59aV3t9//30jyXz77bdx6akrjh8/biSZHTt2dDgmmY7xH3Wl72Q7vn/qkksuMS+++GK77yXj/v5RZ33Han/3+DO7qqoqpaen69prr40uKyoqUkpKivbu3dvp3FdffVWXXnqpRo4cqbKyMv373/92pMfvv/9e1dXVKioqii5LSUlRUVGRqqqq2p1TVVXVZrwkTZs2rcPxTjmf3qXuP6op0ZJlf1+IsWPHKjs7WzfddJN27dqV0F5+fK5kRkZGh2OScZ93pW8p+Y7vrjxCLRn3t1OPfmuP4z8E7bRgMHjWxzV9+/ZVRkZGp99Z3HXXXRoyZIhycnL0ySefaNGiRTpy5IjWrVsX8x6//vprtbS0KCsrq83yrKwsffrpp+3OCQaD7Y6P5/cw0vn1np+fr9WrV7d5VNPkyZM7fVRTonW0v8PhsP7zn//ooosuSlBn55adna2VK1fq2muvVSQS0YsvvqjCwkLt3btX48ePj3s/ra2tWrhwoa6//nqNHDmyw3HJcoz/qKt9J9Px/fNHqK1fv77dJ8pIybW/u9N3rPZ30obd4sWL9eSTT3Y65vDhw+e9/p9+pzdq1ChlZ2frxhtv1Oeff64rr7zyvNeL83tUE85ffn6+8vPzo68nT56szz//XE8//bRefvnluPdTWlqqAwcO6IMPPoj7ti9EV/tOpuO7O49QSyZOP/qtPUkbdo8++qjuvffeTscMHTpUPp/vrAsl/vvf/+rEiRPy+Xxd3t7EiRMlSXV1dTEPu0svvVR9+vRRU1NTm+VNTU0d9ujz+bo13inn0/vP9YRHNXW0vz0eT1Kf1XVkwoQJCQmbBQsWRC8SO9f/dSfLMS51r++fS+Tx3Z1HqCXT/k7Eo9+S9ju7gQMHatiwYZ1Wamqq/H6/Tp48qerq6ujcbdu2qbW1NRpgXVFTUyNJjjxOKDU1VQUFBaqsrIwua21tVWVlZYefU/v9/jbjJamioqLTz7WdcD69/1xPeFRTsuzvWKmpqYnr/jbGaMGCBVq/fr22bdumK6644pxzkmGfn0/fP5dMx3dnj1BLhv3dkbg8+u2CL3FJArfccosZN26c2bt3r/nggw/MVVddZWbNmhV9/8svvzT5+flm7969xhhj6urqzNKlS82+fftMfX292bhxoxk6dKiZMmWKYz2uXbvWuN1uU15ebg4dOmTuu+8+k56eboLBoDHGmHvuuccsXrw4On7Xrl2mb9++ZtmyZebw4cNmyZIlpl+/fqa2ttaxHmPV++OPP27eeecd8/nnn5vq6mpz5513mrS0NHPw4MG49Xzq1Cmzf/9+s3//fiPJ/P3vfzf79+83R48eNcYYs3jxYnPPPfdEx3/xxRemf//+5rHHHjOHDx82y5cvN3369DFbt26NW8/n2/vTTz9tNmzYYD777DNTW1trHnroIZOSkmLee++9uPX84IMPGq/Xa7Zv324aGxuj9e9//zs6JhmP8fPpOxmOb2N+OA527Nhh6uvrzSeffGIWL15sXC6Xeffdd9vtOxn29/n0Hav9bUXYffPNN2bWrFnm4osvNh6Px8yZM8ecOnUq+n59fb2RZN5//31jjDENDQ1mypQpJiMjw7jdbpOXl2cee+wxEwqFHO3zueeeM4MHDzapqalmwoQJZs+ePdH3pk6dakpKStqMf/31183VV19tUlNTzYgRI8zbb7/taH+d6U7vCxcujI7Nysoyt912m/n444/j2u+Pl+P/vH7ss6SkxEydOvWsOWPHjjWpqalm6NCh5qWXXoprzz/tozu9P/nkk+bKK680aWlpJiMjwxQWFppt27bFtef2+pXUZh8m4zF+Pn0nw/FtjDG//vWvzZAhQ0xqaqoZOHCgufHGG6OB0V7fxiR+fxvT/b5jtb95xA8AwHpJ+50dAACxQtgBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKxH2AEArEfYAQCsR9gBAKz3/6p6vXCHR3wfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: (1, 1), policy: [0.477 0.    0.    0.523], policy argmax:Right policy value: 0.9999779462814331\n",
      "search: [0.469 0.    0.    0.531], search argmax: Right\n",
      "Position: (1, 2), policy: [0.   0.41 0.   0.59], policy argmax:Right policy value: 0.9999561905860901\n",
      "search: [0.    0.041 0.    0.959], search argmax: Right\n",
      "Position: (2, 1), policy: [0.463 0.    0.537 0.   ], policy argmax:Left policy value: 0.9999696016311646\n",
      "search: [0.939 0.    0.061 0.   ], search argmax: Down\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters, device)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{cfg.learn.num_learn_iters - 1}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/{cfg.name}_model_{100}.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"checkpoints/maze_4x4_binaryreward_maxsteps2_wstepsleft_round3_model_13.pt\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "mcts = AlphaMCTS(maze, search_cfg=cfg.search, model=model)\n",
    "\n",
    "_ = mcts.play_game()\n",
    "\n",
    "positions = [(x, y) for x in range(1, cfg.maze.width-1) for y in range(1, cfg.maze.height-1)]\n",
    "for pos in positions:\n",
    "    if pos == maze.target:\n",
    "        continue\n",
    "    state = Maze.State(*pos, 1, 0)\n",
    "    policy, value = mcts.query_model(state)\n",
    "    print(f\"Position: {pos}, policy: {policy}, policy argmax:{maze.action_to_string(np.argmax(policy))} policy value: {value}\")\n",
    "    search_probs = mcts.search(state)\n",
    "    print(f\"search: {search_probs}, search argmax: {maze.action_to_string(np.argmax(search_probs))}\")\n",
    "# Actions: Down, Up, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
