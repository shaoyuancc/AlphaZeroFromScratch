{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import namedtuple\n",
    "print(np.__version__)\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration using OmegaConf\n",
    "cfg = OmegaConf.create({\n",
    "    \"name\": \"maze_4x4_binaryreward_maxsteps2_softmaxbugfix\",\n",
    "    \"maze\": {\n",
    "        \"width\": 4,\n",
    "        \"height\": 4,\n",
    "        \"cell_occupancy_prob\": 0,\n",
    "        \"max_steps\": 2\n",
    "    },\n",
    "    \"search\": {\n",
    "        # MCTS configuration\n",
    "        \"num_simulations\": 50,\n",
    "        \"c_puct\": 2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_resBlocks\": 4,\n",
    "        \"num_filters\": 64,\n",
    "    },\n",
    "    \"learn\": {\n",
    "        \"num_learn_iters\": 8,\n",
    "        \"num_self_play_iters\": 500,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"train_batch_size\": 64,\n",
    "        \"lr\": 0.001\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"2D Gridworld Maze Game\n",
    "    \"\"\"\n",
    "\n",
    "    State = namedtuple('State', ['x', 'y', 'steps_left', 'reward'])\n",
    "\n",
    "    TARGET_REWARD = 1\n",
    "    # MOVE_REWARD = -1\n",
    "    TIMEOUT_REWARD = -1\n",
    "\n",
    "    def __init__(self, width: int, height: int, seed: Optional[int] = None, cell_occupancy_prob: float = 0.3):\n",
    "        assert 0 <= cell_occupancy_prob < 1, \"Cell occupancy probability must be in the range [0, 1)\"\n",
    "        assert width > 2 and height > 2, \"Width and height must be greater than 2\"\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.seed = seed\n",
    "        self.cell_occupancy_prob = cell_occupancy_prob\n",
    "        self.generate_map()\n",
    "\n",
    "        # self.action_size = 5  # Up, Down, Left, Right, Stay\n",
    "        self.action_size = 4\n",
    "\n",
    "        # self.max_steps=width*height\n",
    "        # For this simplest possible maze, set the max length to be 2\n",
    "        self.max_steps = cfg.maze.max_steps\n",
    "\n",
    "\n",
    "        self.observation_width = 5 # 5x5 observation window centered at the agent\n",
    "        # In this maze, all the free space in the maze is observable from any position\n",
    "\n",
    "    def get_initial_state(self) -> State:\n",
    "        return Maze.State(self.source[0], self.source[1], self.max_steps, 0)\n",
    "    \n",
    "    def get_next_state(self, state: State, action):\n",
    "        dx, dy = self.action_to_delta(action)\n",
    "        # Additional reward is -1 for each x or y coordinate moved.\n",
    "        # dr = (abs(dx) + abs(dy)) * Maze.MOVE_REWARD\n",
    "        dr = 0\n",
    "        if (state.x + dx, state.y + dy) == self.target:\n",
    "            dr += Maze.TARGET_REWARD\n",
    "        elif state.steps_left == 1:\n",
    "            dr += Maze.TIMEOUT_REWARD\n",
    "        return Maze.State(state.x + dx, state.y + dy, state.steps_left - 1, state.reward + dr)\n",
    "    \n",
    "    def get_encoded_observation(self, state: State):\n",
    "        # Get the observation window centered at the agent\n",
    "        # Assumes width is odd\n",
    "        half_width = self.observation_width // 2\n",
    "\n",
    "        # Pad the maze with obstacles (1s) to handle boundaries\n",
    "        padded_maze = np.pad(self.map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "        # Adjust the agent's position due to padding\n",
    "        x_padded = state.x + half_width\n",
    "        y_padded = state.y + half_width\n",
    "\n",
    "        # Plane 0: Obstacles\n",
    "        # Extract the observation window where obstacle is 1 and free space is 0\n",
    "        plane_obstacles = padded_maze[\n",
    "            x_padded - half_width : x_padded + half_width + 1,\n",
    "            y_padded - half_width : y_padded + half_width + 1\n",
    "        ]\n",
    "\n",
    "        # Make sure that any number that is not 1 is 0\n",
    "        plane_obstacles[plane_obstacles != 1] = 0\n",
    "\n",
    "        return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "        # # Plane 1: Free Space (1s where free space, 0s where obstacles)\n",
    "        # plane_free_space = plane_obstacles == 0\n",
    "\n",
    "        # # Plane 2: Agent's position\n",
    "        # plane_agent = np.zeros_like(plane_obstacles)\n",
    "        # plane_agent[half_width, half_width] = 1\n",
    "\n",
    "        # encoded_observation = np.stack([plane_obstacles, plane_free_space, plane_agent], axis=0)\n",
    "\n",
    "        # return encoded_observation\n",
    "    \n",
    "    def get_normalized_agent_position(self, state: State):\n",
    "        # Normalize the positions\n",
    "        return (state.x / self.width, state.y / self.height)\n",
    "    \n",
    "    def get_normalized_target_position(self):\n",
    "        return (self.target[0] / self.width, self.target[1] / self.height)\n",
    "\n",
    "    def get_valid_actions(self, state: State):\n",
    "        valid_moves = []\n",
    "        for action in range(self.action_size):\n",
    "            dx, dy = self.action_to_delta(action)\n",
    "            nx, ny = state.x + dx, state.y + dy\n",
    "            if self.map[nx, ny] != 1:\n",
    "                valid_moves.append(action)\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state: State):\n",
    "        # In this case we are using binary reward\n",
    "        if (state.x, state.y) == self.target or state.steps_left == 0:\n",
    "            return state.reward, True\n",
    "    \n",
    "        return state.reward, False\n",
    "    \n",
    "    def action_to_delta(self, action):\n",
    "        # action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0), (0, 0)]  # Down, Up, Left, Right, Stay\n",
    "        action_to_delta = [(0, 1), (0, -1), (-1, 0), (1, 0)] \n",
    "        return action_to_delta[action]\n",
    "    \n",
    "    def action_to_string(self, action):\n",
    "        action_to_string = ['Down', 'Up', 'Left', 'Right', 'Stay']\n",
    "        return action_to_string[action]\n",
    "    \n",
    "    def generate_map(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            map = np.random.choice([0, 1], size=(self.width, self.height), p=[1-self.cell_occupancy_prob, self.cell_occupancy_prob])\n",
    "            # Make the boundaries of the maze walls\n",
    "            map[0, :] = 1\n",
    "            map[-1, :] = 1\n",
    "            map[:, 0] = 1\n",
    "            map[:, -1] = 1\n",
    "\n",
    "            # Randomly select two unique non-border positions for the source and target\n",
    "            while True:\n",
    "                # Generate two random positions within the non-border range\n",
    "                source = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                target = (np.random.randint(1, self.width - 1), np.random.randint(1, self.height - 1))\n",
    "                \n",
    "                # Ensure the positions are unique\n",
    "                if source != target:\n",
    "                    break\n",
    "            \n",
    "            # Make sure the source and target do not have obstacles\n",
    "            map[source] = 2\n",
    "            map[target] = 3\n",
    "\n",
    "            self.source = source\n",
    "            self.target = target\n",
    "\n",
    "            # Set the max steps to be 3 * the L1 distance between source and target\n",
    "            # self.max_steps = 3 * (abs(source[0] - target[0]) + abs(source[1] - target[1]))\n",
    "\n",
    "            self.map = map\n",
    "            astar = AStar(self)\n",
    "            success, self.shortest_path = astar.solve()\n",
    "            if success:\n",
    "                break\n",
    "            if count % 20 == 0:\n",
    "                print(f\"Unsolvable maze {count}. Regenerating...\")\n",
    "\n",
    "    def visualize_path(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.shortest_path\n",
    "        map = self.map.copy()\n",
    "        truncated_path = path[1:-1]  # Exclude source and target\n",
    "        for pos in truncated_path:\n",
    "            map[pos] = 4\n",
    "        self.visualize_state(map)\n",
    "\n",
    "    def visualize_state(self, map: Optional[np.ndarray] = None):\n",
    "        if map is None:\n",
    "            map = self.map\n",
    "        # Define colors for each type of cell\n",
    "        cmap = mcolors.ListedColormap(['white', 'black', 'red', 'green', 'cyan'])\n",
    "        \n",
    "        # Plot the maze using imshow\n",
    "        plt.imshow(map.T, cmap=cmap, vmin=0, vmax=4)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        plt.show()\n",
    "\n",
    "class AStar:\n",
    "    def __init__(self, maze: Maze):\n",
    "        self.maze = maze\n",
    "        self.start = maze.source\n",
    "        self.goal = maze.target\n",
    "        self.height, self.width = maze.height, maze.width\n",
    "\n",
    "    def heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> int:\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def successors(self, pos: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        x, y = pos\n",
    "        successors = []\n",
    "        directions = [(0, 1),(0, -1), (-1, 0), (1, 0)]  # Down, Up, Left, Right\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if self.maze.map[nx, ny] != 1:\n",
    "                successors.append((nx, ny))\n",
    "        return successors\n",
    "\n",
    "    def solve(self) -> bool:\n",
    "        open = []\n",
    "        heapq.heappush(open, (0, self.start))\n",
    "        came_from = {}\n",
    "        g_score = {self.start: 0}\n",
    "\n",
    "        while open:\n",
    "            _, current = heapq.heappop(open)\n",
    "            \n",
    "            if current == self.goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return True, path  # Maze is solvable\n",
    "\n",
    "            for successor in self.successors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if successor not in g_score or tentative_g_score < g_score[successor]:\n",
    "                    came_from[successor] = current\n",
    "                    g_score[successor] = tentative_g_score\n",
    "                    f_score = tentative_g_score + self.heuristic(successor, self.goal)\n",
    "                    heapq.heappush(open, (f_score, successor))\n",
    "\n",
    "        return False, []  # Maze is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_resBlocks, num_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        OBSERVATION_WIDTH = 5\n",
    "        ACTION_SIZE = 4\n",
    "\n",
    "\n",
    "        # Initial convolutional block\n",
    "        # The single input channel is for the observation where obstacles are 1 and free space is 0\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_filters) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        # Policy head convolutional part that gets flattened\n",
    "        self.policyHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        policy_conv_output_size = 32 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Policy head fully connected part\n",
    "        self.policyHead_flat = nn.Sequential(\n",
    "            nn.Linear(policy_conv_output_size + 4, 256),  # Adding 4 for the positions of agent and target\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, ACTION_SIZE),\n",
    "        )\n",
    "        # NOTE: Do not apply softmax here or in the forward method because it is applied in the loss function\n",
    "\n",
    "        # Value head convolutional part\n",
    "        self.valueHead_conv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute the size after flattening\n",
    "        value_conv_output_size = 3 * OBSERVATION_WIDTH ** 2\n",
    "\n",
    "        # Value head fully connected part\n",
    "        self.valueHead_flat = nn.Sequential(\n",
    "            nn.Linear(value_conv_output_size + 4, 256),  # Adding 4 for the positions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Value is between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, agent_pos, goal_pos):\n",
    "        # x: Input tensor of shape (batch_size, 3, maze_height, maze_width)\n",
    "        # agent_pos and goal_pos: tensors of shape (batch_size, 2), normalized\n",
    "\n",
    "        # Initial convolutional block\n",
    "        x = self.startBlock(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "\n",
    "        # Policy head\n",
    "        policy_x = self.policyHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        policy_x_concat = torch.cat([policy_x, agent_pos, goal_pos], dim=1)\n",
    "        policy = self.policyHead_flat(policy_x_concat)\n",
    "\n",
    "        # Value head\n",
    "        value_x = self.valueHead_conv(x)  # Output is already flattened\n",
    "        # Concatenate positions\n",
    "        value_x_concat = torch.cat([value_x, agent_pos, goal_pos], dim=1)\n",
    "        value = self.valueHead_flat(value_x_concat)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, valid_actions, parent=None, last_action=None, prior_prob=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.last_action = last_action\n",
    "        self.valid_actions = valid_actions\n",
    "        self.prior_prob = prior_prob\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.is_leaf = True\n",
    "        self.children = []\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "    \n",
    "class AlphaMCTS:\n",
    "    def __init__(self, game: Maze, num_simulations, c_puct, model):\n",
    "        self.game = game\n",
    "        self.num_simulations = num_simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.model = model\n",
    "    \n",
    "    def play_game(self, max_iters = 1000, verbose=True, visualize=True):\n",
    "        state = self.game.get_initial_state()\n",
    "        path = []\n",
    "        memory = []\n",
    "        for i in range(max_iters):\n",
    "            \n",
    "            action_probs = self.search(state)\n",
    "            path.append((state.x, state.y))\n",
    "            memory.append((self.game.get_encoded_observation(state), \n",
    "                           self.game.get_normalized_agent_position(state), \n",
    "                           self.game.get_normalized_target_position(),\n",
    "                           action_probs))\n",
    "\n",
    "            # Sample action from the action probabilities\n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            # Take the action with the highest probability\n",
    "            # action = np.argmax(action_probs)\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {i+1}: {state}, action_probs: {action_probs} action chosen: {self.game.action_to_string(action)}\")\n",
    "            state = self.game.get_next_state(state, action)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state)\n",
    "\n",
    "            if is_terminal:\n",
    "                path.append((state.x, state.y))\n",
    "\n",
    "                ret_mem = [(*mem, value) for mem in memory]\n",
    "\n",
    "                if verbose:\n",
    "                    if (state.x, state.y) == self.game.target:\n",
    "                        print(f\"Reached target in {i+1} steps: {path}\")\n",
    "                    else:\n",
    "                        print(f\"Terminated due to timeout in {i+1} steps: {path}\")\n",
    "                if visualize:\n",
    "                    self.game.visualize_path(path)\n",
    "                \n",
    "                return ret_mem\n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(state, self.game.get_valid_actions(state))\n",
    "\n",
    "        # Conduct num_simulations simulations\n",
    "        for i in range(self.num_simulations):\n",
    "            node = root\n",
    "            # Selection all the way down till a leaf node\n",
    "            while not node.is_leaf:\n",
    "                node = self.select(node)\n",
    "\n",
    "            # Evaluate the leaf node\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state)\n",
    "\n",
    "            # If the leaf node is not a terminal node then expand it and evaluate it\n",
    "            if not is_terminal:\n",
    "                # Query the model for the policy and value\n",
    "                policy, value = self.query_model(node.state)\n",
    "                \n",
    "                # Mask invalid actions\n",
    "                valid_policy = np.zeros_like(policy)\n",
    "                valid_policy[node.valid_actions] = policy[node.valid_actions]\n",
    "                valid_policy /= np.sum(valid_policy)\n",
    "\n",
    "                self.expand(node, policy=valid_policy)\n",
    "                \n",
    "            self.backpropagate(node, value)\n",
    "\n",
    "        \n",
    "        # Return the action probabilities after search\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.last_action] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "    \n",
    "    def query_model(self, state: Maze.State):\n",
    "        tensor_obs = torch.tensor(self.game.get_encoded_observation(state), dtype=torch.float32).unsqueeze(0)\n",
    "        tensor_agent_pos = torch.tensor(self.game.get_normalized_agent_position(state), dtype=torch.float32).unsqueeze(0)\n",
    "        tensor_goal_pos = torch.tensor(self.game.get_normalized_target_position(), dtype=torch.float32).unsqueeze(0)\n",
    "        # Query the model for the policy and value\n",
    "        policy, value = self.model(\n",
    "            tensor_obs, tensor_agent_pos, tensor_goal_pos\n",
    "            )\n",
    "        \n",
    "        value = value.item()\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "    def select(self, node: Node) -> Node:\n",
    "        ucbs = [self.calc_ucb(node, child) for child in node.children]\n",
    "        return node.children[np.argmax(ucbs)]\n",
    "\n",
    "    def calc_ucb(self, node: Node, child: Node) -> float:\n",
    "        # Assumes normalized values for value_sum\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = child.value_sum / child.visit_count\n",
    "        u_value = self.c_puct * child.prior_prob * np.sqrt(node.visit_count) / (1 + child.visit_count)\n",
    "        return q_value + u_value\n",
    "    \n",
    "    def expand(self, node: Node, policy) -> None:\n",
    "        if (node.state.x, node.state.y) == self.game.target:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        \n",
    "        for action, prior_prob in enumerate(policy):\n",
    "            if prior_prob > 0:\n",
    "                child_state = self.game.get_next_state(node.state, action)\n",
    "                child_node = Node(child_state,\n",
    "                                  self.game.get_valid_actions(child_state),\n",
    "                                  parent=node,\n",
    "                                  last_action=action,\n",
    "                                  prior_prob=prior_prob)\n",
    "                node.children.append(child_node)\n",
    "        node.is_leaf = False\n",
    "\n",
    "    def backpropagate(self, node: Node, value: float) -> None:\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "            node = node.parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, search_alg: AlphaMCTS, num_learn_iters, num_self_play_iters, num_train_epochs, train_batch_size, seed=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.search_alg = search_alg\n",
    "        self.num_learn_iters = num_learn_iters\n",
    "        self.num_self_play_iters = num_self_play_iters\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.train_batch_size = train_batch_size\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def self_play(self):\n",
    "        # Initialize game\n",
    "        # For now train on a fixed size maze\n",
    "        game = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "        self.search_alg.game = game\n",
    "        return self.search_alg.play_game(verbose=False, visualize=False)\n",
    "        \n",
    "    def train(self, memory, iteration, epoch):\n",
    "        random.shuffle(memory)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batchIdx in range(0, len(memory), self.train_batch_size):\n",
    "            batch = memory[batchIdx:batchIdx + self.train_batch_size]\n",
    "            obs, agent_pos, goal_pos, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            obs, agent_pos, goal_pos, policy_targets, value_targets = np.array(obs), np.array(agent_pos), np.array(goal_pos), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            agent_pos = torch.tensor(agent_pos, dtype=torch.float32)\n",
    "            goal_pos = torch.tensor(goal_pos, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "            \n",
    "            policy_pred, value_pred = self.model(obs, agent_pos, goal_pos)\n",
    "            value_loss = F.mse_loss(value_pred, value_targets)\n",
    "            policy_loss = F.cross_entropy(policy_pred, policy_targets)\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Log metrics for the current batch\n",
    "            wandb.log({\"batch_loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / (len(memory) // self.train_batch_size)\n",
    "        # Log average loss for the epoch\n",
    "        wandb.log({\"train_epoch_loss\": avg_loss, \"iteration\": iteration, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        wandb.init(project=\"alpha-zero-discrete-maze\",\n",
    "            name=cfg.name,\n",
    "            config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n",
    "            save_code=True)\n",
    "        \n",
    "        wandb.watch(self.model, log=\"all\", log_freq=10)  # Log model gradients and parameters\n",
    "        \n",
    "        for iteration in range(self.num_learn_iters):\n",
    "            memory = []\n",
    "            successes = 0\n",
    "        \n",
    "            self.model.eval()\n",
    "            for _ in trange(self.num_self_play_iters):\n",
    "                game_mem = self.self_play()\n",
    "                if game_mem[-1][-1] > 0:  # Assuming positive value means successful game\n",
    "                    successes += 1\n",
    "                memory += game_mem\n",
    "\n",
    "            success_rate = successes / self.num_self_play_iters\n",
    "            # Log the success rate for self-play games\n",
    "            wandb.log({\"success_rate\": success_rate, \"iteration\": iteration})\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.num_train_epochs):\n",
    "                self.train(memory, iteration, epoch)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"checkpoints/{cfg.name}_model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"checkpoints/{cfg.name}_optimizer_{iteration}.pt\")\n",
    "\n",
    "            # Log model checkpoint to W&B\n",
    "            wandb.save(f\"{cfg.name}_model_{iteration}.pt\")\n",
    "            wandb.save(f\"{cfg.name}_optimizer_{iteration}.pt\")\n",
    "        wandb.finish()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n",
    "maze.visualize_path()\n",
    "\n",
    "model = ResNet(cfg.model.num_resBlocks, cfg.model.num_filters)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learn.lr)\n",
    "\n",
    "mcts = AlphaMCTS(maze, num_simulations=cfg.search.num_simulations, c_puct=cfg.search.c_puct, model=model)\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, mcts, \n",
    "                      num_learn_iters=cfg.learn.num_learn_iters, \n",
    "                      num_self_play_iters=cfg.learn.num_self_play_iters,\n",
    "                      num_train_epochs=cfg.learn.num_train_epochs,\n",
    "                      train_batch_size=cfg.learn.train_batch_size)\n",
    "# alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = Maze(cfg.maze.width, cfg.maze.height, cell_occupancy_prob=cfg.maze.cell_occupancy_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position (1,2) is showing the observation for being in the top right corner instead of the bottom left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_map = np.ones((4,4))\n",
    "my_map[1:3, 1:3] = 0\n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [1. 0. 0. 1. 1.]\n",
      "  [1. 0. 0. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_encoded_observation(self, state: Maze.State):\n",
    "    # Get the observation window centered at the agent\n",
    "    # Assumes width is odd\n",
    "\n",
    "    half_width = 5 // 2\n",
    "\n",
    "    # Pad the maze with obstacles (1s) to handle boundaries\n",
    "    padded_maze = np.pad(my_map, pad_width=half_width, mode='constant', constant_values=1)\n",
    "\n",
    "    # Adjust the agent's position due to padding\n",
    "    x_padded = state.x + half_width\n",
    "    y_padded = state.y + half_width\n",
    "\n",
    "    # Plane 0: Obstacles\n",
    "    # Extract the observation window where obstacle is 1 and free space is 0\n",
    "    plane_obstacles = padded_maze[\n",
    "        x_padded - half_width : x_padded + half_width + 1,\n",
    "        y_padded - half_width : y_padded + half_width + 1\n",
    "    ]\n",
    "\n",
    "    # Make sure that any number that is not 1 is 0\n",
    "    plane_obstacles[plane_obstacles != 1] = 0\n",
    "\n",
    "    return np.stack([plane_obstacles], axis=0)\n",
    "\n",
    "obs = get_encoded_observation(maze, Maze.State(1, 2, 2, 0))\n",
    "print(obs)\n",
    "maze.visualize_state(obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 1.],\n",
       "        [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_encoded_observation(maze, Maze.State(1, 1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1., 1.],\n",
       "        [1., 0., 0., 1., 1.],\n",
       "        [1., 0., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_encoded_observation(maze, Maze.State(2, 2, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_encoded_observation(maze, Maze.State(2, 1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there is no bug, I just confused myself re: the x and y being flipped. When the observation is rendered it looks correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
